% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={ReCentering Psych Stats: Multilevel/Hierarchical Linear Modeling},
  pdfauthor={Lynette H. Bikos, PhD, ABPP},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{ReCentering Psych Stats: Multilevel/Hierarchical Linear Modeling}
\author{Lynette H. Bikos, PhD, ABPP}
\date{12 Jun 2022}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{book-cover}{%
\chapter*{BOOK COVER}\label{book-cover}}
\addcontentsline{toc}{chapter}{BOOK COVER}

\begin{figure}
\centering
\includegraphics{images/ReC_mlevel_bkcvr.png}
\caption{An image of the book cover. It includes four quadrants of non-normal distributions representing gender, race/ethnicty, sustainability/global concerns, and journal articles}
\end{figure}

This open education resource is available in the following formats, all available in the \href{https://github.com/lhbikos/ReC_MultivModel/tree/main/docs}{docs} folder at the GitHub repository:

\begin{itemize}
\tightlist
\item
  Formatted as an \href{https://lhbikos.github.io/MultilevelModeling/}{html book}
\item
  As a \href{https://github.com/lhbikos/MultilevelModeling/blob/main/ReC_Multilevel.pdf}{PDF}
\end{itemize}

All materials used in creating this OER are available at its \href{https://github.com/lhbikos/MultilevelModeling}{GitHub repo}.

\hypertarget{preface}{%
\chapter*{PREFACE}\label{preface}}
\addcontentsline{toc}{chapter}{PREFACE}

\textbf{If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the purpose teaching my classes and gaining formative feedback from a host of stakeholders. The document was last updated on 12 Jun 2022}. Emerging volumes on other statistics are posted on the \href{https://lhbikos.github.io/BikosRVT/ReCenter.html}{ReCentering Psych Stats} page at my research team's website.

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c932455e-ef06-444a-bdca-acf7012d759a}{Screencasted Lecture Link}

To \emph{center} a variable in regression means to set its value at zero and interpret all other values in relation to this reference point. Regarding race and gender, researchers often center male and White at zero. Further, it is typical that research vignettes in statistics textbooks are similarly seated in a White, Western (frequently U.S.), heteronormative, framework. The purpose of this project is to create a set of open educational resources (OER) appropriate for doctoral and post-doctoral training that contribute to a socially responsive pedagogy -- that is, it contributes to justice, equity, diversity, and inclusion.

Statistics training in doctoral programs are frequently taught with fee-for-use programs (e.g., SPSS/AMOS, SAS, MPlus) that may not be readily available to the post-doctoral professional. In recent years, there has been an increase and improvement in R packages (e.g., \emph{psych}, \emph{lavaan}) used for in analyses common to psychological research. Correspondingly, many graduate programs are transitioning to statistics training in R (free and open source). This is a challenge for post-doctoral psychologists who were trained with other software. This OER will offer statistics training with R and be freely available (specifically in a GitHub respository and posted through GitHub Pages) under a Creative Commons Attribution - Non Commercial - Share Alike license {[}CC BY-NC-SA 4.0{]}.

Training models for doctoral programs in HSP are commonly scholar-practitioner, scientist-practitioner, or clinical-scientist. An emerging model, the \emph{scientist-practitioner-advocacy} training model incorporates social justice advocacy so that graduates are equipped to recognize and address the sociocultural context of oppression and unjust distribution of resources and opportunities \citep{mallinckrodt_scientist-practitioner-advocate_2014}. In statistics textbooks, the use of research vignettes engages the learner around a tangible scenario for identifying independent variables, dependent variables, covariates, and potential mechanisms of change. Many students recall examples in Field's \citeyearpar{field_discovering_2012} popular statistics text: Viagra to teach one-way ANOVA, beer goggles for two-way ANOVA, and bushtucker for repeated measures. What if the research vignettes were more socially responsive?

In this OER, research vignettes will be from recently published articles where:

\begin{itemize}
\tightlist
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC{[}low-middle income countries{]}),
\item
  the research is responsive to issues of justice, equity, inclusion, diversity,
\item
  the lesson's statistic is used in the article, and
\item
  there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s); or it is publicly available.
\end{itemize}

In training for multicultural competence, the saying, ``A fish doesn't know that it's wet'' is often used to convey the notion that we are often unaware of our own cultural characteristics. In recent months and years, there has been an increased awakening to the institutional and systemic racism that our systems are perpetuating. Queuing from the water metaphor, I am hopeful that a text that is recentered in the ways I have described can contribute to \emph{changing the water} in higher education and in the profession of psychology.

\hypertarget{copyright-with-open-access}{%
\section*{Copyright with Open Access}\label{copyright-with-open-access}}
\addcontentsline{toc}{section}{Copyright with Open Access}

This book is published under a a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA.

A \href{https://github.com/lhbikos/MultilevelModeling}{GitHub open-source repository} contains all of the text and source code for the book, including data and images.

\hypertarget{acknowledgements}{%
\chapter*{ACKNOWLEDGEMENTS}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{ACKNOWLEDGEMENTS}

As a doctoral student at the University of Kansas (1992-2005), I learned that ``a foreign language'' was a graduation requirement. \emph{Please note that as one who studies the intersections of global, vocational, and sustainable psychology, I regret that I do not have language skills beyond English.} This could have been met with credit from high school my rural, mid-Missouri high school did not offer such classes. This requirement would have typically been met with courses taken during an undergraduate program -- but my non-teaching degree in the University of Missouri's School of Education was exempt from this. The requirement could have also been met with a computer language (fortran, C++) -- I did not have any of those either. There was a tiny footnote on my doctoral degree plan that indicated that a 2-credit course, ``SPSS for Windows'' would substitute for the language requirement. Given that it was taught by my one of my favorite professors, I readily signed up. As it turns out, Samuel B. Green, PhD, was using the course to draft chapters in the textbook \citep{green_using_2014} that has been so helpful for so many. Unfortunately, Drs. Green (1947 - 2018) and Salkind (2947 - 2017) are no longer with us. I have worn out numerous versions of their text. Another favorite text of mine was Dr.~Barbara Byrne's \citeyearpar{byrne_structural_2016}, ``Structural Equation Modeling with AMOS.'' I loved the way she worked through each problem and paired it with a published journal article, so that the user could see how the statistical evaluation fit within the larger project/article. I took my tea-stained text with me to a workshop she taught at APA and was proud of the signature she added to it (a little catfur might have fallen out). Dr.~Byrne created SEM texts for a number of statistical programs (e.g., LISREL, EQS, MPlus). As I was learning R, I wrote Dr.~Byrne, asking if she had an edition teaching SEM/CFA with R. She promptly wrote back, saying that she did not have the bandwidth to learn a new statistics package. We lost Dr.~Byrne in December 2020. I am so grateful to these role models for their contributions to my statistical training. I am also grateful for the doctoral students who have taken my courses and are continuing to provide input for how to improve the materials.

The inspiration for training materials that re*center statistics and research methods came from the \href{https://www.academics4blacklives.com/}{Academics for Black Survival and Wellness Initiative}. This project, co-founded by Della V. Mosley, Ph.D., and Pearis L. Bellamy, M.S., made clear the necessity and urgency for change in higher education and the profession of psychology.

At very practical levels, I am indebted to SPU's Library, and more specifically, SPU's Education, Technology, and Media Department. Assistant Dean for Instructional Design and Emerging Technologies, R. John Robertson, MSc, MCS, has offered unlimited consultation, support, and connection. Senior Instructional Designer in Graphics \& Illustrations, Dominic Wilkinson, designed the logo and bookcover. Psychology and Scholarly Communications Librarian, Kristin Hoffman, MLIS, has provided consultation on topics ranging from OERS to citations. I am alo indebted to Associate Vice President, Teaching and Learning at Kwantlen Polytechnic University, Rajiv Jhangiani, PhD. Dr.~Jhangiani's text \citeyearpar{jhangiani_research_2019} was the first OER I ever used and I was grateful for his encouraging conversation.

Financial support for this project has been provided the following:

\begin{itemize}
\tightlist
\item
  \emph{Call to Action on Equity, Inclusion, Diversity, Justice, and Social Responsivity Request for Proposals} grant from the Association of Psychology Postdoctoral and Internship Centers (2021-2022).
\item
  \emph{Diversity Seed Grant}, Office of Inclusive Excellence and Advisory Council for Diversity and Reconciliation (ACDR), Seattle Pacific University.
\item
  \emph{ETM Open Textbook \& OER Development Funding}, Office of Education, Technology, \& Media, Seattle Pacific University.
\end{itemize}

\hypertarget{ReCintro}{%
\chapter{Introduction}\label{ReCintro}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=cc9b7c0d-e5c3-4e4e-a469-acf7013ee761}{Screencasted Lecture Link}

\hypertarget{what-to-expect-in-each-chapter}{%
\section{What to expect in each chapter}\label{what-to-expect-in-each-chapter}}

This textbook is intended as \emph{applied,} in that a primary goal is to help the scientist-practitioner-advocate use a variety of statistics in research problems and \emph{writing them up} for a program evaluation, dissertation, or journal article. In support of that goal, I try to provide just enough conceptual information so that the researcher can select the appropriate statistic (i.e., distinguishing between when ANOVA is appropriate and when regression is appropriate) and assign variables to their proper role (e.g., covariate, moderator, mediator).

This conceptual approach does include occasional, step-by-step, \emph{hand-calculations} (only we calculate them arithmetically in R) to provide a \emph{visceral feeling} of what is happening within the statistical algorithm that may be invisible to the researcher. Additionally, the conceptual review includes a review of the assumptions about the characteristics of the data and research design that are required for the statistic. Statistics can be daunting, so I have worked hard to establish a \emph{workflow} through each analysis. When possible, I include a flowchart that is referenced frequently in each chapter and assists the the researcher keep track of their place in the many steps and choices that accompany even the simplest of analyses.

As with many statistics texts, each chapter includes a \emph{research vignette.} Somewhat unique to this resource is that the vignettes are selected from recently published articles. Each vignette is chosen with the intent to meet as many of the following criteria as possible:

\begin{itemize}
\tightlist
\item
  the statistic that is the focus of the chapter was properly used in the article,
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC {[}low middle income countries{]}),
\item
  the research has a justice, equity, inclusion, diversity, and social responsivity focus and will contribute positively to a social justice pedagogy, and
\item
  the data is available in a repository or there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s).
\end{itemize}

In each chapter we employ \emph{R} packages that will efficiently calculate the statistic and the dashboard of metrics (e.g., effect sizes, confidence intervals) that are typically reported in psychological science.

\hypertarget{strategies-for-accessing-and-using-this-oer}{%
\section{Strategies for Accessing and Using this OER}\label{strategies-for-accessing-and-using-this-oer}}

There are a number of ways you can access this resource. You may wish to try several strategies and then select which works best for you. I demonstrate these in the screencast that accompanies this chapter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simply follow along in the .html formatted document that is available on via GitHub Pages, and then

  \begin{itemize}
  \tightlist
  \item
    open a fresh .rmd file of your own, copying (or retyping) the script and running it
  \end{itemize}
\item
  Locate the original documents at the \href{https://github.com/lhbikos/ReC_MultivModel}{GitHub repository} . You can

  \begin{itemize}
  \tightlist
  \item
    open them to simply take note of the ``behind the scenes'' script
  \item
    copy/download individual documents that are of interest to you
  \item
    fork a copy of the entire project to your own GitHub site and further download it (in its entirety) to your personal workspace. The \href{https://desktop.github.com/}{GitHub Desktop app} makes this easy!
  \end{itemize}
\item
  Listen to the accompanying lectures (I think sound best when the speed is 1.75). The lectures are being recorded in Panopto and should include the closed captioning.
\item
  Provide feedback to me! If you fork a copy to your own GitHub repository, you can

  \begin{itemize}
  \tightlist
  \item
    open up an editing tool and mark up the document with your edits,
  \item
    start a discussion by leaving comments/questions, and then
  \item
    sending them back to me by committing and saving. I get an e-mail notiying me of this action. I can then review (accepting or rejecting) them and, if a discussion is appropriate, reply back to you.
  \end{itemize}
\end{enumerate}

\hypertarget{if-you-are-new-to-r}{%
\section{If You are New to R}\label{if-you-are-new-to-r}}

R can be oveRwhelming. Jumping right into advanced statistics might not be the easiest way to start. However, in these chapters, I provide complete code for every step of the process, starting with uploading the data. To help explain what R script is doing, I sometimes write it in the chapter text; sometimes leave hastagged-comments in the chunks; and, particularly in the accompanying screencasted lectures, try to take time to narrate what the R script is doing.

I've found that, somewhere on the internet, there's almost always a solution to what I'm trying to do. I am frequently stuck and stumped and have spent hours searching the internet for even the tiniest of things. When you watch my videos, you may notice that in my R studio, there is a ``scRiptuRe'' file. I takes notes on the solutions and scripts here -- using keywords that are meaningful to me so that when I need to repeat the task, I can hopefully search my own prior solutions and find a fix or a hint.

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

The base program is free and is available here: \url{https://www.r-project.org/}

Because R is already on my machine (and because the instructions are sufficient), I will not walk through the instllation, but I will point out a few things.

\begin{itemize}
\tightlist
\item
  Follow the instructions for your operating system (Mac, Windows, Linux)
\item
  The ``cran'' (I think ``cranium'') is the \emph{Comprehensive R Archive Network.} In order for R to run on your computer, you have to choose a location. Because proximity is somewhat related to processing speed, select one that is geographically ``close to you.''
\item
  You will see the results of this download on your desktop (or elsewhere if you chose to not have it appear there) but you won't ever use R through this platform.
\end{itemize}

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

\emph{R Studio} is the desktop application I work in R. It's a separate download. Choose the free, desktop, option that is appropriate for your operating system: \url{https://www.rstudio.com/products/RStudio/}

\begin{itemize}
\tightlist
\item
  Upper right window: Includes several tabs; we frequently monitor the

  \begin{itemize}
  \tightlist
  \item
    Environment: it lists the \emph{objects} that are available to you (e.g., dataframes)
  \end{itemize}
\item
  Lower right window: has a number of helpful tabs.

  \begin{itemize}
  \tightlist
  \item
    Files: Displays the file structure in your computer's environment. Make it a practice to (a) organize your work in small folders and (b) navigating to that small folder that is holding your project when you are working on it.
  \item
    Packages: Lists the packages that have been installed. If you navigate to it, you can see if it is ``on.'' You can also access information about the package (e.g., available functions, examples of script used with the package) in this menu. This information opens in the Help window.
  \item
    Viewer and Plots are helpful, later, when we can simultaneously look at our output and still work on our script.
  \end{itemize}
\item
  Primary window

  \begin{itemize}
  \tightlist
  \item
    R Studio runs in the background(in the console). Very occasionally, I can find useful troubleshooting information here.
  \item
    More commonly, I open my R Markdown document so that it takes the whole screen and I work directly, right here.
  \end{itemize}
\item
  \emph{R Markdown} is the way that many analysts write \emph{script}, conduct analyses, and even write up results. These are saved as .rmd files.

  \begin{itemize}
  \tightlist
  \item
    In R Studio, open an R Markdown document through File/New File/R Markdown
  \item
    Specify the details of your document (title, author, desired ouput)
  \item
    In a separate step, SAVE this document (File/Save{]} into a NEW FILE FOLDER that will contain anything else you need for your project (e.g., the data).
  \item
    \emph{Packages} are at the heart of working in R. Installing and activating packages require writing script.
  \end{itemize}
\end{itemize}

\hypertarget{r-hygiene}{%
\subsection{R Hygiene}\label{r-hygiene}}

Many initial problems in R can be solved with good R hygiene. Here are some suggestions for basic practices. It can be tempting to ``skip this.'' However, in the first few weeks of class, these are the solutions I am presenting to my students.

\hypertarget{everything-is-documented-in-the-.rmd-file}{%
\subsubsection{Everything is documented in the .rmd file}\label{everything-is-documented-in-the-.rmd-file}}

Although others do it differently, everything is in my .rmd file. That is, for uploading data and opening packages I write the code in my .rmd file. Why? Because when I read about what I did hours or years later, I have a permanent record of very critical things like (a) where my data is located, (b) what version I was using, and (c) what package was associated with the functions.

\hypertarget{file-organization}{%
\subsubsection{File organization}\label{file-organization}}

File organization is a critical key to this:

\begin{itemize}
\tightlist
\item
  Create a project file folder.
\item
  Put the data file in it.
\item
  Open an R Markdown file.
\item
  Save it in the same file folder.
\item
  When your data and .rmd files are in the same folder (not your desktop, but a shared folder), they can be connected.
\end{itemize}

\hypertarget{chunks}{%
\subsubsection{Chunks}\label{chunks}}

The R Markdown document is an incredible tool for integrating text, tables, and analyses. This entire OER is written in R Markdown. A central feature of this is ``chunks.''

The easiest way to insert a chunk is to use the INSERT/R command at the top of this editor box. You can also insert a chunk with the keyboard shortcut: CTRL/ALT/i

``Chunks'' start and end with with those three tic marks and will show up in a shaded box, like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#hashtags let me write comments to remind myself what I did}
\CommentTok{\#here I am simply demonstrating arithmetic (but I would normally be running code)}
\DecValTok{2021} \SpecialCharTok{{-}} \DecValTok{1966}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55
\end{verbatim}

Each chunk must open and close. If one or more of your tic marks get deleted, your chunk won't be read as such and your script will not run. The only thing in the chunks should be script for running R; you can hashtag-out script so it won't run.

Although unnecessary, you can add a brief title for the chunk in the opening row, after the ``r.'' These create something of a table of contents of all the chunks -- making it easier to find what you did. You can access them in the ``Chunks'' tab at the bottom left of R Studio. If you wish to knit a document, you cannot have identical chunk titles.

You can put almost anything you want in the space outside of tics. Syntax for simple formatting in the text areas (e.g,. using italics, making headings, bold, etc.) is found here: \url{https://rmarkdown.rstudio.com/authoring_basics.html}

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

As scientist-practitioners (and not coders), we will rely on \emph{packages} to do our work for us. At first you may feel overwhelmed about the large number of packages that are available. Soon, though, you will become accustomed to the ones most applicable to our work (e.g., psych, tidyverse, lavaan, apaTables).

Researchers treat packages differently. In these lectures, I list all the packages we will use in an opening chunk that asks R to check to see if the package is installed, and if not, installs it.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(psych))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: psych
\end{verbatim}

To make a package operable, you need to open it through the library. This process must be repeated each time you restart R. I don't open the package (through the ``library(package\_name)'') command until it is time to use it. Especially for new users, I think it's important to connect the functions with the specific packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages ("psych")}
\FunctionTok{library}\NormalTok{ (psych)}
\end{Highlighting}
\end{Shaded}

If you type in your own ``install.packages'' code, hashtag it out once it's been installed. It is problematic to continue to re-run this code .

\hypertarget{knitting}{%
\subsubsection{Knitting}\label{knitting}}

An incredible feature of R Markdown is its capacity to \emph{knit} to HTML, powerpoint, or word. If you access the .rmd files for this OER, you can use annotate or revise them to suit your purposes. If you redistribute them, though, please honor the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License with a citation.

\hypertarget{troubleshooting-in-r-markdown}{%
\subsection{tRoubleshooting in R maRkdown}\label{troubleshooting-in-r-markdown}}

Hiccups are normal. Here are some ideas that I have found useful in getting unstuck.

\begin{itemize}
\tightlist
\item
  In an R script, you must have everything in order -- Every. Single. Time.

  \begin{itemize}
  \tightlist
  \item
    All the packages have to be in your library and activated; if you restart R, you need to reload each package.
  \item
    If you open an .rmd file and want a boxplot, you cannot just scroll down to that script. You need to run any \emph{prerequisite} script (like loading the package, importing data, putting the data in the global environment, etc.)
  \item
    Do you feel lost? clear your global environment (broom) and start at the top of the R script. Frequent, fresh starts are good.
  \end{itemize}
\item
  Your .rmd file and your data need to be stored in the same file folder. These should be separate for separate projects, no matter how small.
\item
  Type any warnings you get into a search engine. Odds are, you'll get some decent hints in a manner of seconds. Especially at first, these are common errors:

  \begin{itemize}
  \tightlist
  \item
    The package isn't loaded (if you restarted R, you need to reload your packages)
  \item
    The .rmd file has been saved yet, or isn't saved in the same folder as the data
  \item
    Errors of punctuation or spelling
  \end{itemize}
\item
  Restart R (it's quick -- not like restarting your computer)
\item
  If you receive an error indicating that a function isn't working or recognized, and you have loaded the package, type the name of the package in front of the function with two colons (e.g., psych::describe(df). If multiple packages are loaded with functions that have the same name, R can get confused.
\end{itemize}

\hypertarget{strategies-for-success}{%
\subsection{stRategies for success}\label{strategies-for-success}}

\begin{itemize}
\tightlist
\item
  Engage with R, but don't let it overwhelm you.

  \begin{itemize}
  \tightlist
  \item
    The \emph{mechanical is also the conceptual}. Especially when it is \emph{simpler}, do try to retype the script into your own .rmd file and run it. Track down the errors you are making and fix them.
  \item
    If this stresses you out, move to simply copying the code into the .rmd file and running it. If you continue to have errors, you may have violated one of the best practices above (Is the package loaded? Are the data and .rmd files in the same place? Is all the prerequisite script run?).
  \item
    Still overwhelmed? Keep moving forward by downloading a copy of the .rmd file that accompanies any given chapter and just ``run it along'' with the lecture. Spend your mental power trying to understand what each piece does. Then select a practice problem that is appropriate for your next level of growth.
  \end{itemize}
\item
  Copy script that works elsewhere and replace it with your datafile, variables, etc.\\
\item
  The leaRning curve is steep, but not impossible. Gladwell\citeyearpar{gladwell_outliers_2008} reminds us that it takes about 10,000 hours to get GREAT at something (2,000 to get reasonably competent). Practice. Practice. Practice.
\item
  Updates to R, R Studio, and the packages are NECESSARY, but can also be problematic. It could very well be that updates cause programs/script to fail (e.g., ``X has been deprecated for version X.XX''). Moreover, this very well could have happened between my distribution of these resources and your attempt to use it. My personal practice is to update R, R Studio, and the packages a week or two before each academic term.
\item
  Embrace your downward dog. Also, walk away, then come back.
\end{itemize}

\hypertarget{resources-for-getting-started}{%
\subsection{Resources for getting staRted}\label{resources-for-getting-started}}

R for Data Science: \url{https://r4ds.had.co.nz/}

R Cookbook: \url{http://shop.oreilly.com/product/9780596809164.do}

R Markdown homepage with tutorials: \url{https://rmarkdown.rstudio.com/index.html}

R has cheatsheets for everything, here's one for R Markdown: \url{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}

R Markdown Reference guide: \url{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}

Using R Markdown for writing reproducible scientific papers: \url{https://libscie.github.io/rmarkdown-workshop/handout.html}

LaTeX equation editor: \url{https://www.codecogs.com/latex/eqneditor.php}

\hypertarget{wGroups}{%
\chapter{Nested Within Groups}\label{wGroups}}

Lynette H. Bikos, PhD, ABPP, and Kiet D. Huynh, PhD

Co-Authors

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=f24e4b50-1204-4412-a78f-ad2500198bb6}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen=}\DecValTok{999}\NormalTok{)}\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

This chapter provides an introduction to multilevel modeling (MLM). Known by a variety of names, MLM offers researchers the ability to manage data that is \emph{nested} within groups (cross-sectional), within persons (longitudinal), or both. MLM is complex and powerful. This chapter will provide an introduction and worked example of MLM when data is collected in groups (churches). At the risk of oversimplification, my goal is to make it as accessible as possible. To that end, the chapter and lecture will err on the side of application. If you are interested in the more technical details of this procedure, there are tremendous resources that far exceed my capacity and the purpose of this OER \citep[e.g.,][]{bryk_hierarchical_1992}.

At the outset of this series of chapters on MLM, let me share with you why I get so excited about this statistical approach. Remember ANOVA? And its assumptions? Among these were assumptions of \emph{balanced designs} (i.e., equal cell sizes); \emph{independence} (i.e., unless using repeated measures ANOVA, participants could not be related/connected in other ways); and to rule out confounds, \emph{random assignment} to treatment conditions. Unless the data to be analyzed comes from an experiment, these are difficult conditions to meet. When we use a multilevel approach to analyze cross-sectional research where there is clear nesting in groups (e.g., teams, classrooms) we are no longer bound by these restrictive assumptions. Presuming there is an adequate sample \citep[ suggested a minimum of 10 clusters with 5 members each]{bell_how_2014},even the group size can vary. Of course there are other benefits and challenges that we will address throughout the series of chapters.

\hypertarget{navigating-this-lesson}{%
\section{Navigating this Lesson}\label{navigating-this-lesson}}

There is about 1 hour and 30 minutes of lecture. If you work through the materials with me it would be plan for an additional 2 hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_CPA}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives}{%
\subsection{Learning Objectives}\label{learning-objectives}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Recognize when MLM is appropriate as an analytic strategy for a particular research design and set of data.
\item
  Distinguish between L1 and L2 variables.
\item
  Describe the compositional effects approach to centering variables and explain how it completely captures within- and between- group variance.
\item
  Write R script to group-mean center, grand-mean center, and aggregate L1 variables for their L2 representation.
\item
  Utilize a sequential and systematic approach to testing a series of multilevel models.
\item
  Create corresponding figures and tables.
\item
  Write up the results of a cross-sectional multilevel model in APA style.
\end{itemize}

\hypertarget{planning-for-practice}{%
\subsection{Planning for Practice}\label{planning-for-practice}}

In this chapter we offer three suggestions for practice. Each are graded in complexity. At a minimum, we recommend that you analyze a multilevel model that contains one level-1 (within-group) predictor, one level-2 (between groups) predictor, and their interaction.

\begin{itemize}
\tightlist
\item
  Rework the problem in the chapter by changing the random seed in the code that simulates the data. This should provide very minor changes to the data, but the results will likely be very similar.
\item
  The research vignette analyzes a number of variables, simultaneously. We selected only two for the example. Swap out one or more variables in the multilevel model and compare your solution to the one in the chapter (and/or one you mimicked in the journal article). If you wish to increase your probability of finding statistically significant effects, look for hints in Table 2 of the \citep{lefevor_homonegativity_2020} research article that sources the vignettes by selecting a variable(s) with a significant relationship with your DV.
\item
  Conduct a multilevel model with data to which you have access. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources}{%
\subsection{Readings \& Resources}\label{readings-resources}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Cohen, J., Cohen, P., West, S. G., \& Aiken, L. S. (2003). \emph{Applied multiple regression/correlation analysis for the behavioral sciences, 3rd ed.} Lawrence Erlbaum Associates Publishers
\item
  Enders, C. K., \& Tofighi, D. (2007). Centering predictor variables in cross-sectional multilevel models: A new look at an old issue. \emph{Psychological Methods, 12}(2), 121-138. \url{doi:10.1037/1082-989X.12.2.121}
\item
  McCoach, D. B., \& Adelson, J. L. (2010). Dealing with dependence (Part I): Understanding the effects of clustered data. \emph{Gifted Child Quarterly, 54}(2), 152--155. \url{https://doi-org.ezproxy.spu.edu/10.1177/0016986210363076}
\item
  McCoach, D. B. (2010). Dealing with dependence (Part II): A gentle introduction to hierarchical linear modeling. \emph{Gifted Child Quarterly, 54}(3), 252-256. doi: 10.1177/0016986210373475
\item
  Lefevor, G. T., Paiz, J. Y., Stone, W.-M., Huynh, K. D., Virk, H. E., Sorrell, S. A., \& Gage, S. E. (2020). Homonegativity and the Black church: Is congregational variation the missing link? \emph{The Counseling Psychologist, 48}(6), 826--851. \url{https://doi-org.ezproxy.spu.edu/10.1177/0011000020918558}
\end{itemize}

\hypertarget{packages-1}{%
\subsection{Packages}\label{packages-1}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#will install the package if not already installed}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(lme4))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"lme4"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(nlme))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"nlme"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(sjstats))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"sjstats"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(tidyverse))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(psych))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(lmerTest))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"lmerTest"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(robumeta))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"robumeta"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(sjstats))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"sjstats"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(PerformanceAnalytics))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"PerformanceAnalytics"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{multilevel-modeling-nested-within-groups}{%
\section{Multilevel Modeling: Nested within Groups}\label{multilevel-modeling-nested-within-groups}}

\hypertarget{the-dilemma-of-aggregation-and-disaggregation}{%
\subsection{The dilemma of aggregation and disaggregation}\label{the-dilemma-of-aggregation-and-disaggregation}}

It was the 1980s and researchers were studying group attitudes and were confused about how to analyze the data \citep{singer_applied_2003}.They ran into difficulties with \emph{aggregation} and asked, ``Do we aggregate the data'' by summing individuals within groups (i.e., giving everyone in the group the same score)?'' Or, ``Do we disaggregate the data'' by ignoring group membership and analyzing the individual cases.

Problems with aggregation (using group means) include the following:

\begin{itemize}
\tightlist
\item
  Regression equations describe the relationship of means of predictors in individual clusters to the mean of the dependent variable in those clusters.
\item
  There is a decrease in variability regarding the the ability to explain what is going on with the dependent variable.
\item
  It can be misleading to generalize from the group level variable to the individual. This is termed the \emph{ecological fallacy} (also known as the \emph{Robinson Effect}).
\end{itemize}

Problems with disaggregation (using individual scores and ignoring the group influence) include:

\begin{itemize}
\tightlist
\item
  Results that ignore group level variables.
\item
  There is often clustering among group members.
\item
  Clustering (i.e., group effects, dependency in the data) violates the assumption of independence for most ANOVA and regression statistics.
\item
  We are more likely to make a Type I error (i.e., declaring a statistically significant relationship when there is none) because

  \begin{itemize}
  \tightlist
  \item
    Alpha inflation
  \item
    Standard error is based on N; standard errors are smaller than they should be.
  \item
    Dependency in the data may reduce within group variance.
  \end{itemize}
\end{itemize}

\hypertarget{multilevel-modeling-the-definitional-and-conceptual}{%
\subsection{Multilevel modeling: The definitional and conceptual}\label{multilevel-modeling-the-definitional-and-conceptual}}

Multilevel modeling (MLM) has a host of names:

\begin{itemize}
\tightlist
\item
  Hierarchical linear modeling (but this also references a specific, fee-for-use, package)
\item
  Mixed effects
\item
  Linear mixed effects (LME -- you'll see this acronym in our R package and functions)
\item
  Random coefficient regression (RCR)
\item
  Random coefficient modeling (RCM)
\end{itemize}

By whatever name we call it, the \emph{random coefficient regression model} is an alternative to ordinary least squares regression (OLS) that is structured to handle clustered data. Random coefficient regression differs from OLS regression in the assumptions made about the nature of the regression coefficients and the correlational structure of the individual observations.

Highlights of RC regression models,

\begin{itemize}
\tightlist
\item
  individuals are clustered into groups

  \begin{itemize}
  \tightlist
  \item
    and we can have multiple levels of measurement at the individual and group levels),
  \end{itemize}
\item
  the equations are mathematically different from OLS regression,
\item
  they can be applied cross-sectional and repeated measures designs.
\end{itemize}

In this chapter our focus is on the cross-sectional, nested analyses.

\begin{figure}
\centering
\includegraphics{images/wiGroups/nesting.jpg}
\caption{Image of a three-level model}
\end{figure}

``Levels'' on these figures are important and represent the hierarchical structure of RCR.

\begin{itemize}
\tightlist
\item
  Level 1: lowest level of aggregation, the individual, a micro-level
\item
  Level 2: cluster or group level, the macro-level
\item
  Levels 3 +: higher-order clustering; beyond the scope of this class (and instructor).
\end{itemize}

As we work through this chapter we will be reviewing essential elements to MLM. These include:

\begin{itemize}
\tightlist
\item
  Levels
\item
  Fixed and random effects
\item
  Variance components
\item
  Centering to maximize interpretability and a complete accounting of variance
\item
  Equations
\end{itemize}

Because these are complicated, it makes sense to me to start introduce the research vignette a little earlier than usual so that we have a concrete example for locating these concepts. First, though, let's look at how we manage an MLM analysis.

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

\begin{figure}
\centering
\includegraphics{images/wiGroups/MLM_workflow_xs.jpg}
\caption{Image of a workflow through a nested within-groups MLM}
\end{figure}

\hypertarget{research-vignette}{%
\section{Research Vignette}\label{research-vignette}}

The research vignette comes from Lefevor et al.'s \citeyearpar{lefevor_homonegativity_2020} article, ``Homonegativity and the Black Church: Is congregational variation the missing link?'' The article was published in \emph{The Counseling Psychlogist}. I am so grateful to the authors who provided their R script. It was a terrific source of consultation as I planned the chapter.

Data is from congregants in 15 Black churches (with at least 200 members in each church) in a mid-sized city in the South. Congregational participation ranged from 2 to 28. The research design allows the analysts to identify individual level and contextual (i.e., congregational) level predictors of attitudes toward same-sex sexuality.

The research question asks, what individual-level and church-level predictors influence an indivdiual's attitudes toward same-sex sexuality (i.e., homonegativity).

Variables used in the study included:

\begin{itemize}
\item
  \textbf{Attitudes toward Same Sex Sexuality(ATSS/homonegativity)}: The short form of the Attitudes Toward Lesbian Women and Gay Men Scale \citep{herek_assessing_1994} is 10 items on a 5-point likert scale of agreement. Sample items include, ``Sex between two men is just plan wrong'' and ``Lesbians are sick.'' Higher scores represent more homonegative views.
\item
  \textbf{Religiousness} Organizational religiousness was assessed through with the single-item organizational religious activity scale of the Duke University Religiousness Index \citep{koenig_duke_2010}. The item asks participants to report how often they attend church or other religious meetings on a 9-point Likert-type scale ranging from 0 (\emph{never}) to 9 (\emph{several times a week}). Higher scores indicate more frequent attendance.
\item
  \textbf{Racial homogeneity} This was calculated by estimating the proportion of respondents from a single race prior to excluding those who did not meet the inclusion criteria (e.g., one criteria was that the participants self-identify as Black).
\item
  \textbf{Age, Education, Gender}: Along with other demographic and background variables, age, education, and gender were collected. Gender is dichotomous with 0 = woman and 1 = man.
\end{itemize}

In the article, Lefevor \citeyearpar{lefevor_homonegativity_2020} and colleagues predict attitudes toward same-sex sexuality from a number of person-level (L1) and congregation-level (L2) predictors. Because this is an instructional article, we are choosing one each: attendance (used as both L1 and L2) and homogeneity of the congregation (L2). Although the authors do not include cross-level (i.e., an interaction between L1 and L2 variables), we will test a cross-level interaction of attendance*homogeneity.

\hypertarget{simulating-the-data-from-the-journal-article}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article}}

Muldoon \citeyearpar{muldoon_simulate_2018} has provided clear and intuitive instructions for simulating multilevel data.

Simulating the data gives us some information about the nature of MLM. You can see that we have identified:

\begin{itemize}
\tightlist
\item
  the number of churches
\item
  the number of members from each church

  \begin{itemize}
  \tightlist
  \item
    Note: in this simulation we have the benefit of non-missing data (unless we specify it)
  \end{itemize}
\item
  the b weights (and ranges) reported in the Lefevor et al. \citeyearpar{lefevor_homonegativity_2020} article
\item
  the mean and standard deviation of the dependent variable
\end{itemize}

Further down in the code, we feed R the regression equation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200407}\NormalTok{)}
\NormalTok{n\_church }\OtherTok{=} \DecValTok{15}
\NormalTok{n\_mbrs }\OtherTok{=} \DecValTok{15}
\NormalTok{b0 }\OtherTok{=} \FloatTok{3.43} \CommentTok{\#intercept for ATSS}
\NormalTok{b1 }\OtherTok{=}\NormalTok{ .}\DecValTok{14} \CommentTok{\#b weight for L1 var gender}
\NormalTok{b2 }\OtherTok{=}\NormalTok{ .}\DecValTok{00} \CommentTok{\#b weight or L1 var age}
\NormalTok{b3 }\OtherTok{=}\NormalTok{ .}\DecValTok{02} \CommentTok{\#b weight for L1 var education}
\NormalTok{b4 }\OtherTok{=}\NormalTok{ .}\DecValTok{10} \CommentTok{\#b weight for the L1 variable religious attendance}
\NormalTok{b5 }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{89} \CommentTok{\#b weight for the L2 variable, racial homogeneity}
\NormalTok{( }\AttributeTok{Gender =} \FunctionTok{runif}\NormalTok{(n\_church}\SpecialCharTok{*}\NormalTok{n\_mbrs, }\SpecialCharTok{{-}}\FloatTok{1.09}\NormalTok{, }\FloatTok{1.67}\NormalTok{)) }\CommentTok{\#calc L1 gender}
\NormalTok{( }\AttributeTok{Age =} \FunctionTok{runif}\NormalTok{(n\_church}\SpecialCharTok{*}\NormalTok{n\_mbrs, }\FloatTok{6.44}\NormalTok{, }\FloatTok{93.93}\NormalTok{)) }\CommentTok{\#calc L1 age}
\NormalTok{( }\AttributeTok{Education =} \FunctionTok{runif}\NormalTok{(n\_church}\SpecialCharTok{*}\NormalTok{n\_mbrs, }\DecValTok{0}\NormalTok{, }\FloatTok{8.46}\NormalTok{)) }\CommentTok{\#calc L1 education}
\NormalTok{( }\AttributeTok{Attendance =} \FunctionTok{runif}\NormalTok{(n\_church}\SpecialCharTok{*}\NormalTok{n\_mbrs,}\FloatTok{5.11}\NormalTok{, }\FloatTok{10.39}\NormalTok{)) }\CommentTok{\#calc L1 attendance by grabbing  its M +/{-} 3SD}
\NormalTok{( }\AttributeTok{Homogeneity =} \FunctionTok{rep}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n\_church, .}\DecValTok{37}\NormalTok{, }\FloatTok{1.45}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_mbrs)) }\CommentTok{\#calc L2 homogeneity by grabbing  its M +/{-} 3SD}
\NormalTok{mu }\OtherTok{=} \FloatTok{3.39} 
\NormalTok{sds }\OtherTok{=}\NormalTok{ .}\DecValTok{64} \CommentTok{\#this is the SD of the DV}
\NormalTok{sd }\OtherTok{=} \DecValTok{1} \CommentTok{\#this is the observation{-}level random effect variance that we set at 1}

\NormalTok{( }\AttributeTok{church =} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_church], }\AttributeTok{each =}\NormalTok{ n\_mbrs) )}
\CommentTok{\#( mbrs = numbers[1:(n\_church*n\_mbrs)] )}
\NormalTok{( }\AttributeTok{churcheff =} \FunctionTok{rnorm}\NormalTok{(n\_church, }\DecValTok{0}\NormalTok{, sds) )}
\NormalTok{( }\AttributeTok{churcheff =} \FunctionTok{rep}\NormalTok{(churcheff, }\AttributeTok{each =}\NormalTok{ n\_mbrs) )}
\NormalTok{( }\AttributeTok{mbrseff =} \FunctionTok{rnorm}\NormalTok{(n\_church}\SpecialCharTok{*}\NormalTok{n\_mbrs, }\DecValTok{0}\NormalTok{, sd) )}
\NormalTok{( }\AttributeTok{ATSS =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{Gender }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{Age }\SpecialCharTok{+}\NormalTok{ b3}\SpecialCharTok{*}\NormalTok{Education }\SpecialCharTok{+}\NormalTok{ b4}\SpecialCharTok{*}\NormalTok{Attendance }\SpecialCharTok{+}\NormalTok{ b5}\SpecialCharTok{*}\NormalTok{Homogeneity }\SpecialCharTok{+}\NormalTok{ churcheff }\SpecialCharTok{+}\NormalTok{ mbrseff)}
\NormalTok{( }\AttributeTok{dat =} \FunctionTok{data.frame}\NormalTok{(church, churcheff, mbrseff, Gender, Age, Education, Attendance, Homogeneity, ATSS) )}

\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())}
\CommentTok{\#moving the ID number to the first column; requires }
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{Lefevor2020 }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, church, Gender, Age, Education, Attendance, Homogeneity, ATSS)}
\CommentTok{\#rounded gender into dichotomous variable}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Gender, }\DecValTok{0}\NormalTok{)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0 }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Gender)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{\#( dat$ATSS = with(dat, mu + churcheff + mbrseff ) )}
\end{Highlighting}
\end{Shaded}

Below is script that will allow you to export and reimport the dataset we just simulated. This may come in handy if you wish to start from the simulated data (and not wait for the simulation each time) and/or if you would like to use the dataset for further practice.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(Lefevor2020, }\AttributeTok{file=}\StringTok{"Lefevor2020.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{Lefevor2020 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"Lefevor2020.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Because we are simulating data, we have the benefit of no missingness and relatively normal distributions. Because of these reasons we will skip the formal data preparation stage. We will, though, take a look at our characteristics and bivariate relations of our three variables of interest.

\hypertarget{working-the-problem-and-learning-mlm}{%
\section{Working the Problem (and learning MLM)}\label{working-the-problem-and-learning-mlm}}

\hypertarget{data-diagnostics}{%
\subsection{Data diagnostics}\label{data-diagnostics}}

Multilevel modeling holds assumptions that will likely be familiar to use:

\begin{itemize}
\tightlist
\item
  linearity
\item
  homogeneity of variance
\item
  normal distribution of the model's residuals
\end{itemize}

Because I cover strategies for evaluating these assumptions in the \href{https://lhbikos.github.io/ReC_MultivModel/DataDx.html}{Data Dx} chapter, I won't review them here. Another helpful resource for reviewing assumptions related to MLM is provided in by \href{https://ademos.people.uic.edu/Chapter18.html\#6_assumptions}{Michael Palmeri}.

We should, though take a look at the relations between the variables in our model in their \emph{natural} form. In this case \emph{natural} refers to their scored, ready-to-be-analyzed (but not further centered).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{pairs.panels}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{stars =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-11-1.pdf}
What do we observe in this preliminary, zero-ordered relationship?

\begin{itemize}
\tightlist
\item
  As racial homogeneity increases, homonegativity decreases.

  \begin{itemize}
  \tightlist
  \item
    Curiously, there is a non-linear curve between those two variables -- but that seems to be ``pulled'' by an outlier(?) in the lower right quandrant of the ATSS/Homonegativity relationship.
  \end{itemize}
\item
  ATTS appears to be normally distributed
\item
  Attendance has a flat distribution
\end{itemize}

We can learn more by examining descriptive statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            vars   n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 225 3.32 1.18   3.43    3.34 1.11 -1.23  6.45  7.69 -0.30
Attendance     2 225 7.52 1.52   7.36    7.47 1.84  5.12 10.35  5.22  0.24
Homogeneity    3 225 1.04 0.25   1.14    1.04 0.34  0.69  1.40  0.71 -0.04
            kurtosis   se
ATSS            0.27 0.08
Attendance     -1.19 0.10
Homogeneity    -1.60 0.02
\end{verbatim}

These descriptives allow us a glimpse of the means and standard deviations of our study variables. Additionally, we can look at skew and kurtosis to see that our variables are within the normal ranges (i.e., below 3 for skew; below 8 for kurtosis \citep{kline_principles_2016}).

\hypertarget{levels}{%
\subsection{Levels}\label{levels}}

\emph{Levels} are a critical component of MLM. In the context of MLM models of nesting within groups/clusters (e.g., cross-sectional MLM):

\begin{itemize}
\tightlist
\item
  Level 1 (L1) variables ``belong to the person''

  \begin{itemize}
  \tightlist
  \item
    Age, race, attitudinal or behavioral assessment
  \end{itemize}
\item
  Level 2 (L2) variables ``belong to the group/cluster''

  \begin{itemize}
  \tightlist
  \item
    Leader characteristic, economic indicator that is unique to the group/cluster
  \item
    Aggregate/composite representation of L1 variables
  \end{itemize}
\end{itemize}

In our tiny model from the Lefevor et al. \citeyearpar{lefevor_homonegativity_2020} vignette:

\begin{itemize}
\tightlist
\item
  ATSS/homonegativity is our DV; it is an L1 observation because we are predicting individual's attitudes toward same-sex sexuality.
\item
  Attendance is an L1 observation \emph{when} we are using it as the individual's own church attendance.
\item
  Attendance \emph{will be } an L2 observation when we aggregate it an use it as a value to represent the church.
\item
  Racial homogeneity is only entered as an L2 variable. It was collected at the individual level via self-identification of race and calculated to represent the proportion of Black individuals in the church.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"church"}\NormalTok{, }\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{n =}\NormalTok{ 30L)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   church      ATSS Attendance Homogeneity
1       A 4.7835442   6.318674   0.7957395
2       A 5.3851521   9.391428   0.7957395
3       A 4.3722317   9.832894   0.7957395
4       A 4.8635210   7.721731   0.7957395
5       A 4.9733886   9.917289   0.7957395
6       A 4.3455429   8.844333   0.7957395
7       A 3.5357514   6.630585   0.7957395
8       A 4.1572480   8.111701   0.7957395
9       A 4.2946421   9.658424   0.7957395
10      A 3.9311877   5.743799   0.7957395
11      A 4.9380802   6.048393   0.7957395
12      A 4.6318423   5.908200   0.7957395
13      A 3.4139728   7.685524   0.7957395
14      A 2.6348562   6.900703   0.7957395
15      A 4.8415811   6.594790   0.7957395
16      B 0.9835024   6.176724   1.1691398
17      B 1.9247771   7.907341   1.1691398
18      B 3.7576164   5.469059   1.1691398
19      B 3.6992782   9.262913   1.1691398
20      B 2.9125454   9.001514   1.1691398
21      B 4.0568240   7.486189   1.1691398
22      B 1.2471085   9.417031   1.1691398
23      B 3.2280375   6.066014   1.1691398
24      B 3.7688923   7.413705   1.1691398
25      B 4.0418646   6.047803   1.1691398
26      B 5.4240898   7.157878   1.1691398
27      B 3.8355654   9.687518   1.1691398
28      B 2.6657722   5.972512   1.1691398
29      B 2.8502831   9.761905   1.1691398
30      B 2.8982202   9.689345   1.1691398
\end{verbatim}

In this display of the first 30 rows, we see the data for the first two churches (i.e., A and B). The value is (potentially) different for each individual in each church for the two L1 variables: \emph{ATSS}, \emph{Attendance.} In contrast, the value of the variable is constant for the L2 variable, \emph{Homogeneity} for churches A and B.

\hypertarget{centering}{%
\subsection{Centering}\label{centering}}

Before we continue with modeling, we need to consider \emph{centering}. That is, we transform our predictor variables to give the intercept parameters more useful interpretations.

While there are some general practices, there are often arguments for different approaches:

\begin{itemize}
\tightlist
\item
  We usually focus centering on L1 predictors.
\item
  We usually focus centering on continuously scaled variables.
\item
  Dichotomous variables are considered to be centered, so long as there is a meaningful 0 (e.g., control group = 0; treatment group = 1), many do not further center.

  \begin{itemize}
  \tightlist
  \item
    Newsom \citeyearpar{newsom_centering_2019}, though, argues that if a binary variable is an L1 predictor, group mean centering produces intercepts weighted by the proportion of 1 to 0 values for each group; grand mean centering provides the sample weight adjustment to make the sample mean (each group's mean) proportionate to the population (full sample)
  \end{itemize}
\item
  Dependent variables are generally not centered
\end{itemize}

we generally consider three centering strategies:

The \textbf{natural metric} is ideal if the variable has a meaningful zero point (e.g., drug dosage, time). It is more difficult when there is a non-zero metric. When there are dichotomous variables, the natural metric works well (i.e., 0 = control group, 1 = treatment group). The natural metric is an acceptable choice when the interest is only on the effects of L1 variables, rather than on the effects of group-level variables.

\textbf{Grand mean centering (GCM)} involves subtracting the mean from each case's score on the variable. The intercept is interpreted as the expected value of the DV for a person/group that is compared with all individuals/groups.

\begin{itemize}
\tightlist
\item
  Intercepts are adjusted group means (like an ANCOVA model)
\item
  Variance in the intercepts represents between-group variance in the adjusted means (i.e., adjusted for L1 predictors)
\item
  The effects of L1 predictors are partialed out (controlled for) of the between-group variance
\item
  GCM is most useful when we are interested in

  \begin{itemize}
  \tightlist
  \item
    L2 predictors with L1 covariates
  \item
    Interactions specified L2
  \end{itemize}
\item
  GCM is a good choice when the primary interest is on the effects of L2 variables, controlling for the L1 predictors.
\end{itemize}

\textbf{Group mean centering} or \textbf{Centering within Context (CWC)} involves subtracting the mean of the individual's group from each score. The L1 intercept is interpreted as the expected mean on the DV for the person's group. Group mean centering/CWC:

\begin{itemize}
\tightlist
\item
  Provides a measure of the IV that accounts for one's relative standing within the group
\item
  Removes between-group variability from the model (deviations rom the group means are now the predictors)

  \begin{itemize}
  \tightlist
  \item
    If we only use group mean centering (CWC), we lose information about between-group differences
  \end{itemize}
\item
  Assumes that relative standing within the group is an important factor
\item
  Is most useful when we are interested in

  \begin{itemize}
  \tightlist
  \item
    Relations among L1 variables
  \item
    Interactions among L1 variables
  \item
    Interactions between L1 and L2 variables
  \end{itemize}
\item
  CWC is an acceptable choice when the interest is only on the effects of L1 variables, rather than on the effects of group-level variables because it provides unbiased estimates of the pooled within group effect of an individual variable.
\end{itemize}

In the case of making centering choices with our variables, we are must think about the \emph{frog pond effect}. That is, for the same size frog, the experience of being in a pond with big frogs may be different from being in a pond with small frogs. When we consider our present research vignette, we might ask,

\begin{itemize}
\tightlist
\item
  Does the effect of church attendance on ATSS depend only on the individual's own church attendance. Or,
\item
  Does the overall church attendance (``size'' of the pond) also related to ATSS?
\end{itemize}

\textbf{Compositional effects} \citep{enders_centering_2007} involves transforming the natural metric of the score into a group-mean centered (CWC) variable at L1 and a group mean \textbf{aggregate} at L2. Both the CWC/L1 and aggregate/L2 are entered into the MLM.

\begin{itemize}
\tightlist
\item
  When the aggregate is added back in at L2, we get \emph{direct} estimates of both the within- and between- group effects through group-mean centering
\item
  We term it \emph{compositional effects} because it represents the difference between the contextual-level effect and the person-level predictor.
\item
  This is a great strategy when the interest is on distinguishing individual effects of variables (e.g., church attendance) from group-level effects of that same variable (e.g., overall church attendance).
\end{itemize}

Following the Lefevor and colleagues' \citeyearpar{lefevor_homonegativity_2020} example, we will use the \emph{compositional effects} approach with our data. The \emph{group.center()} function in the R package, \emph{robumeta} will group mean center (CWC) variables. All we need to do is identify the clustering variable in our case, ``church.''

Similarly, \emph{robumeta}'s \emph{group.mean} function will aggregate variables at the group's mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(robumeta)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AttendL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Attendance, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AttendL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Attendance, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"church"}\NormalTok{, }\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{n =}\NormalTok{ 30L)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   church      ATSS Attendance     AttendL1 AttendL2 Homogeneity
1       A 4.7835442   6.318674 -1.368556846 7.687231   0.7957395
2       A 5.3851521   9.391428  1.704196481 7.687231   0.7957395
3       A 4.3722317   9.832894  2.145662408 7.687231   0.7957395
4       A 4.8635210   7.721731  0.034499326 7.687231   0.7957395
5       A 4.9733886   9.917289  2.230057976 7.687231   0.7957395
6       A 4.3455429   8.844333  1.157102115 7.687231   0.7957395
7       A 3.5357514   6.630585 -1.056646112 7.687231   0.7957395
8       A 4.1572480   8.111701  0.424469781 7.687231   0.7957395
9       A 4.2946421   9.658424  1.971192960 7.687231   0.7957395
10      A 3.9311877   5.743799 -1.943432538 7.687231   0.7957395
11      A 4.9380802   6.048393 -1.638837820 7.687231   0.7957395
12      A 4.6318423   5.908200 -1.779031167 7.687231   0.7957395
13      A 3.4139728   7.685524 -0.001707147 7.687231   0.7957395
14      A 2.6348562   6.900703 -0.786528003 7.687231   0.7957395
15      A 4.8415811   6.594790 -1.092441414 7.687231   0.7957395
16      B 0.9835024   6.176724 -1.591106297 7.767830   1.1691398
17      B 1.9247771   7.907341  0.139510679 7.767830   1.1691398
18      B 3.7576164   5.469059 -2.298771340 7.767830   1.1691398
19      B 3.6992782   9.262913  1.495083015 7.767830   1.1691398
20      B 2.9125454   9.001514  1.233683676 7.767830   1.1691398
21      B 4.0568240   7.486189 -0.281641399 7.767830   1.1691398
22      B 1.2471085   9.417031  1.649200996 7.767830   1.1691398
23      B 3.2280375   6.066014 -1.701816135 7.767830   1.1691398
24      B 3.7688923   7.413705 -0.354124518 7.767830   1.1691398
25      B 4.0418646   6.047803 -1.720027268 7.767830   1.1691398
26      B 5.4240898   7.157878 -0.609951815 7.767830   1.1691398
27      B 3.8355654   9.687518  1.919687868 7.767830   1.1691398
28      B 2.6657722   5.972512 -1.795317977 7.767830   1.1691398
29      B 2.8502831   9.761905  1.994075223 7.767830   1.1691398
30      B 2.8982202   9.689345  1.921515292 7.767830   1.1691398
\end{verbatim}

If we look again at the first two churches, we can see the

\begin{itemize}
\tightlist
\item
  Natural metric (ATSS, Attendance) which differs for each person across all churches

  \begin{itemize}
  \tightlist
  \item
    This would be an L1 variable
  \end{itemize}
\item
  Group-mean centering (CWC; ATSSL2) which is identifiable because if you added up each of the values in each of the churches, the sum would be zero for each church

  \begin{itemize}
  \tightlist
  \item
    This would be an L1 variable
  \end{itemize}
\item
  Aggregate group mean (AttendL2) which is identifiable because the value is constant across each of the groups

  \begin{itemize}
  \tightlist
  \item
    This would be an L2 variable
  \end{itemize}
\item
  You might notice, I didn't mention the \emph{homogeneity} variable. This is because it was collected and entered as an L2 variable and needs no further centering/transformation. Similarly, we typically leave the dependent variable (\emph{ATSS}) in the natural metric.
\end{itemize}

We can also see the effects of centering in our descriptives.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            vars   n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 225 3.32 1.18   3.43    3.34 1.11 -1.23  6.45  7.69 -0.30
Attendance     2 225 7.52 1.52   7.36    7.47 1.84  5.12 10.35  5.22  0.24
AttendL1       3 225 0.00 1.48  -0.01   -0.02 1.88 -2.93  3.01  5.94  0.10
AttendL2       4 225 7.52 0.32   7.55    7.52 0.33  7.01  8.06  1.05  0.02
Homogeneity    5 225 1.04 0.25   1.14    1.04 0.34  0.69  1.40  0.71 -0.04
            kurtosis   se
ATSS            0.27 0.08
Attendance     -1.19 0.10
AttendL1       -1.14 0.10
AttendL2       -0.93 0.02
Homogeneity    -1.60 0.02
\end{verbatim}

Note that the mean for the ATTSL1 and AttendL1 variables are now zero, while the aggregated group means are equal to the mean of the natural metric.

Looking at the descriptives for each church also helps clarify what we have done.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(ATSS }\SpecialCharTok{+}\NormalTok{ Attendance }\SpecialCharTok{+}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ AttendL2 }\SpecialCharTok{+}\NormalTok{ Homogeneity }\SpecialCharTok{\textasciitilde{}}\NormalTok{ church, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Descriptive statistics by group 
church: A
            vars  n mean   sd median trimmed  mad   min  max range  skew
ATSS           1 15 4.34 0.72   4.37    4.39 0.70  2.63 5.39  2.75 -0.78
Attendance     2 15 7.69 1.52   7.69    7.67 2.03  5.74 9.92  4.17  0.23
AttendL1       3 15 0.00 1.52   0.00   -0.02 2.03 -1.94 2.23  4.17  0.23
AttendL2       4 15 7.69 0.00   7.69    7.69 0.00  7.69 7.69  0.00   NaN
Homogeneity    5 15 0.80 0.00   0.80    0.80 0.00  0.80 0.80  0.00   NaN
            kurtosis   se
ATSS           -0.21 0.19
Attendance     -1.63 0.39
AttendL1       -1.63 0.39
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: B
            vars  n mean   sd median trimmed  mad   min  max range  skew
ATSS           1 15 3.15 1.15   3.23    3.15 0.83  0.98 5.42  4.44 -0.22
Attendance     2 15 7.77 1.59   7.49    7.79 2.24  5.47 9.76  4.29 -0.01
AttendL1       3 15 0.00 1.59  -0.28    0.02 2.24 -2.30 1.99  4.29 -0.01
AttendL2       4 15 7.77 0.00   7.77    7.77 0.00  7.77 7.77  0.00   NaN
Homogeneity    5 15 1.17 0.00   1.17    1.17 0.00  1.17 1.17  0.00   NaN
            kurtosis   se
ATSS           -0.50 0.30
Attendance     -1.75 0.41
AttendL1       -1.75 0.41
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: C
            vars  n mean   sd median trimmed  mad   min  max range skew
ATSS           1 15 2.93 0.92   2.98    2.90 1.14  1.68 4.58  2.91 0.14
Attendance     2 15 7.01 1.01   6.78    6.95 0.87  5.83 8.96  3.13 0.75
AttendL1       3 15 0.00 1.01  -0.22   -0.06 0.87 -1.18 1.95  3.13 0.75
AttendL2       4 15 7.01 0.00   7.01    7.01 0.00  7.01 7.01  0.00  NaN
Homogeneity    5 15 1.22 0.00   1.22    1.22 0.00  1.22 1.22  0.00  NaN
            kurtosis   se
ATSS           -1.20 0.24
Attendance     -0.88 0.26
AttendL1       -0.88 0.26
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: D
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 2.62 1.07   2.79    2.64 1.43  0.88  4.10  3.22 -0.21
Attendance     2 15 8.06 1.85   8.41    8.12 2.27  5.12 10.20  5.08 -0.29
AttendL1       3 15 0.00 1.85   0.36    0.06 2.27 -2.93  2.14  5.08 -0.29
AttendL2       4 15 8.06 0.00   8.06    8.06 0.00  8.06  8.06  0.00   NaN
Homogeneity    5 15 1.18 0.00   1.18    1.18 0.00  1.18  1.18  0.00   NaN
            kurtosis   se
ATSS           -1.54 0.28
Attendance     -1.71 0.48
AttendL1       -1.71 0.48
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: E
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 3.42 0.92   3.57    3.41 0.49  1.70  5.33  3.63 -0.05
Attendance     2 15 8.00 1.70   7.95    8.02 2.27  5.50 10.33  4.82 -0.01
AttendL1       3 15 0.00 1.70  -0.05    0.01 2.27 -2.50  2.32  4.82 -0.01
AttendL2       4 15 8.00 0.00   8.00    8.00 0.00  8.00  8.00  0.00   NaN
Homogeneity    5 15 1.32 0.00   1.32    1.32 0.00  1.32  1.32  0.00   NaN
            kurtosis   se
ATSS           -0.49 0.24
Attendance     -1.56 0.44
AttendL1       -1.56 0.44
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: F
            vars  n mean   sd median trimmed  mad   min  max range skew
ATSS           1 15 3.61 1.04   3.44    3.57 0.96  2.11 5.67  3.56 0.46
Attendance     2 15 7.27 1.18   7.27    7.19 1.04  5.68 9.90  4.22 0.69
AttendL1       3 15 0.00 1.18   0.00   -0.08 1.04 -1.59 2.63  4.22 0.69
AttendL2       4 15 7.27 0.00   7.27    7.27 0.00  7.27 7.27  0.00  NaN
Homogeneity    5 15 0.93 0.00   0.93    0.93 0.00  0.93 0.93  0.00  NaN
            kurtosis   se
ATSS           -0.95 0.27
Attendance     -0.33 0.30
AttendL1       -0.33 0.30
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: G
            vars  n mean   sd median trimmed  mad   min  max range  skew
ATSS           1 15 4.51 0.94   4.49    4.44 1.08  3.48 6.45  2.98  0.65
Attendance     2 15 7.01 0.94   7.18    7.01 1.27  5.50 8.47  2.96 -0.05
AttendL1       3 15 0.00 0.94   0.17    0.00 1.27 -1.50 1.46  2.96 -0.05
AttendL2       4 15 7.01 0.00   7.01    7.01 0.00  7.01 7.01  0.00   NaN
Homogeneity    5 15 0.71 0.00   0.71    0.71 0.00  0.71 0.71  0.00   NaN
            kurtosis   se
ATSS           -0.94 0.24
Attendance     -1.35 0.24
AttendL1       -1.35 0.24
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: H
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 3.55 1.19   3.48    3.55 0.84  1.20  5.83  4.63 -0.17
Attendance     2 15 7.35 1.70   7.57    7.30 2.45  5.13 10.20  5.07  0.23
AttendL1       3 15 0.00 1.70   0.22   -0.05 2.45 -2.22  2.86  5.07  0.23
AttendL2       4 15 7.35 0.00   7.35    7.35 0.00  7.35  7.35  0.00   NaN
Homogeneity    5 15 0.82 0.00   0.82    0.82 0.00  0.82  0.82  0.00   NaN
            kurtosis   se
ATSS           -0.43 0.31
Attendance     -1.46 0.44
AttendL1       -1.46 0.44
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: I
            vars  n mean   sd median trimmed  mad   min   max range skew
ATSS           1 15 2.71 1.13   2.73    2.71 0.83  0.68  4.76  4.08 0.16
Attendance     2 15 7.26 1.63   7.25    7.20 2.26  5.14 10.17  5.03 0.12
AttendL1       3 15 0.00 1.63  -0.01   -0.06 2.26 -2.12  2.90  5.03 0.12
AttendL2       4 15 7.26 0.00   7.26    7.26 0.00  7.26  7.26  0.00  NaN
Homogeneity    5 15 1.28 0.00   1.28    1.28 0.00  1.28  1.28  0.00  NaN
            kurtosis   se
ATSS           -0.61 0.29
Attendance     -1.43 0.42
AttendL1       -1.43 0.42
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: J
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 3.47 0.99   3.81    3.50 1.05  1.67  4.90  3.23 -0.44
Attendance     2 15 7.86 1.89   7.89    7.88 2.89  5.25 10.25  5.00 -0.09
AttendL1       3 15 0.00 1.89   0.03    0.02 2.89 -2.62  2.39  5.00 -0.09
AttendL2       4 15 7.86 0.00   7.86    7.86 0.00  7.86  7.86  0.00   NaN
Homogeneity    5 15 1.14 0.00   1.14    1.14 0.00  1.14  1.14  0.00   NaN
            kurtosis   se
ATSS           -0.94 0.26
Attendance     -1.70 0.49
AttendL1       -1.70 0.49
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: K
            vars  n mean   sd median trimmed  mad   min   max range skew
ATSS           1 15 2.45 1.04   2.53    2.41 1.09  0.92  4.49  3.57 0.30
Attendance     2 15 7.43 1.83   6.73    7.40 2.10  5.31 10.03  4.72 0.15
AttendL1       3 15 0.00 1.83  -0.71   -0.04 2.10 -2.12  2.59  4.72 0.15
AttendL2       4 15 7.43 0.00   7.43    7.43 0.00  7.43  7.43  0.00  NaN
Homogeneity    5 15 1.37 0.00   1.37    1.37 0.00  1.37  1.37  0.00  NaN
            kurtosis   se
ATSS           -0.82 0.27
Attendance     -1.81 0.47
AttendL1       -1.81 0.47
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: L
            vars  n mean   sd median trimmed  mad   min  max range skew
ATSS           1 15 2.87 0.95   2.94    2.82 1.12  1.73 4.67  2.94  0.5
Attendance     2 15 7.68 1.27   7.54    7.69 1.36  5.57 9.71  4.14  0.1
AttendL1       3 15 0.00 1.27  -0.15    0.01 1.36 -2.11 2.03  4.14  0.1
AttendL2       4 15 7.68 0.00   7.68    7.68 0.00  7.68 7.68  0.00  NaN
Homogeneity    5 15 1.40 0.00   1.40    1.40 0.00  1.40 1.40  0.00  NaN
            kurtosis   se
ATSS           -1.06 0.25
Attendance     -1.33 0.33
AttendL1       -1.33 0.33
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: M
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 3.72 0.79   3.93    3.72 0.73  2.34  5.06  2.72 -0.27
Attendance     2 15 7.56 1.52   7.80    7.52 1.94  5.45 10.07  4.63  0.12
AttendL1       3 15 0.00 1.52   0.25   -0.03 1.94 -2.11  2.52  4.63  0.12
AttendL2       4 15 7.56 0.00   7.56    7.56 0.00  7.56  7.56  0.00   NaN
Homogeneity    5 15 0.81 0.00   0.81    0.81 0.00  0.81  0.81  0.00   NaN
            kurtosis   se
ATSS           -1.06 0.20
Attendance     -1.42 0.39
AttendL1       -1.42 0.39
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: N
            vars  n mean   sd median trimmed  mad   min  max range  skew
ATSS           1 15 2.82 1.79   3.11    2.91 1.51 -1.23 5.61  6.84 -0.48
Attendance     2 15 7.55 1.44   7.66    7.55 1.08  5.24 9.79  4.55 -0.10
AttendL1       3 15 0.00 1.44   0.11    0.01 1.08 -2.31 2.24  4.55 -0.10
AttendL2       4 15 7.55 0.00   7.55    7.55 0.00  7.55 7.55  0.00   NaN
Homogeneity    5 15 0.69 0.00   0.69    0.69 0.00  0.69 0.69  0.00   NaN
            kurtosis   se
ATSS           -0.38 0.46
Attendance     -1.27 0.37
AttendL1       -1.27 0.37
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
------------------------------------------------------------ 
church: O
            vars  n mean   sd median trimmed  mad   min   max range  skew
ATSS           1 15 3.65 0.91   3.75    3.67 0.63  1.71  5.28  3.57 -0.67
Attendance     2 15 7.34 1.53   6.68    7.23 1.31  5.70 10.35  4.64  0.58
AttendL1       3 15 0.00 1.53  -0.65   -0.11 1.31 -1.63  3.01  4.64  0.58
AttendL2       4 15 7.34 0.00   7.34    7.34 0.00  7.34  7.34  0.00   NaN
Homogeneity    5 15 0.79 0.00   0.79    0.79 0.00  0.79  0.79  0.00   NaN
            kurtosis   se
ATSS            0.11 0.23
Attendance     -1.16 0.40
AttendL1       -1.16 0.40
AttendL2         NaN 0.00
Homogeneity      NaN 0.00
\end{verbatim}

Tables are produced for each church's data. Again, because of group-mean centering (CWC) the mean of the ATSSL1 and AttendL1 variables are 0. The values of the ATSSL2 and AttendL1 variables equal the natural metric. These, though, are different for each of the churches.

Looking at the correlations between all forms of these variables can further help clarify why the \emph{compositional effects} approach is useful.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Multilevel level correlation matrix}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}
\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{show.conf.interval =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename=}\StringTok{"ML\_CorMatrix.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The ability to suppress reporting of reporting confidence intervals has been deprecated in this version.
The function argument show.conf.interval will be removed in a later version.
\end{verbatim}

\begin{verbatim}

Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable       M     SD   1            2           3           4         
  1. ATSS        3.32  1.18                                                
                                                                           
  2. Attendance  7.52  1.52 .10                                            
                            [-.03, .23]                                    
                                                                           
  3. AttendL1    -0.00 1.48 .12          .98**                             
                            [-.01, .25]  [.97, .98]                        
                                                                           
  4. AttendL2    7.52  0.32 -.10         .21**       -.00                  
                            [-.23, .03]  [.08, .33]  [-.13, .13]           
                                                                           
  5. Homogeneity 1.04  0.25 -.33**       .07         .00         .32**     
                            [-.44, -.21] [-.07, .19] [-.13, .13] [.19, .43]
                                                                           

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

The AttendL2 (aggregated group means) we created correlates with the Attendance (natural metric) version. However, it has ZERO correlation with the AttendL1 (group-mean centered, CWC) version. This means that it effectively and completely separates within- and between-subjects variance. If we enter these both into the MLM prediction equation, we will completely capture the within- and between-subjects contributions of attendance.

The compositional effects approach to representing L1 variables also works well with longitudinal MLM.

\hypertarget{model-building}{%
\subsection{Model Building}\label{model-building}}

Multilevel modelers often approach model building in a systematic and sequential manner. This approach was true for Lefevor and colleagues \citeyearpar{lefevor_homonegativity_2020} who planned a four staged approach, but stopped after three because it appeared that adding the next term would not result in model improvement. The four planned steps include:

\begin{itemize}
\tightlist
\item
  Examining an intercept-only model
\item
  Adding the L1 variables
\item
  Adding the L2 variables
\item
  Adding cross-level interactions
\end{itemize}

\hypertarget{model-1-the-empty-model}{%
\subsubsection{\texorpdfstring{Model 1: The \emph{empty} model}{Model 1: The empty model}}\label{model-1-the-empty-model}}

This preliminary model has several names: unconditional cell means model, one-way ANOVA with random effects, intercept-only model, and empty model. Why? The only variable in the model is the DV. That is, it is a model with no predictors.

As you can see in the script below, we are specifying its intercept (\textasciitilde1). The ``\emph{(1 \textbar{} church)}'' portion of the code indicates there is a random intercept with a fixed mean. That is, the formula acknowledges that the ATSS means will differ across churches. This model will have no slope. That is, each individual score is predicted solely from the mean. In another lecture, I talk about the transition from null hypothesis statistical testing to statistical modeling. In that lecture I reflected on Cumming's \citeyearpar{cumming_new_2014} notion that ``even the mean is a model'' -- that it explains something and not others. In this circumstance, the mean is a model! What is, perhaps, unique about this model is that the code below allows the mean to vary across groups.

There are two packages (\emph{lme4}, \emph{nlme}) that are primarily used for MLM. We are providing the code for both because -- although the core features are identical -- they are slightly different. The \emph{lmerTest} package offers some handy follow-up tests that help us understand our results. Finally, the \emph{tab\_model()} function from the \emph{sjPlot} package will help create a table that is readily usable in an APA style journal article.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{Mod1 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ 1 + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   694.7    705.0   -344.4    688.7      222 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9130 -0.6410  0.0392  0.5764  2.5252 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.2673   0.5171  
 Residual             1.1299   1.0630  
Number of obs: 225, groups:  church, 15

Fixed effects:
            Estimate Std. Error      df t value          Pr(>|t|)    
(Intercept)   3.3214     0.1511 15.0000   21.98 0.000000000000803 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(Mod1) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 694.73
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(Mod1) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 704.9783
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}
\NormalTok{ModB1 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ModB1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed-effects model fit by maximum likelihood
  Data: Lefevor2020 
     AIC      BIC   logLik
  694.73 704.9783 -344.365

Random effects:
 Formula: ~1 | church
        (Intercept) Residual
StdDev:   0.5170587 1.062978

Fixed effects:  ATSS ~ 1 
               Value Std.Error  DF  t-value p-value
(Intercept) 3.321371 0.1514833 210 21.92566       0

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.91296264 -0.64096244  0.03922427  0.57637964  2.52520439 

Number of Observations: 225
Number of Groups: 15 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ModB1) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            numDF denDF  F-value p-value
(Intercept)     1   210 480.7347  <.0001
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmerTest)}
\FunctionTok{ranova}\NormalTok{(Mod1) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ (1 | church)
             npar  logLik    AIC   LRT Df   Pr(>Chisq)    
<none>          3 -344.37 694.73                          
(1 | church)    2 -356.89 717.79 25.06  1 0.0000005558 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(Mod1) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                2.5 %    97.5 %
.sig01      0.3231248 0.8347723
.sigma      0.9688860 1.1733458
(Intercept) 3.0051108 3.6376320
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract Variances to compute R\^{}2}
\NormalTok{  var\_table }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(Mod1))}
\NormalTok{  Mod1\_var\_tot }\OtherTok{=}\NormalTok{ var\_table[}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ var\_table[}\DecValTok{2}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\CommentTok{\# var\_table[1,\textquotesingle{}vcov\textquotesingle{}] = L1 var; var\_table[2,\textquotesingle{}vcov\textquotesingle{}] = L2 var;  }

\FunctionTok{library}\NormalTok{(sjPlot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Learn more about sjPlot with 'browseVignettes("sjPlot")'.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(Mod1, ModB1, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"ModB1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

ModB1

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

3.32

0.15

\textless0.001

3.32

0.15

\textless0.001

Random Effects

2

1.13

1.13

00

0.27 church

0.27 church

ICC

0.19

0.19

N

15 church

15 church

Observations

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.000 / 0.191

Deviance

688.730

688.730

AIC

696.669

694.730

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

It is customary to report MLM models side-by-side for comparison. In this first run, I have extracted the intercept-only models from both the \emph{lmer()} and \emph{nlme()} runs to show that the results are identical. In subsequent runs, I will pull from the \emph{lmer()} models.

The unconditional cell means model is equivalent to a one-factor random effects ANOVA of attitudes toward same-sex sexuality as the sole factor; the 15 churches become the 15 levels of the churches factor.

With the \emph{plot\_model()} function in \emph{sjPlot}, we can plot the random effects. For this intercept-only model, we see the mean and range of the ATSS variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod1, }\AttributeTok{type=}\StringTok{"re"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in checkMatrixPackageVersion(): Package version inconsistency detected.
TMB was built with Matrix version 1.3.4
Current Matrix version is 1.4.0
Please re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-20-1.pdf}

Focusing on the information in viewer, we can first check to see if things look right. We know we have 15 churches, each with 15 observations (225), so our data is reading correctly.

The top of the output includes our \emph{fixed effects}. In this case, we have only the intercept, its standard error, and p value. We see that across all individuals in all churches, the mean (the grand mean) of ATSS is 3.32 (this is consistent with the \emph{M} we saw in descriptives). The values of fixed effects do not vary between L2 units. The \emph{tab\_model} viewer is very customizable; we can ask for different features.

The section of \emph{random effects} is different from OLS. Random effects include \emph{variance components}; these are reported here.

\(\sigma^{2}\) is within-church variance; the pooled scatter of each individual's response around the church's mean.
\(\tau _{00}\) is between-church variance; the scatter of each church's data around the grand mean.
The \emph{intraclass correlation coefficient} (ICC) describes the proportion of variance that lies \emph{between} churches. It is the essential piece of data that we need from this model. Because the total variation in \emph{Y} is just the sum of the within- and between- church variance components, we could have calculated this value from \(\sigma^{2}\) and \(\tau _{00}\). Yet, it is handy that the \emph{lmer()} function does it or us.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{27}\SpecialCharTok{/}\NormalTok{(}\FloatTok{1.13+.27}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1928571
\end{verbatim}

The ICC value of 0.19 means that 19\% of the total variation in attitudes toward same-sex sexuality is attributable to differences between churches. The balance\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{1.00} \SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{19}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.81
\end{verbatim}

\ldots(81\%) is attributable to within-church variation (or differences in people).

We will monitor these variance components to see if the terms we have added reduce the variance. They can provide some sort of guide as to whether the remaining/unaccounted for variance is within-groups (where an L1 variable could help) or between-groups (where an L2 variable might be indicated). As they approach zero, it could be there is nothing left to explain.

\textbf{Marginal} \(R^2\) provides the variance provided only by the fixed effects.

\textbf{Conditional} \(R^2\) provides the variance provided by both the fixed and random effects (i.e., the mean random effect variances). Thus, the conditional \(R^2\) is appropriate or mixed models with random slopes or nested random effects. Already, without any predictors in the model, we have accounted for 19\% of the variance. How is this possible? Our empty model did include the clustering/nesting in churches. This is a random effect.

The \textbf{deviance statistic} compares log-likelihood statistics for two models at a time: (a) the current model and (b) a saturated model (e.g., a more general model that fits the sample data perfectly). Deviance quantifies \emph{how much worse} the current model is in comparison to the best possible model. The deviance is identical to the residual sums of squares used in regression. While you cannot directly interpret any particular deviance statistic, you can compare \emph{nested} models; the smaller value ``wins.'' The deviance statistic has a number of conditions. After we evaluate several models, we can formally test to if the decrease in deviance statistic is statistically significant.

The \emph{AIC} is another fit index. The AIC (Akaike Information Criteria) allows the comparison of the relative \emph{goodness of fit} of models that are not nested. That is, they can involve different sets of parameters. Like the deviance statistic, the AIC is based on the log-likelihood statistic. Instead of using the LL itself, the AIC penalizes (e.g., decreases) the LL based on the number of parameters. Why? Adding parameters (even if they have no effect) \emph{will} increase the LL statistic and decrease the deviance statistic. \emph{As long as two models are fit to the identical same set of data}, the AICs can be compared. The model with the smaller information critera ``wins.'' There are no established criteria for determining how large the difference is for it to matter.

\hypertarget{model-2-adding-the-l1-predictor}{%
\subsubsection{Model 2: Adding the L1 predictor}\label{model-2-adding-the-l1-predictor}}

When we add the L1 predictor(s), we add them in their group-mean centered (CWC) form. In our specific research question, we are asking, ``What effect does an individual's church attendance (relative to the attendance of others at the same church) have on an individual's attitudes toward same-sex sexuality (homonegativity)?

We update our script by:

\begin{itemize}
\tightlist
\item
  Renaming the object (I'm changing from Mod1 to Mod2), including all the places it is used.
\item
  Adding the L1 variable into the \emph{lmer()} models
\item
  Adding ``Mod2'' to the \emph{anova()} function (this will let us know if the models are statistically significantly different from each other)
\item
  Replacing ``ModB1'' with ``Mod2'' in the \emph{tab\_model()} function
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 2}
\NormalTok{Mod2 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL1 + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   692.5    706.2   -342.3    684.5      221 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.1385 -0.6440  0.0700  0.6112  2.4747 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.2688   0.5185  
 Residual             1.1075   1.0524  
Number of obs: 225, groups:  church, 15

Fixed effects:
             Estimate Std. Error        df t value          Pr(>|t|)    
(Intercept)   3.32137    0.15115  15.00000  21.975 0.000000000000803 ***
AttendL1      0.09763    0.04738 210.00000   2.061            0.0406 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
         (Intr)
AttendL1 0.000 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(Mod2) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 692.5259
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(Mod2) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 706.1903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModB2 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{  AttendL1, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ModB2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed-effects model fit by maximum likelihood
  Data: Lefevor2020 
       AIC      BIC    logLik
  692.5259 706.1903 -342.2629

Random effects:
 Formula: ~1 | church
        (Intercept) Residual
StdDev:   0.5185005 1.052391

Fixed effects:  ATSS ~ AttendL1 
               Value  Std.Error  DF   t-value p-value
(Intercept) 3.321371 0.15182255 209 21.876668  0.0000
AttendL1    0.097630 0.04758854 209  2.051538  0.0415
 Correlation: 
         (Intr)
AttendL1 0     

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-4.13849479 -0.64398455  0.07003128  0.61124359  2.47473671 

Number of Observations: 225
Number of Groups: 15 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Mod2) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
         Sum Sq Mean Sq NumDF DenDF F value  Pr(>F)  
AttendL1 4.7032  4.7032     1   210  4.2466 0.04056 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ranova}\NormalTok{(Mod2) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ AttendL1 + (1 | church)
             npar  logLik    AIC    LRT Df   Pr(>Chisq)    
<none>          4 -342.26 692.53                           
(1 | church)    3 -355.20 716.40 25.873  1 0.0000003647 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(Mod2) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                  2.5 %    97.5 %
.sig01      0.325488416 0.8356621
.sigma      0.959236271 1.1616597
(Intercept) 3.005111274 3.6376316
AttendL1    0.004347046 0.1909124
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DevM2 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod1, Mod2) }

\CommentTok{\# Extract Variances to compute R\^{}2}
\NormalTok{  var\_table }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(Mod2))}
\NormalTok{  Mod2\_var\_tot }\OtherTok{=}\NormalTok{ var\_table[}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ var\_table[}\DecValTok{2}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\CommentTok{\# var\_table[1,\textquotesingle{}vcov\textquotesingle{}] = L1 var; var\_table[2,\textquotesingle{}vcov\textquotesingle{}] = L2 var; }

\FunctionTok{tab\_model}\NormalTok{(Mod1, Mod2, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Predictors

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

AttendL1

0.10

0.041

Random Effects

2

1.13

1.11

00

0.27 church

0.27 church

ICC

0.19

0.20

N

15 church

15 church

Observations

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.015 / 0.207

Deviance

688.730

684.526

AIC

696.669

698.719

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Looking at the Viewer we can look a the models side by side. We observe:

\begin{itemize}
\tightlist
\item
  There is now a row that includes our AttendL1 predictor.

  \begin{itemize}
  \tightlist
  \item
    This is a statistically significant predictor.
  \item
    The value of the intercept is interpreted as meaning the ATSS value when all other predictors are 0.00. When an individual (relative to others in their church) increases church attendance by 1 unit, ATSS scores increase by 0.10 units.
  \item
    \(\sigma^{2}\) is an indicator of within-church variance. This value has declined (1.13 to 1.11). Given that we added an L1 (within-church) variable, this is sensible. Because there is within-church variance remaining, we might consider adding another L1 variable.
  \item
    \(\tau _{00}\) is an indicator of between-group variance. This value remains constant at 0.27. Given that AttendL1 was a within-church variable, this is sensible. Because there is between-church variance remaining, we are justified in proceeding to adding L2 variables.
  \item
    The ICC has nudged up, indicating that 20\% of the remaining (unaccounted for) variance is between groups.
  \item
    \textbf{Marginal} \(R^2\), the variance attributed to the fixed effects (in this case, the AttendL1 variable) has increased a smidge.
  \item
    Similarly, \textbf{Conditional} \(R^2\), the variance attributed to both the fixed and random effects has nudged upward.
  \item
    AIC values that are lower indicate a better fitting model. There is no formal way to compare these values, but we see that the Mod2 value is a little lower.
  \item
    If we meet the requirements to do so (listed below) we can formally evaluate the decrease of the \textbf{deviance} statistic by looking at the ANOVA model comparison we specified. The requirements include:

    \begin{itemize}
    \tightlist
    \item
      Identical dataset; there can be no missing or additional observations or variables.
    \item
      The model must be \emph{nested} within the other. Every parameter must be in both models; the difference is in the constraints.
    \item
      If we use FML (we did when we set REML = FALSE), a deviance comparison describes the fit of the entire model (both fixed and random effects). Thus, deviance statistics test hypotheses about any combination of parameters, fixed effects, or variance components.
      ed.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DevM2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: Lefevor2020
Models:
Mod1: ATSS ~ 1 + (1 | church)
Mod2: ATSS ~ AttendL1 + (1 | church)
     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  
Mod1    3 694.73 704.98 -344.37   688.73                       
Mod2    4 692.53 706.19 -342.26   684.53 4.2042  1    0.04032 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Deviance statistics can be formally with a chi-square test. The new value is subtracted from the older value. If the difference is greater than the test critical value associated with the change in degrees of freedom, then the model with the lower deviance value is statistically significantly improved. Our deviance values differed by 4.20 units and this was a statistically significant differnce (\emph{p} = .040).

Plots can help us further understand what is happening. The ``pred'' (predicted values) type of plot from \emph{sjPlot} echoes statistically significant, positive, fixed effect result of individual church attendance (relative to their church attendance) on ATSS (homonegativity).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod2, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{, }\AttributeTok{terms=} \FunctionTok{c}\NormalTok{(}\StringTok{"AttendL1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-25-1.pdf}
MLM is a little different than other statistics in that our evaluation of the statistical assumptions continues through the evaluative process. The diagnostic plots (type = ``diag'') provide a check of the model assumptions. Each of the plots provides some guidance of how to interpret them to see if we have violated the assumptions. In the QQ plots, the points generally track along the lines. In the non-normality of residuals, our distribution approximates the superimposed normal curve. In the homoscedasticity plot, the points are scattered above/blow the line in a reasonably equal amounts with random spread.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod2, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-26-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$church
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-26-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-26-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-26-4.pdf}

With the \emph{plot\_model()} function in \emph{sjPlot}, we can plot the random effects. For this intercept-only model, we see the mean and range of the ATSS variable.

Summarizing what we learned in Mod2:

\begin{itemize}
\tightlist
\item
  An individual's church attendance (relative to others in their church) has a significant effect on homonegativity.
\item
  The addition of this L1 variable accounted for a little of the within-church variance, but there is justification for adding additional L1 variables.
\item
  There appears to be between-church variance. Thus, adding L2 variables is justified.
\item
  The L1 model is an improvement over the empty model.
\end{itemize}

Although Lefevor and colleagues \citeyearpar{lefevor_homonegativity_2020} included more L1 variables (you can choose one or more of them for practice), because the purpose of this is instructional, we will proceed by adding two, L2 variables. The first is the aggregate form of church attendance (AttendL2), the second is an exclusive L2 variable, homogeneity (proportion of Black congregants).

\hypertarget{model-3-adding-the-l2-predictors}{%
\subsubsection{Model 3: Adding the L2 predictors}\label{model-3-adding-the-l2-predictors}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 3}
\NormalTok{Mod3 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ AttendL2 }\SpecialCharTok{+}\NormalTok{ Homogeneity }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   687.5    708.0   -337.8    675.5      219 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.4350 -0.6238  0.0782  0.6362  2.2297 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.1141   0.3378  
 Residual             1.1075   1.0524  
Number of obs: 225, groups:  church, 15

Fixed effects:
             Estimate Std. Error        df t value Pr(>|t|)   
(Intercept)   4.85555    2.70843  15.00000   1.793  0.09320 . 
AttendL1      0.09763    0.04738 210.00000   2.061  0.04056 * 
AttendL2      0.01658    0.37518  15.00000   0.044  0.96533   
Homogeneity  -1.59347    0.47613  15.00000  -3.347  0.00441 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) AttnL1 AttnL2
AttendL1     0.000              
AttendL2    -0.984  0.000       
Homogeneity  0.147  0.000 -0.317
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(Mod3) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 687.5166
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(Mod3) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 708.0132
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModB3 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{  AttendL1 }\SpecialCharTok{+}\NormalTok{  AttendL2 }\SpecialCharTok{+}\NormalTok{ Homogeneity, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   687.5    708.0   -337.8    675.5      219 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.4350 -0.6238  0.0782  0.6362  2.2297 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.1141   0.3378  
 Residual             1.1075   1.0524  
Number of obs: 225, groups:  church, 15

Fixed effects:
             Estimate Std. Error        df t value Pr(>|t|)   
(Intercept)   4.85555    2.70843  15.00000   1.793  0.09320 . 
AttendL1      0.09763    0.04738 210.00000   2.061  0.04056 * 
AttendL2      0.01658    0.37518  15.00000   0.044  0.96533   
Homogeneity  -1.59347    0.47613  15.00000  -3.347  0.00441 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) AttnL1 AttnL2
AttendL1     0.000              
AttendL2    -0.984  0.000       
Homogeneity  0.147  0.000 -0.317
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Mod3) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
             Sum Sq Mean Sq NumDF DenDF F value   Pr(>F)   
AttendL1     4.7032  4.7032     1   210  4.2466 0.040562 * 
AttendL2     0.0022  0.0022     1    15  0.0020 0.965326   
Homogeneity 12.4049 12.4049     1    15 11.2005 0.004415 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ranova}\NormalTok{(Mod3) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
             npar  logLik    AIC    LRT Df Pr(>Chisq)   
<none>          6 -337.76 687.52                        
(1 | church)    5 -341.78 693.57 8.0497  1   0.004551 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(Mod3) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                   2.5 %     97.5 %
.sig01       0.153249528  0.5914827
.sigma       0.959236807  1.1616599
(Intercept) -0.811596834 10.5226949
AttendL1     0.004347107  0.1909123
AttendL2    -0.768446894  0.8016149
Homogeneity -2.589725095 -0.5972141
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devM3 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod1, Mod2, Mod3) }

\FunctionTok{tab\_model}\NormalTok{(Mod1, Mod2, Mod3, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Predictors

Estimates

p

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

4.86

0.074

AttendL1

0.10

0.041

0.10

0.041

AttendL2

0.02

0.965

Homogeneity

-1.59

0.001

Random Effects

2

1.13

1.11

1.11

00

0.27 church

0.27 church

0.11 church

ICC

0.19

0.20

0.09

N

15 church

15 church

15 church

Observations

225

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.015 / 0.207

0.126 / 0.208

Deviance

688.730

684.526

675.517

AIC

696.669

698.719

694.159

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Again, looking at the Viewer we can look at the three models side by side. We observe:

\begin{itemize}
\tightlist
\item
  The intercept changes values. The value of 4.86 is the mean when AttendL1 is average for its church (recall we mean centered it so that 0.0 is the church mean), AttendL2 is the mean across churches, and racial homogeneity is 0.00.\\
\item
  There is now a row that includes our two L2 predictors: AttendL2 (aggregate of AttendL1) and Homogeneity (racial homogeneity of each church).
\item
  AttendL1 remains significant (with the values of the \(B\) and \(p\) remaining the same; but the L2 aggregate does not add in a significant manner
\item
  AttendL2 is not a significant predictor.
\item
  Homogeneity is a significant predictor. For every 1 unit increase in racial homogeneity, ATSS values decrease (i.e., there is a decrease in homonegativity).
\item
  \(\sigma^{2}\) is an indicator of within-church variance. This value declined from Mod1 to Mod2 (1.13 to 1.11), but has held constant. Given that we added an L2 (between-church) variables, this is sensible. Because there is within-church variance remaining, we might consider adding another L1 variable.
\item
  \(\tau _{00}\) is an indicator of between-group variance. This value dropped from 0.27 to .11. Given that AttendL2 and Homogeneity were between-church variables, this is sensible. There is some between church variance remaining.
\item
  The ICC dropped, indicating that 9\% of the remaining (unaccounted for) variance is between groups.
\item
  \textbf{Marginal} \(R^2\), the variance attributed to the fixed effects (in this case, the AttendL1 variable) increased to 13\%.
\item
  Similarly, \textbf{Conditional} \(R^2\), the variance attributed to both the fixed and random effects increased to 21\%.
\item
  AIC values that are lower indicate a better fitting model. There is no formal way to compare these values, but we see that the Mod3 value is lower.
\item
  We can call up the object we created to formally compare the deviance statistics.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devM3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: Lefevor2020
Models:
Mod1: ATSS ~ 1 + (1 | church)
Mod2: ATSS ~ AttendL1 + (1 | church)
Mod3: ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  
Mod1    3 694.73 704.98 -344.37   688.73                       
Mod2    4 692.53 706.19 -342.26   684.53 4.2042  1    0.04032 *
Mod3    6 687.52 708.01 -337.76   675.52 9.0093  2    0.01106 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This repeats the comparison of Mod2 to Mod1 (it was significant). The comparison of Mod3 to Mod2 suggests even more statistically significant improvement. Specifically, \(\chi ^{2}(2) = 9.009, p = .011\)

Let's look at plots. With three predictors, we can examine each of their relations with the dependent variable. Of course they are consistent with the fixed effects:

\begin{itemize}
\tightlist
\item
  as individual church attendance (relative to others in their church) increases, homonegativity increases,
\item
  overall church attendance has no apparent effect on homonegativity, and
\item
  as racial homogeneity increases, homonegativity decreases.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod3, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$AttendL1
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-29-1.pdf}

\begin{verbatim}
$AttendL2
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-29-2.pdf}

\begin{verbatim}
$Homogeneity
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-29-3.pdf}

Because the next phase of model building will include cross-level interactions, let's display this mode by examining the relationship between individual attendance and homonegativity, chunked into clusters that let us also examine the influene of church-level attendance and racial homogeneity. This is not a formal test of an interaction; however, I don't sense that there will be interacting effects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod3, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-30-1.pdf}

Our diagnostic plots continue to support our modeling. In the QQ plots, the points generally track along the lines. In the non-normality of residuals, our distribution approximates the superimposed normal curve. In the homoscedasticity plot, the points are scattered above/blow the line in a reasonably equal amounts with random spread.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod3, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-31-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$church
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-31-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-31-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-31-4.pdf}

Lefevor and colleagues \citep{lefevor_homonegativity_2020} had intended to include interaction terms. However, because only one predictor was significant at each of the individual and congregational levels, their final model did not include interaction effects. I am guessing they tried it and trimmed it out.

We add the interaction term by placing an asterisk between the two variables. There is no need (also no harm in) to enter them separately.

\hypertarget{model-4-adding-a-cross-level-interaction-term}{%
\subsubsection{Model 4: Adding a cross-level interaction term}\label{model-4-adding-a-cross-level-interaction-term}}

In multilevel modeling, we have the opportunity to cross the levels (individual, group) when we specify interactions. In this model, we include an interaction between individual attendance and racial homogeneity of the church.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 4}
\NormalTok{Mod4 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttendL2 }\SpecialCharTok{+}\NormalTok{ AttendL1}\SpecialCharTok{*}\NormalTok{Homogeneity }\SpecialCharTok{+}\NormalTok{(}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL2 + AttendL1 * Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   689.5    713.4   -337.7    675.5      218 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.4652 -0.6250  0.0838  0.6268  2.2495 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.1141   0.3378  
 Residual             1.1073   1.0523  
Number of obs: 225, groups:  church, 15

Fixed effects:
                      Estimate Std. Error        df t value Pr(>|t|)   
(Intercept)            4.85555    2.70843  15.00000   1.793  0.09320 . 
AttendL2               0.01658    0.37518  15.00000   0.044  0.96533   
AttendL1               0.14098    0.21674 210.00000   0.650  0.51612   
Homogeneity           -1.59347    0.47613  15.00000  -3.347  0.00441 **
AttendL1:Homogeneity  -0.04061    0.19816 210.00000  -0.205  0.83782   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) AttnL2 AttnL1 Hmgnty
AttendL2    -0.984                     
AttendL1     0.000  0.000              
Homogeneity  0.147 -0.317  0.000       
AttndL1:Hmg  0.000  0.000 -0.976  0.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(Mod4) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 689.4746
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(Mod4) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 713.3873
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModB3 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{  AttendL2 }\SpecialCharTok{+}\NormalTok{ AttendL1}\SpecialCharTok{*}\NormalTok{Homogeneity, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL2 + AttendL1 * Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   689.5    713.4   -337.7    675.5      218 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.4652 -0.6250  0.0838  0.6268  2.2495 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.1141   0.3378  
 Residual             1.1073   1.0523  
Number of obs: 225, groups:  church, 15

Fixed effects:
                      Estimate Std. Error        df t value Pr(>|t|)   
(Intercept)            4.85555    2.70843  15.00000   1.793  0.09320 . 
AttendL2               0.01658    0.37518  15.00000   0.044  0.96533   
AttendL1               0.14098    0.21674 210.00000   0.650  0.51612   
Homogeneity           -1.59347    0.47613  15.00000  -3.347  0.00441 **
AttendL1:Homogeneity  -0.04061    0.19816 210.00000  -0.205  0.83782   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) AttnL2 AttnL1 Hmgnty
AttendL2    -0.984                     
AttendL1     0.000  0.000              
Homogeneity  0.147 -0.317  0.000       
AttndL1:Hmg  0.000  0.000 -0.976  0.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Mod4) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
                      Sum Sq Mean Sq NumDF DenDF F value   Pr(>F)   
AttendL2              0.0022  0.0022     1    15  0.0020 0.965326   
AttendL1              0.4685  0.4685     1   210  0.4231 0.516125   
Homogeneity          12.4024 12.4024     1    15 11.2005 0.004415 **
AttendL1:Homogeneity  0.0465  0.0465     1   210  0.0420 0.837816   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ranova}\NormalTok{(Mod4) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ AttendL2 + AttendL1 + Homogeneity + (1 | church) + AttendL1:Homogeneity
             npar  logLik    AIC    LRT Df Pr(>Chisq)   
<none>          7 -337.74 689.47                        
(1 | church)    6 -341.76 695.53 8.0536  1   0.004541 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(Mod4) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                          2.5 %     97.5 %
.sig01                0.1533133  0.5914950
.sigma                0.9591409  1.1615437
(Intercept)          -0.8115834 10.5226817
AttendL2             -0.7684450  0.8016131
AttendL1             -0.2857791  0.5677291
Homogeneity          -2.5897227 -0.5972164
AttendL1:Homogeneity -0.4307735  0.3495523
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devM4 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod1, Mod2, Mod3, Mod4) }
\FunctionTok{tab\_model}\NormalTok{(Mod1, Mod2, Mod3, Mod4, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{, }\StringTok{"Mod4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Mod4

Predictors

Estimates

p

Estimates

p

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

4.86

0.074

4.86

0.074

AttendL1

0.10

0.041

0.10

0.041

0.14

0.516

AttendL2

0.02

0.965

0.02

0.965

Homogeneity

-1.59

0.001

-1.59

0.001

AttendL1 * Homogeneity

-0.04

0.838

Random Effects

2

1.13

1.11

1.11

1.11

00

0.27 church

0.27 church

0.11 church

0.11 church

ICC

0.19

0.20

0.09

0.09

N

15 church

15 church

15 church

15 church

Observations

225

225

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.015 / 0.207

0.126 / 0.208

0.126 / 0.208

Deviance

688.730

684.526

675.517

675.475

AIC

696.669

698.719

694.159

697.496

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Again, looking at the Viewer we can look at the four models side by side. We observe:

\begin{itemize}
\tightlist
\item
  Although the B weight increases from 0.10 to 0.14, we lose the significance associated with AttendL1.
\item
  The AttendL1*Homogeneity interaction effect is non-significant.
\item
  There are no changes in our random effects (e.g., \(\sigma^{2}\) and \(\tau _{00}\), nor the \textbf{Marginal} and \textbf{Conditional} \(R^2\)
\item
  The AIC value increases -- meaning that Mod4 is worse than Mod3.
\item
  Examining the formal comparison of Mod4 to Mod3 suggests no statistically significant difference (\(\chi_{2}(1) = 0.838\)).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devM4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: Lefevor2020
Models:
Mod1: ATSS ~ 1 + (1 | church)
Mod2: ATSS ~ AttendL1 + (1 | church)
Mod3: ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
Mod4: ATSS ~ AttendL2 + AttendL1 * Homogeneity + (1 | church)
     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  
Mod1    3 694.73 704.98 -344.37   688.73                       
Mod2    4 692.53 706.19 -342.26   684.53 4.2042  1    0.04032 *
Mod3    6 687.52 708.01 -337.76   675.52 9.0093  2    0.01106 *
Mod4    7 689.47 713.39 -337.74   675.47 0.0420  1    0.83763  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Not surprisingly, our resultant models are consistent with the ``peek'' we took at an interaction term after Mod3.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod4, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{), }\AttributeTok{mdrt.values =} \StringTok{"meansd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-34-1.pdf}
Another way to view this is to request the ``int'' (interaction) model type.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod4, }\AttributeTok{type=}\StringTok{"int"}\NormalTok{, }\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"AttendL1"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{), }\AttributeTok{mdrt.values =} \StringTok{"meansd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-35-1.pdf}
Even though the addition of the interaction term did not improve our model, it does not look like it harmed it. These diagnostics remain consistent with those we saw before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod4, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-36-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$church
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-36-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-36-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-36-4.pdf}

\hypertarget{final-model}{%
\subsection{Final Model}\label{final-model}}

Our analysis is consistent with Lefevor and colleagues' decision to stop after Mod3. Therefore I will ``trim'' the interaction term out.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 3}
\NormalTok{Mod3 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ AttendL2 }\SpecialCharTok{+}\NormalTok{ Homogeneity }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(Mod3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ AttendL1 + AttendL2 + Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   687.5    708.0   -337.8    675.5      219 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.4350 -0.6238  0.0782  0.6362  2.2297 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.1141   0.3378  
 Residual             1.1075   1.0524  
Number of obs: 225, groups:  church, 15

Fixed effects:
             Estimate Std. Error        df t value Pr(>|t|)   
(Intercept)   4.85555    2.70843  15.00000   1.793  0.09320 . 
AttendL1      0.09763    0.04738 210.00000   2.061  0.04056 * 
AttendL2      0.01658    0.37518  15.00000   0.044  0.96533   
Homogeneity  -1.59347    0.47613  15.00000  -3.347  0.00441 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) AttnL1 AttnL2
AttendL1     0.000              
AttendL2    -0.984  0.000       
Homogeneity  0.147  0.000 -0.317
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#AIC(Mod3) \# request AIC}
\CommentTok{\#BIC(Mod3) \# request BIC}
\CommentTok{\#ModB3 \textless{}{-} lme(ATSS \textasciitilde{}  AttendL1 +  AttendL2 + Homogeneity, random = \textasciitilde{} 1|church, method="ML", na.action = na.omit, data =Lefevor2020)}
\CommentTok{\#summary(Mod3)}
\CommentTok{\#anova(Mod3) \# request F{-}tests for fixed effects}
\CommentTok{\#ranova(Mod3) \# request test of random effects}
\CommentTok{\#confint(Mod3) \# request test of random effects (variance displayed as SD)}
\NormalTok{devM3 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod1, Mod2, Mod3) }

\FunctionTok{tab\_model}\NormalTok{(Mod1, Mod2, Mod3, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Model 1"}\NormalTok{, }\StringTok{"Model 2"}\NormalTok{, }\StringTok{"Model 3"}\NormalTok{), }\AttributeTok{file =} \StringTok{"Lefevor\_Table.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

Model 1

Model 2

Model 3

Predictors

Estimates

p

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

4.86

0.074

AttendL1

0.10

0.041

0.10

0.041

AttendL2

0.02

0.965

Homogeneity

-1.59

0.001

Random Effects

2

1.13

1.11

1.11

00

0.27 church

0.27 church

0.11 church

ICC

0.19

0.20

0.09

N

15 church

15 church

15 church

Observations

225

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.015 / 0.207

0.126 / 0.208

Deviance

688.730

684.526

675.517

AIC

696.669

698.719

694.159

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\hypertarget{oh-right-the-formulae}{%
\subsection{Oh right, the Formulae}\label{oh-right-the-formulae}}

Finally, the formulae (yes, plural)

Recall the simplicity of the OLS regresion equation with a single predictor:

\[\hat{Y}_{i} = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i}\]
Where:

\begin{itemize}
\tightlist
\item
  \(\hat{Y}_{i}\) (the outcome) has a subscript ``\(i\)'', indicating that it is predicted for each individual.
\item
  \(\beta_{0}\) is the population \emph{intercept}
\item
  \(\beta_{1}X_{i}\) is the population unstandardized regression \emph{slope}
\item
  \(X\) is a \emph{linear} predictor of \(Y\)
\item
  \(\epsilon_{i}\) is the random error in prediction for case \emph{i}
\end{itemize}

Importantly, the intercept and slope are both \emph{fixed} in a basic linear regression model. You can recognize that the intercept and slope are fixed because they do not include a subscript \(i\) or \(j\) (as we will see later). The lack of a subscript indicates that these parameters each only take on a single value that is meant to represent the entire population intercept or slope, respectively.

Now let's take a look at the most basic \textbf{multilevel model} and compare it to the simple linear regression model above:

\[ Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij} \]
Where\ldots{}

\begin{itemize}
\tightlist
\item
  \(Y_{ij}\) outcome measure for individual \emph{i} in group \emph{j}
\item
  \(X_{ij}\) is the value of the predictor for individual \emph{i} in group \emph{j}
\item
  \(\beta_{0j}\) is the intercept for group \emph{j}
\item
  \(\beta_{1j}\) is the slope for group \emph{j}
\item
  \(\epsilon_{ij}\) is the residual
\end{itemize}

Does it look familiar? The only difference in between this MLM equation and the linear regression equation is that the MLM equation contains more subscripts. As you will see, this \emph{simple} update provides us with much more information. For the purpose of defining the model, let's assume that the subscript \(j\) represents a group of individuals. In the multilevel model, \(i\) can take on any value in \((1, ..., N)\), where \(N\) is the number of individuals in the study. The subscript \(j\) may take on values in \((1, ..., J)\), where \(J\) is the number of groups in the study. In this model, recognize that each group is allowed its own unique intercept and slope. You could read the entire model as:

\emph{``The outcome value for person \(i\) in group \(j\) is equal to the intercept for group \(j\), plus the slope for group \(j\) multiplied by the x-value for person \(i\) in group \(j\), plus some error that cannot be explained by the model for person \(i\) in group \(j\).''}

The errors in \(\epsilon_{ij}\) are typically assumed to be independently and identically distributed \((iid)\) \textasciitilde{}\(N(0, \sigma^2)\).

Level 2 (macro-level) regression equations carry the group structure.

\[\beta _{0j}=\gamma _{00}+\mu _{0j}\]
\[\beta _{1j}=\gamma _{10}+\mu_{1j}\]

\(\beta _{0j}\) models the differences in the group intercepts, predicting the intercept for group \emph{j}

\begin{itemize}
\tightlist
\item
  \(\gamma _{00}\) is the population regression intercept

  \begin{itemize}
  \tightlist
  \item
    the grand mean
  \end{itemize}
\item
  \(\gamma _{00}\) assesses how much group \emph{j} differs from the grand mean

  \begin{itemize}
  \tightlist
  \item
    a measure of variance
  \end{itemize}
\end{itemize}

\(\beta _{1j}\) models the differences in group slopes

\begin{itemize}
\tightlist
\item
  \(\gamma _{10}\) is a fixed or constant population slopes
\item
  \(u_{1j}\) assesses the extent to which group \emph{j}'s slope differs from the \emph{grand slope}.
\end{itemize}

\(\mu _{0j}\) and \(\mu_{1j}\) are the residuals from trying to predict the intercepts and slopes, respectively.

So far it lwe have treated the L1 and L2 equations are treated separately. We combine them to form a single \emph{multilevel} regression equation referred to as the ``mixed model.'' This is Cohen et al.'s \citeyearpar{cohen_applied_2003} rendition. As you view it, you may wonder what happened to the B- or beta weights. There are some curious traditions in MLM. When modeling nesting in groups, some researchers use the \(\gamma\) (``g'' is for group) and when modeling nesting within people, some researchers use the \(\pi\) (``p'' is for people):

\[Y_{ij}=\gamma _{10}X_{ij}+\gamma _{00}+U_{0j}+U_{ij}X_{ij}+r_{ij}\]

\hypertarget{apa-style-writeup}{%
\subsection{APA Style Writeup}\label{apa-style-writeup}}

In this write-up, please presume that the \emph{apa.cor.table} of L1 and L2 variables and the \emph{tab\_model()} table (Mod3) we created will serve as the basis for the APA style tables. I would use one of the Mod3 predictions as the graph.

\textbf{Method/Analytic Strategy}
The nested structure of our data (congregants {[}L1{]} nested within churches {[}L2{]}), multilevel modeling was appropriate because it allows for (a) the dependent nature of the congregants within their churches and (b) varying numbers of church members within each church. We analyzed the data with the \emph{lme4} (v. 1.1-26) in R (v. 4.0.5) using full maximum likelihood. We used a compositional effect \citep{enders_centering_2007} approach to center our variables. Specifically, we used group-mean centering (centering within context) for our L1 variables. Calculating their group aggregate, we entered them back into the model as L2 predictors. This allowed each predictor to completely capture within- and between-group variance.

Model development and evaluation waas approached in a systematic and sequential manner. This exploratory approach is consistent with recommendations to pursue model generating approaches in complex models \citep{bollen_testing_1993} by first understanding the relatively simpler relations between the variables \citep[e.g.,][]{hancock_hierarchical_2010, petscher_linear_2013} and assessing the viability of more complexity based on the results. Accordingly, we began with an intercept-only model. We followed sequentially by entering L1 (Model 2), L2 (Model 3), and a cross-level interaction (Model 4). Throughout we monitored variance components and fit decisions to determine our final model.

\textbf{Results}

\textbf{Preliminary Analyses}

\begin{itemize}
\tightlist
\item
  Missing data analysis and managing missing data
\item
  Bivariate correlations, means, SDs
\item
  Address assumptions; in MLM this includes

  \begin{itemize}
  \tightlist
  \item
    linearity
  \item
    homogeneity of variance
  \item
    normal distribution of the model's residuals
  \end{itemize}
\item
  Address any apriorily known limitations and concerns
\end{itemize}

\textbf{Primary Analyses}

Table \# reports the the bivariate correlations between ATSS/homonegative attitudes and the L1 and L2 predictors. Our first model was an intercept-only, ``empty'', model with ATSS/homonegative attitudes as the dependent variable and no predictors in the model. The intraclass correlation (ICC) suggested that 19\% of the variance in homonegative attitudes was between congregations; correspondingly, 81\% was within congregations (i.e., between individuals).

We added the L1 predictor of individual church attendance in the second model. As shown in Table \#, there was a significant effect such that as individual church attendance increased, so did homonegative attitudes. We added the L2 variables of the aggregate form of church attendance and racial homogeneity in our third model. The L2 form of church attendance had a non-significant effect, however racial homogeneity was significant. Specifically, as homogeneity increased, homonegativity decreased; this relationship is illustrated in Figure \#. Our fourth model (not shown) included a cross-level interaction between individual church attendance and homogeneity. Because it was non-significant, made no changes in the variance components, and caused the AIC to increase, we trimmed it from the model. Thus, Model 3 is our final model. Further support for this model is noted by the corresponding decreases in \(\sigma^{2}\) and \(\tau _{00}\) when L1 and L2 variables were added, respectively. Additionally, marginal and conditional \(R^2\) increased and formal evaluation of the deviance statistic suggested that each addition was a statistically significant improvement.

\hypertarget{a-conversation-with-dr.-lefevor}{%
\section{A Conversation with Dr.~Lefevor}\label{a-conversation-with-dr.-lefevor}}

Doctoral student (and student in one of my classes) Taniyah Roach and I were able to interview the first author (Tyler Lefevor, PhD) about the article and what it means. Here's a direct \href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c13bb0e0-8b69-41ab-b53d-aeb001440ea7}{link} to that interview.

Among others, we asked:

\begin{itemize}
\tightlist
\item
  What were unexpected challenges to the research method or statistical analysis?
\item
  Retrospectively, what additional variables might you like to have included?
\item
  As a Principal Investigator you are a White male. How did you think about (and maybe what did you do) to see socially and culturally responsive to this particular topics which was focused on the Black church?
\item
  How do you expect the article to change science, practice, and/or advocacy?
\end{itemize}

\hypertarget{practice-problems}{%
\section{Practice Problems}\label{practice-problems}}

The suggested practice problem for this chapter is to conduct a MLM that includes at least one L1 predictor, at least one L2 predictor, and a cross-level interaction.

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation, then rework the problem. This should provide minor changes to the data (maybe in the second or third decimal point), but the results will likely be very similar.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Use a compositional effects approach to centering to group-mean center the L1 variables and then ``bring back'' their aggregate as an L2 variable & 5 & \_\_\_\_\_ \\
3. Model 1: empty model & 5 & \_\_\_\_\_ \\
4. Model 2: L1 predictors & 5 & \_\_\_\_\_ \\
5. Model 3: L2 predictors & 5 & \_\_\_\_\_ \\
6. Model 4: A cross-level interaction & 5 & \_\_\_\_\_ \\
7. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
8. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
9. APA Style writeup & 5 & \_\_\_\_\_ \\
10. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-2-rework-the-research-vignette-but-swap-one-or-more-variables}{%
\subsection{Problem \#2: Rework the research vignette, but swap one or more variables}\label{problem-2-rework-the-research-vignette-but-swap-one-or-more-variables}}

The research vignette analyzes a number of variables, simultaneously. We selected only two for the example. Swap out one or more variables in the multilevel model and compare your solution to the one in the chapter (and/or oNe you mimicked in the journal article). If you wish to increase your probability of finding statistically significant effects, look for hints in Table 2 of the \citep{lefevor_homonegativity_2020} research article that sources the vignettes by selecting a variable(s) with a significant relationship with your DV.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Use a compositional effects approach to centering to group-mean center the L1 variables and then ``bring back'' their aggregate as an L2 variable & 5 & \_\_\_\_\_ \\
3. Model 1: empty model & 5 & \_\_\_\_\_ \\
4. Model 2: L1 predictors & 5 & \_\_\_\_\_ \\
5. Model 3: L2 predictors & 5 & \_\_\_\_\_ \\
6. Model 4: A cross-level interaction & 5 & \_\_\_\_\_ \\
7. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
8. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
9. APA Style writeup & 5 & \_\_\_\_\_ \\
10. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-3-use-other-data-that-is-available-to-you}{%
\subsection{Problem \#3: Use other data that is available to you}\label{problem-3-use-other-data-that-is-available-to-you}}

Conduct a multilevel model with data to which you have access. This could include data you simulate on your own or from a published article.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Use a compositional effects approach to centering to group-mean center the L1 variables and then ``bring back'' their aggregate as an L2 variable & 5 & \_\_\_\_\_ \\
3. Model 1: empty model & 5 & \_\_\_\_\_ \\
4. Model 2: L1 predictors & 5 & \_\_\_\_\_ \\
5. Model 3: L2 predictors & 5 & \_\_\_\_\_ \\
6. Model 4: A cross-level interaction & 5 & \_\_\_\_\_ \\
7. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
8. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
9. APA Style writeup & 5 & \_\_\_\_\_ \\
10. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{bonus-track}{%
\section{Bonus Track:}\label{bonus-track}}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.45833in,height=2.19792in]{images/film-strip-1.jpg}
\caption{Image of a filmstrip}\label{id}
}
\end{figure}

\hypertarget{working-the-entire-vignette}{%
\subsection{Working the Entire Vignette}\label{working-the-entire-vignette}}

Below is the script that works the entire vignette in the Lefevor et al. \citeyearpar{lefevor_homonegativity_2020} article.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{pairs.panels}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Female0"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Education"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{stars =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-38-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Female0"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Education"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            vars   n  mean    sd median trimmed   mad   min   max range  skew
ATSS           1 225  3.32  1.18   3.43    3.34  1.11 -1.23  6.45  7.69 -0.30
Female0        2 225  0.24  0.43   0.00    0.18  0.00  0.00  1.00  1.00  1.21
Age            3 225 51.78 24.62  52.80   52.15 30.05  6.91 93.76 86.85 -0.11
Education      4 225  4.37  2.35   4.62    4.42  2.76  0.01  8.44  8.43 -0.18
Attendance     5 225  7.52  1.52   7.36    7.47  1.84  5.12 10.35  5.22  0.24
Homogeneity    6 225  1.04  0.25   1.14    1.04  0.34  0.69  1.40  0.71 -0.04
            kurtosis   se
ATSS            0.27 0.08
Female0        -0.54 0.03
Age            -1.11 1.64
Education      -1.06 0.16
Attendance     -1.19 0.10
Homogeneity    -1.60 0.02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Single level correlation matrix}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}
\StringTok{"ATSS"}\NormalTok{, }\StringTok{"Female0"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Education"}\NormalTok{, }\StringTok{"Attendance"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{show.conf.interval =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename=}\StringTok{"CorMatrix.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The ability to suppress reporting of reporting confidence intervals has been deprecated in this version.
The function argument show.conf.interval will be removed in a later version.
\end{verbatim}

\begin{verbatim}

Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable       M     SD    1            2           3           4          
  1. ATSS        3.32  1.18                                                  
                                                                             
  2. Female0     0.24  0.43  -.04                                            
                             [-.17, .09]                                     
                                                                             
  3. Age         51.78 24.62 .06          -.03                               
                             [-.07, .19]  [-.16, .10]                        
                                                                             
  4. Education   4.37  2.35  -.07         .12         -.01                   
                             [-.20, .06]  [-.01, .25] [-.14, .12]            
                                                                             
  5. Attendance  7.52  1.52  .10          .03         .02         .24**      
                             [-.03, .23]  [-.10, .16] [-.11, .15] [.11, .36] 
                                                                             
  6. Homogeneity 1.04  0.25  -.33**       .09         -.07        .10        
                             [-.44, -.21] [-.04, .22] [-.19, .07] [-.03, .23]
                                                                             
  5          
             
             
             
             
             
             
             
             
             
             
             
             
             
             
  .07        
  [-.07, .19]
             

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(robumeta)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{ATSSL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{ATSS, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{ATSSL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{ATSS, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AttendL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Attendance, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AttendL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Attendance, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AgeL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Age, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{AgeL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Age, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{GenderL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{GenderL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Female0, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{EducL1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.center}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Education, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#centered within context (group mean centering)}
\NormalTok{Lefevor2020}\SpecialCharTok{$}\NormalTok{EducL2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2020}\SpecialCharTok{$}\NormalTok{Education, Lefevor2020}\SpecialCharTok{$}\NormalTok{church))}\CommentTok{\#aggregated at group mean}
\end{Highlighting}
\end{Shaded}

CALCULATE L1 AND L2 ATTS

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Multilevel level correlation matrix}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Lefevor2020[}\FunctionTok{c}\NormalTok{(}
\StringTok{"ATSSL1"}\NormalTok{, }\StringTok{"GenderL1"}\NormalTok{, }\StringTok{"AgeL1"}\NormalTok{, }\StringTok{"EducL1"}\NormalTok{, }\StringTok{"AttendL1"}\NormalTok{,}
\StringTok{"ATSSL2"}\NormalTok{,}\StringTok{"GenderL2"}\NormalTok{, }\StringTok{"AgeL2"}\NormalTok{, }\StringTok{"EducL2"}\NormalTok{, }\StringTok{"AttendL2"}\NormalTok{, }\StringTok{"Homogeneity"}\NormalTok{)], }\AttributeTok{show.conf.interval =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename=}\StringTok{"ML\_CorMatrix.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The ability to suppress reporting of reporting confidence intervals has been deprecated in this version.
The function argument show.conf.interval will be removed in a later version.
\end{verbatim}

\begin{verbatim}

Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable        M     SD    1           2           3           4          
  1. ATSSL1       0.00  1.03                                                 
                                                                             
  2. GenderL1     0.00  0.41  -.01                                           
                              [-.14, .12]                                    
                                                                             
  3. AgeL1        0.00  24.07 .05         -.04                               
                              [-.08, .18] [-.17, .09]                        
                                                                             
  4. EducL1       -0.00 2.25  .01         .09         .00                    
                              [-.12, .14] [-.04, .22] [-.13, .13]            
                                                                             
  5. AttendL1     -0.00 1.48  .14*        .02         .04         .22**      
                              [.01, .27]  [-.11, .15] [-.09, .17] [.09, .34] 
                                                                             
  6. ATSSL2       3.32  0.59  .00         .00         -.00        -.00       
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  7. GenderL2     0.24  0.13  -.00        -.00        -.00        -.00       
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  8. AgeL2        51.78 5.19  .00         -.00        -.00        .00        
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  9. EducL2       4.37  0.69  -.00        .00         -.00        .00        
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  10. AttendL2    7.52  0.32  -.00        .00         .00         -.00       
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  11. Homogeneity 1.04  0.25  -.00        .00         .00         -.00       
                              [-.13, .13] [-.13, .13] [-.13, .13] [-.13, .13]
                                                                             
  5           6            7          8            9          10        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        
  -.00                                                                  
  [-.13, .13]                                                           
                                                                        
  -.00        -.17**                                                    
  [-.13, .13] [-.30, -.04]                                              
                                                                        
  .00         .15*         .22**                                        
  [-.13, .13] [.02, .28]   [.10, .34]                                   
                                                                        
  .00         -.51**       .39**      -.15*                             
  [-.13, .13] [-.60, -.41] [.27, .49] [-.27, -.02]                      
                                                                        
  -.00        -.20**       .16*       -.32**       .45**                
  [-.13, .13] [-.33, -.08] [.03, .28] [-.44, -.20] [.34, .55]           
                                                                        
  .00         -.67**       .31**      -.31**       .34**      .32**     
  [-.13, .13] [-.74, -.59] [.19, .43] [-.42, -.19] [.22, .45] [.19, .43]
                                                                        

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}

\NormalTok{ATSSm1 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ 1 + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   694.7    705.0   -344.4    688.7      222 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9130 -0.6410  0.0392  0.5764  2.5252 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.2673   0.5171  
 Residual             1.1299   1.0630  
Number of obs: 225, groups:  church, 15

Fixed effects:
            Estimate Std. Error      df t value          Pr(>|t|)    
(Intercept)   3.3214     0.1511 15.0000   21.98 0.000000000000803 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(ATSSm1) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 694.73
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(ATSSm1) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 704.9783
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}
\NormalTok{ATSSmb1 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSmb1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed-effects model fit by maximum likelihood
  Data: Lefevor2020 
     AIC      BIC   logLik
  694.73 704.9783 -344.365

Random effects:
 Formula: ~1 | church
        (Intercept) Residual
StdDev:   0.5170587 1.062978

Fixed effects:  ATSS ~ 1 
               Value Std.Error  DF  t-value p-value
(Intercept) 3.321371 0.1514833 210 21.92566       0

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.91296264 -0.64096244  0.03922427  0.57637964  2.52520439 

Number of Observations: 225
Number of Groups: 15 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ATSSmb1) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            numDF denDF  F-value p-value
(Intercept)     1   210 480.7347  <.0001
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmerTest)}
\FunctionTok{ranova}\NormalTok{(ATSSm1) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ (1 | church)
             npar  logLik    AIC   LRT Df   Pr(>Chisq)    
<none>          3 -344.37 694.73                          
(1 | church)    2 -356.89 717.79 25.06  1 0.0000005558 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(ATSSm1) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                2.5 %    97.5 %
.sig01      0.3231248 0.8347723
.sigma      0.9688860 1.1733458
(Intercept) 3.0051108 3.6376320
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract Variances to compute R\^{}2}
\NormalTok{  var\_table }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(ATSSm1))}
\NormalTok{  ATSSm1\_var\_tot }\OtherTok{=}\NormalTok{ var\_table[}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ var\_table[}\DecValTok{2}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\CommentTok{\# var\_table[1,\textquotesingle{}vcov\textquotesingle{}] = L1 var; var\_table[2,\textquotesingle{}vcov\textquotesingle{}] = L2 var;  }

\FunctionTok{library}\NormalTok{(sjPlot)}

\FunctionTok{tab\_model}\NormalTok{(ATSSm1, ATSSmb1, }\AttributeTok{p.style =} \StringTok{"stars"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"ATSSm1"}\NormalTok{, }\StringTok{"ATSSmb1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

ATSSm1

ATSSmb1

Predictors

Estimates

std. Error

CI

Estimates

std. Error

CI

(Intercept)

3.32 ***

0.15

-Inf~--~Inf

3.32 ***

0.15

-Inf~--~Inf

Random Effects

2

1.13

1.13

00

0.27 church

0.27 church

ICC

0.19

0.19

N

15 church

15 church

Observations

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.000 / 0.191

Deviance

688.730

688.730

AIC

696.669

694.730

\begin{itemize}
\tightlist
\item
  p\textless0.05~~~** p\textless0.01~~~*** p\textless0.001
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 2}
\NormalTok{ATSSm2 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ GenderL1 }\SpecialCharTok{+}\NormalTok{ AgeL1 }\SpecialCharTok{+}\NormalTok{ EducL1 }\SpecialCharTok{+}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   697.9    721.8   -341.9    683.9      218 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.1484 -0.6541  0.0720  0.6251  2.4179 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.2691   0.5187  
 Residual             1.1040   1.0507  
Number of obs: 225, groups:  church, 15

Fixed effects:
              Estimate Std. Error         df t value          Pr(>|t|)    
(Intercept)   3.321371   0.151146  15.000000  21.975 0.000000000000803 ***
GenderL1     -0.032468   0.172588 210.000000  -0.188            0.8510    
AgeL1         0.002031   0.002922 210.000000   0.695            0.4878    
EducL1       -0.011469   0.032158 210.000000  -0.357            0.7217    
AttendL1      0.100379   0.048559 210.000000   2.067            0.0399 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
         (Intr) GndrL1 AgeL1  EducL1
GenderL1  0.000                     
AgeL1     0.000  0.044              
EducL1    0.000 -0.091  0.002       
AttendL1  0.000 -0.003 -0.041 -0.222
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(ATSSm2) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 697.8517
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(ATSSm2) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 721.7644
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ATSSmb2 }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{  GenderL1 }\SpecialCharTok{+}\NormalTok{ AgeL1 }\SpecialCharTok{+}\NormalTok{ EducL1 }\SpecialCharTok{+}\NormalTok{ AttendL1, }\AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\SpecialCharTok{|}\NormalTok{church, }\AttributeTok{method=}\StringTok{"ML"}\NormalTok{, }\AttributeTok{na.action =}\NormalTok{ na.omit, }\AttributeTok{data =}\NormalTok{Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSmb2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed-effects model fit by maximum likelihood
  Data: Lefevor2020 
       AIC      BIC    logLik
  697.8517 721.7644 -341.9259

Random effects:
 Formula: ~1 | church
        (Intercept) Residual
StdDev:   0.5187287 1.050703

Fixed effects:  ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 
                Value  Std.Error  DF   t-value p-value
(Intercept)  3.321371 0.15285420 206 21.729017  0.0000
GenderL1    -0.032468 0.17453771 206 -0.186024  0.8526
AgeL1        0.002031 0.00295477 206  0.687232  0.4927
EducL1      -0.011469 0.03252185 206 -0.352657  0.7247
AttendL1     0.100379 0.04910769 206  2.044049  0.0422
 Correlation: 
         (Intr) GndrL1 AgeL1  EducL1
GenderL1  0.000                     
AgeL1     0.000  0.044              
EducL1    0.000 -0.091  0.002       
AttendL1  0.000 -0.003 -0.041 -0.222

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-4.14841995 -0.65405081  0.07201772  0.62509925  2.41789073 

Number of Observations: 225
Number of Groups: 15 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ATSSm2) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
         Sum Sq Mean Sq NumDF DenDF F value  Pr(>F)  
GenderL1 0.0391  0.0391     1   210  0.0354 0.85096  
AgeL1    0.5332  0.5332     1   210  0.4830 0.48783  
EducL1   0.1404  0.1404     1   210  0.1272 0.72172  
AttendL1 4.7174  4.7174     1   210  4.2731 0.03995 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ranova}\NormalTok{(ATSSm2) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + (1 | church)
             npar  logLik    AIC    LRT Df   Pr(>Chisq)    
<none>          7 -341.93 697.85                           
(1 | church)    6 -354.93 721.86 26.005  1 0.0000003406 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(ATSSm2) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                   2.5 %      97.5 %
.sig01       0.325859422 0.835802678
.sigma       0.957698392 1.159796802
(Intercept)  3.005112413 3.637630470
GenderL1    -0.372286324 0.307349771
AgeL1       -0.003722209 0.007783432
EducL1      -0.074787815 0.051849686
AttendL1     0.004767776 0.195989247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ATSSmb1, ATSSmb2) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Model df      AIC      BIC    logLik   Test L.Ratio p-value
ATSSmb1     1  3 694.7300 704.9783 -344.3650                       
ATSSmb2     2  7 697.8517 721.7644 -341.9259 1 vs 2 4.87833     0.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract Variances to compute R\^{}2}
\NormalTok{  var\_table }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(ATSSm2))}
\NormalTok{  m2\_var\_tot }\OtherTok{=}\NormalTok{ var\_table[}\DecValTok{1}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ var\_table[}\DecValTok{2}\NormalTok{,}\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\CommentTok{\# var\_table[1,\textquotesingle{}vcov\textquotesingle{}] = L1 var; var\_table[2,\textquotesingle{}vcov\textquotesingle{}] = L2 var; }

\FunctionTok{tab\_model}\NormalTok{(ATSSm1, ATSSm2, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"ATSSm1"}\NormalTok{, }\StringTok{"ATSSm2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

ATSSm1

ATSSm2

Predictors

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

GenderL1

-0.03

0.851

AgeL1

0.00

0.488

EducL1

-0.01

0.722

AttendL1

0.10

0.040

Random Effects

2

1.13

1.10

00

0.27 church

0.27 church

ICC

0.19

0.20

N

15 church

15 church

Observations

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.017 / 0.210

Deviance

688.730

683.852

AIC

696.669

720.557

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MODEL 3}
\NormalTok{ATSSm3 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ATSS }\SpecialCharTok{\textasciitilde{}}\NormalTok{ GenderL1 }\SpecialCharTok{+}\NormalTok{ AgeL1 }\SpecialCharTok{+}\NormalTok{ EducL1 }\SpecialCharTok{+}\NormalTok{ AttendL1 }\SpecialCharTok{+}\NormalTok{ GenderL2 }\SpecialCharTok{+}\NormalTok{ AgeL2 }\SpecialCharTok{+}\NormalTok{ EducL2 }\SpecialCharTok{+}\NormalTok{ AttendL2 }\SpecialCharTok{+}\NormalTok{ Homogeneity }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ church), }\AttributeTok{REML=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSm3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + GenderL2 + AgeL2 +  
    EducL2 + AttendL2 + Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   694.4    735.4   -335.2    670.4      213 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5681 -0.6456  0.1041  0.6341  2.1664 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.06591  0.2567  
 Residual             1.10398  1.0507  
Number of obs: 225, groups:  church, 15

Fixed effects:
              Estimate Std. Error         df t value Pr(>|t|)   
(Intercept)   5.289974   3.064703  15.000000   1.726  0.10486   
GenderL1     -0.032468   0.172588 210.000000  -0.188  0.85096   
AgeL1         0.002031   0.002922 210.000000   0.695  0.48783   
EducL1       -0.011469   0.032158 210.000000  -0.357  0.72172   
AttendL1      0.100379   0.048559 210.000000   2.067  0.03995 * 
GenderL2      1.018112   0.923092  15.000000   1.103  0.28744   
AgeL2        -0.014487   0.022000  15.000000  -0.659  0.52018   
EducL2       -0.377632   0.170601  15.000000  -2.214  0.04277 * 
AttendL2      0.243757   0.362136  15.000000   0.673  0.51112   
Homogeneity  -1.580202   0.458259  15.000000  -3.448  0.00358 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) GndrL1 AgeL1  EducL1 AttnL1 GndrL2 AgeL2  EducL2 AttnL2
GenderL1     0.000                                                        
AgeL1        0.000  0.044                                                 
EducL1       0.000 -0.091  0.002                                          
AttendL1     0.000 -0.003 -0.041 -0.222                                   
GenderL2     0.235  0.000  0.000  0.000  0.000                            
AgeL2       -0.634  0.000  0.000  0.000  0.000 -0.388                     
EducL2       0.083  0.000  0.000  0.000  0.000 -0.317  0.075              
AttendL2    -0.877  0.000  0.000  0.000  0.000 -0.041  0.249 -0.352       
Homogeneity -0.136  0.000  0.000  0.000  0.000 -0.317  0.330 -0.119 -0.102
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(ATSSm3) }\CommentTok{\# request AIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 694.372
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(ATSSm3) }\CommentTok{\# request BIC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 735.3652
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#running the nlme ogtained an error indicating there is singularity in the variables}
\CommentTok{\#the stack exchange conversation referenced below indicates that nlme/lmer is sensitive to this}
\CommentTok{\# https://stackoverflow.com/questions/50505290/singularity{-}in{-}backsolve{-}at{-}level{-}0{-}block{-}1{-}in{-}lme{-}model }
\CommentTok{\#ATSSmb3 \textless{}{-} lme(ATSS \textasciitilde{}  Female0L1 + AgeL1 + EducL1 + AttendL1 + Female0L2 + AgeL2 + EducL2 + AttendL2 + Homogeneity, random = \textasciitilde{} 1|church, method="ML", na.action = na.omit, data =Lefevor2020)}
\FunctionTok{summary}\NormalTok{(ATSSm3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + GenderL2 + AgeL2 +  
    EducL2 + AttendL2 + Homogeneity + (1 | church)
   Data: Lefevor2020

     AIC      BIC   logLik deviance df.resid 
   694.4    735.4   -335.2    670.4      213 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5681 -0.6456  0.1041  0.6341  2.1664 

Random effects:
 Groups   Name        Variance Std.Dev.
 church   (Intercept) 0.06591  0.2567  
 Residual             1.10398  1.0507  
Number of obs: 225, groups:  church, 15

Fixed effects:
              Estimate Std. Error         df t value Pr(>|t|)   
(Intercept)   5.289974   3.064703  15.000000   1.726  0.10486   
GenderL1     -0.032468   0.172588 210.000000  -0.188  0.85096   
AgeL1         0.002031   0.002922 210.000000   0.695  0.48783   
EducL1       -0.011469   0.032158 210.000000  -0.357  0.72172   
AttendL1      0.100379   0.048559 210.000000   2.067  0.03995 * 
GenderL2      1.018112   0.923092  15.000000   1.103  0.28744   
AgeL2        -0.014487   0.022000  15.000000  -0.659  0.52018   
EducL2       -0.377632   0.170601  15.000000  -2.214  0.04277 * 
AttendL2      0.243757   0.362136  15.000000   0.673  0.51112   
Homogeneity  -1.580202   0.458259  15.000000  -3.448  0.00358 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) GndrL1 AgeL1  EducL1 AttnL1 GndrL2 AgeL2  EducL2 AttnL2
GenderL1     0.000                                                        
AgeL1        0.000  0.044                                                 
EducL1       0.000 -0.091  0.002                                          
AttendL1     0.000 -0.003 -0.041 -0.222                                   
GenderL2     0.235  0.000  0.000  0.000  0.000                            
AgeL2       -0.634  0.000  0.000  0.000  0.000 -0.388                     
EducL2       0.083  0.000  0.000  0.000  0.000 -0.317  0.075              
AttendL2    -0.877  0.000  0.000  0.000  0.000 -0.041  0.249 -0.352       
Homogeneity -0.136  0.000  0.000  0.000  0.000 -0.317  0.330 -0.119 -0.102
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ATSSm3) }\CommentTok{\# request F{-}tests for fixed effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
             Sum Sq Mean Sq NumDF DenDF F value   Pr(>F)   
GenderL1     0.0391  0.0391     1   210  0.0354 0.850959   
AgeL1        0.5332  0.5332     1   210  0.4830 0.487825   
EducL1       0.1404  0.1404     1   210  0.1272 0.721718   
AttendL1     4.7174  4.7174     1   210  4.2731 0.039946 * 
GenderL2     1.3430  1.3430     1    15  1.2165 0.287436   
AgeL2        0.4787  0.4787     1    15  0.4337 0.520182   
EducL2       5.4092  5.4092     1    15  4.8998 0.042775 * 
AttendL2     0.5002  0.5002     1    15  0.4531 0.511117   
Homogeneity 13.1269 13.1269     1    15 11.8906 0.003585 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ranova}\NormalTok{(ATSSm3) }\CommentTok{\# request test of random effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA-like table for random-effects: Single term deletions

Model:
ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + GenderL2 + AgeL2 + EducL2 + AttendL2 + Homogeneity + (1 | church)
             npar  logLik    AIC   LRT Df Pr(>Chisq)  
<none>         12 -335.19 694.37                      
(1 | church)   11 -336.91 695.83 3.455  1    0.06306 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(ATSSm3) }\CommentTok{\# request test of random effects (variance displayed as SD)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Computing profile confidence intervals ...
\end{verbatim}

\begin{verbatim}
                   2.5 %       97.5 %
.sig01       0.000000000  0.490967315
.sigma       0.957698031  1.159796607
(Intercept) -1.122609232 11.702556036
GenderL1    -0.372286415  0.307349862
AgeL1       -0.003722211  0.007783434
EducL1      -0.074787832  0.051849703
AttendL1     0.004767750  0.195989272
GenderL2    -0.913365344  2.949589723
AgeL2       -0.060519620  0.031544903
EducL2      -0.734596215 -0.020667110
AttendL2    -0.513976611  1.001489745
Homogeneity -2.539062899 -0.621340312
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(ATSSm1, ATSSm2, ATSSm3) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: Lefevor2020
Models:
ATSSm1: ATSS ~ 1 + (1 | church)
ATSSm2: ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + (1 | church)
ATSSm3: ATSS ~ GenderL1 + AgeL1 + EducL1 + AttendL1 + GenderL2 + AgeL2 + EducL2 + AttendL2 + Homogeneity + (1 | church)
       npar    AIC    BIC  logLik deviance   Chisq Df Pr(>Chisq)  
ATSSm1    3 694.73 704.98 -344.37   688.73                        
ATSSm2    7 697.85 721.76 -341.93   683.85  4.8783  4    0.30001  
ATSSm3   12 694.37 735.37 -335.19   670.37 13.4797  5    0.01928 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(ATSSm1, ATSSm2, ATSSm3, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"ATSSm1"}\NormalTok{, }\StringTok{"ATSSm2"}\NormalTok{, }\StringTok{"ATSSm3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

ATSSm1

ATSSm2

ATSSm3

Predictors

Estimates

p

Estimates

p

Estimates

p

(Intercept)

3.32

\textless0.001

3.32

\textless0.001

5.29

0.086

GenderL1

-0.03

0.851

-0.03

0.851

AgeL1

0.00

0.488

0.00

0.488

EducL1

-0.01

0.722

-0.01

0.722

AttendL1

0.10

0.040

0.10

0.040

GenderL2

1.02

0.271

AgeL2

-0.01

0.511

EducL2

-0.38

0.028

AttendL2

0.24

0.502

Homogeneity

-1.58

0.001

Random Effects

2

1.13

1.10

1.10

00

0.27 church

0.27 church

0.07 church

ICC

0.19

0.20

0.06

N

15 church

15 church

15 church

Observations

225

225

225

Marginal R2 / Conditional R2

0.000 / 0.191

0.017 / 0.210

0.163 / 0.210

Deviance

688.730

683.852

670.372

AIC

696.669

720.557

724.366

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\hypertarget{just-the-code-please}{%
\subsection{Just the Code Please}\label{just-the-code-please}}

STAY TUNED

\hypertarget{MLMexplore}{%
\chapter{Preliminary (OLS style) Exploration of Longitudinal Growth}\label{MLMexplore}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=36075f1d-b805-40b9-92da-ad2b0053f46c}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen=}\DecValTok{999}\NormalTok{)}\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

This lesson is an introduction to longitudinal modeling when time is a factor. In this lecture we explore the longitudinal data using OLS tools. Doing so provides the proper screening/vetting of the data to ensure that it is appropriate for multilevel analysis. Simultaneously, it provides an orientation to the types of questions that MLM will address.

\hypertarget{navigating-this-lesson-1}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-1}}

There is about 1 hour and 20 minutes of lecture. If you work through the materials with me it would be plan for an additional two hours

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_CPA}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-1}{%
\subsection{Learning Objectives}\label{learning-objectives-1}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Identify the 3 criteria for longitudinal analysis (in an HLM/MLM framework)
\item
  Know the key variable (type) requirements of a longitudinal dataset.
\item
  Know the distinction (and lingo) between level-1 and level-2.
\item
  Recognize the difference between wide and long files by instantaneous sight.
\item
  Speculate about the findings by looking at the figures, means(\emph{SD}), and correlations we produced.
\item
  Interpret a correlation coefficient of intercepts and slopes.
\end{itemize}

\hypertarget{planning-for-practice-1}{%
\subsection{Planning for Practice}\label{planning-for-practice-1}}

The suggestions for homework are graded in complexity and I encourage you to select an option(s) that will stretch you -- at least a bit. The more complete descriptions at the end of the chapter follow these suggestions.

The assignment is intended to span several lessons. Using a dataset that is provided (or one of your own), walk through exploring, conducting, and writing up a complete multilevel model for change with each step below. Minimally, predictors must include time and an L2 variable.

FROM THIS LESSON

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Restructure the dataset from wide to long.
\item
  Provide three examples of data exploration

  \begin{itemize}
  \tightlist
  \item
    An unfitted model
  \item
    A model fitted with a linear growth trajectory
  \item
    The fitted (or unfitted) data identified by the L2 predictor
  \end{itemize}
\end{enumerate}

FROM SUBSEQUENT LESSONS

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Using a staged approach to model development, report on at least four models, these must include

  \begin{itemize}
  \tightlist
  \item
    An unconditional means model
  \item
    An unconditional growth model
  \item
    An intermediary model (please test both a time variable and an L2 variable)
  \item
    A final model
  \end{itemize}
\item
  Write up the Results as demonstrated in the lecture
\item
  Table (use the tab\_model outfile) and Figure are required
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Rework the problem in the chapter by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar. SINCE I'M NOT CONVINCED THAT CHANGING THE RANDOM SEED DOES MUCH, I MAY CHANGE THIS ONE TO AN EXAMPLE I WORK WITH ANSWERS AT THE END.
\item
  Data from a second MLM analysis from the research vignette are provided at the end. The only difference in the scenario is that the outcome variable changes from anxiety to depression. Use this data.
\item
  Conduct a multi-level analysis with data to which you have access. This could include data you simulate on your own or from a published article. It is quite possible the conditions of your data will necessitate deviations from this approach. Investigate what they are and apply them.
\end{itemize}

\hypertarget{readings-resources-1}{%
\subsection{Readings \& Resources}\label{readings-resources-1}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Singer, J. D., \& Willett, J. B. (2003). A framework for investigating change over time/Chapter 1 and Exporing longitudinal data on change/Chapter 2 in \emph{Applied longitudinal data analysis: Modeling change and event occurrence.} Oxford University Press. \url{https://doi-org.ezproxy.spu.edu/10.1093/acprof:oso/9780195152968.001.0001}

  \begin{itemize}
  \tightlist
  \item
    The \href{https://stats.idre.ucla.edu/other/examples/alda/}{UCLA IDRE website} hosts R solutions (as well as SPSS, SAS, MPlus, and HLM) to many of the examples in this text.
  \end{itemize}
\item
  Lefevor, G. T., Janis, R. A., \& Park, S. Y. (2017). Religious and sexual identities: An intersectional, longitudinal examination of change in therapy. \emph{The Counseling Psychologist, 45}(3), 387--413. \url{https://doi-org.ezproxy.spu.edu/10.1177/0011000017702721}
\end{itemize}

I love the Singer and Willett \citeyearpar{singer_applied_2003} text for so many reasons. Singer and Willet have published a number of articles together. In the Preface of their text they indicated that they were hired at Harvard at about he same time. Their colleagues expected them to voracious competitors. In contrast, they became great collaborators and made decisions about authorship early on. Their agreement was that in any collaboration they would randomly select who was first author, and they have (including for their text).

\hypertarget{packages-2}{%
\subsection{Packages}\label{packages-2}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#will install the package if not already installed}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(robumeta))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"robumeta"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(tidyverse))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(psych))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(lme4))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"lme4"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(nlme))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"nlme"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(sjstats))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"sjstats"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(scales))\{}\FunctionTok{install.packages}\NormalTok{(}\StringTok{"scales"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{change-over-time-analytics}{%
\section{Change-Over-Time Analytics}\label{change-over-time-analytics}}

There are a host of ways to investigate change over time: longitudinal SEM, latent growth curve modeling, latent mixture models, and so forth. We are focused on the subset of this approach that has so many names: individual growth curve models, random coefficient models, multilevel models, linear mixed effects models, hierachical linear models.

Before we start down the longitudinal/repeated measures path, a note to say that this class of statistics was born out of a need to deal with \emph{dependency} in the data and it applies to both \emph{cross-sectional} and \emph{longitudinal/repeated measures} models.

\begin{itemize}
\tightlist
\item
  Remember ANOVA? \emph{(Excepting repeated measure or mixed design ANOVA)} One of the statistical assumptions was that the data had to be \emph{independent.} That is, you could not have family members, co-workers, etc., in the dataset because their data would likely be correlated.
\item
  In the context of these related circumstances (students in a classroom, supervisees of a manager) researchers were confused about how to handle the data. Should they \emph{aggregate} the dependent data (effectively reduce the sample size by taking the mean of all those in a dependent cluster and using it with the non-dependent data)? Should they keep it \emph{disaggregated} (effectively repeating/copying the non-dependent data for each member of the cluster)? Each approach was fraught with difficulty.
\end{itemize}

Random coefficient regression models (RCR or RCM) are an effective alternative to ordinary least squares (OLS) to account for dependencies within the data. The math and approach toward longitudinal modeling is largely the same as when we manage dependencies cross-sectional studies (e.g., members of a team, supervisors reporting to a leader/manager). A more thorough review of aggregation and disaggregation can be found in the \emph{ReCentering Psych Stats} \protect\hyperlink{wGroups}{chapter} devoted to cross-sectional data.

\includegraphics{images/LongExpl/LongNesting.jpg}
This class of analysis allows us to address questions about:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Within-individual change}: How does each person change over time?

  \begin{itemize}
  \tightlist
  \item
    These are descriptive questions: Is change linear? nonlinear? consistent? fluctuating?
  \item
    \emph{level-1} concerns \textbf{within-individual change} over time
  \item
    \emph{individual growth trajectory} -- the way outcome values rise and fall over time
  \item
    Goal is to describe the \emph{shape} of each person's growth trajectory
  \end{itemize}
\item
  \textbf{Interindividual differences in change}: What predicts differences among people in their changes?

  \begin{itemize}
  \tightlist
  \item
    A relational question: What is the association between predictors and patterns of change? Are these relations moderated?
  \item
    \emph{level-2} concerns \textbf{interindividual differences in change}
  \item
    Do different people manifest different patterns of within-individual change? What predicts these differences?
  \item
    Goal is to detect heterogeneity in change across individuals and to determine the relationship between predictors and the shape of each person's individual growth trajectory.
  \end{itemize}
\end{enumerate}

Together, we map the research questions onto a \emph{linked pair} of statistical models known as the \emph{multilevel model for change}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a \emph{level-1} (L1) model, describing within-individual change over time; and
\item
  a \emph{level-2} (L2) model, relating predictors to any interindividual differences in change
\end{enumerate}

Asking these questions requires three criteria of the research design/data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Multiple waves of data}

  \begin{itemize}
  \tightlist
  \item
    Contrast with a developmental psychologist analyzing cross-sectional data composed of children of differing ages. In spite of compelling (and totally fine) research designs, the cross-sectional nature of the design can not rule out \emph{plausible rival hypotheses}.
  \item
    Contrast with two waves of data. Singer and Willett \citeyearpar{singer_applied_2003} state that these data, ``are only marginally better'' (p.~10). Two-wave researchers argued in favor of their \emph{increment} (i.e, the simple difference between scores assessed on the two measurement occasions). Even if the \emph{increment} is large, Singer and Willett \citeyearpar{singer_applied_2003} argue that the increment cannot describe the \emph{process of change} because it cannot describe the \emph{shape} (the focus of the \emph{level-1} question). Singer and Willett further argue that two-wave studies cannot describe individual change trajectories because they confound true change with measurement error.
  \item
    \emph{How many waves?} Within cost and logistical constraints, ``more waves are always better.'' More allows more complex modeling. The statistical rule is that you need one-more-wave than the shape you wish to model. For example, you need 2 waves for a straight line (linear model), 3 waves for a quadratic (1 hump) model, 4 waves for a cubic (2 curves) model, and so forth.
  \end{itemize}
\item
  \textbf{A substantively meaningful metric for time}

  \begin{itemize}
  \tightlist
  \item
    ``Time is the fundamental predictor in every study of change; it must be measured reliably and validly in a sensible metric'' (p.~10).\\
  \item
    What is sensible? ages, grades, months-since-intake, miles, etc. The choice of metric affects related decisions including number and spacing of waves.
  \item
    Consider the \emph{cadence} you expect in the outcome. Weeks or number of sessions is sensible for psychotherapy studies. Grade or age is sensible for education. Parental or child age might make sense for parenting.
  \item
    ``The temporal variable can change only monotonically'' (p.~12), that is, it cannot \emph{reverse} diretions. This means you can have height as a temporal variable, but not weight.
  \item
    There is NOTHING SACRED about evenly spaced variables. In fact, if you expect rapid nonlinear change during some periods, you should collect mored data at those times. If you expect little movement, you can maybe space them urther apart.
  \item
    \emph{Time-structured} schedules assess all participants on an identical schedule (cb equally or unequally spaced). \emph{Time-unstructured} schedules allow data collection schedules to vary across individuals. Multi-level modeling can accomodate both.
  \item
    No requirement for \emph{balance}. That is, each person can have a different number of waves. While non-random attrition can be problematic for drawing inferences, individual growth modeling does not require balanced data.
  \end{itemize}
\item
  \textbf{An outcome that changes systematically}

  \begin{itemize}
  \tightlist
  \item
    The \emph{content} of measurement is a substantive, not statistical decision. However\ldots{}
  \item
    \emph{How} the construct is measured \emph{is} a statistical decision and ``not all variables are equally suited'' (p.~13). Individual growth models are designed for continuous outcomes whose values change systematically over time.
  \item
    \emph{Continuous outcomes} are those that support ``all the usual manipulations of arithmetic'' (p.~13). That is, you can take differences between pairs of scores, add, subtract, multiply, divide. Most psychometrically credible instruments will work. BUT!
  \item
    the ``metric, validity, and precision'' of the outcome must be preserved across time. That is, the outcome scores must be ``equatable over time''. That is, a given value of the outcome on any occasion must represent the same ``amount'' of the outcome on every occasion. \emph{Outcome equatability} is supported (in part) by using the identical instrument each time.
  \item
    \emph{Standardization} in the longitudinal context is hotly debated and not a simple solution for equating shifty metrics. Why? the SD units likely have different size/meaning at different intervals. Transforming the \emph{M} to 0.0 and the \emph{SD} to 1.0 masks the variance differences that may exist. The raw metric preserves the variance and avoids all the issues.
  \item
    Outcomes should be \emph{equally valid} across all measurement occasions. For example, a multiplication test may be a valid measure of mathematical skill among young children, but a measure of memory among teenagers.
  \item
    Although \emph{precision} need not be identical at every occasion, the goal is to minimize errors introduced by instrument administration. Look for reliabilities of .8 and above.
  \end{itemize}
\end{enumerate}

``Structuring up'' a longitudinal data set and engaging in preliminary data anlaysis is a great way to further understand this approach to statistical modeling. So, let's transition to our research vignette.

\hypertarget{workflow-for-longitudinal-mlm}{%
\section{Workflow for Longitudinal MLM}\label{workflow-for-longitudinal-mlm}}

\begin{figure}
\centering
\includegraphics{images/LongExpl/WrkFlowLong.jpg}
\caption{Workflow for a longitudinal MLM}
\end{figure}

\hypertarget{research-vignette-1}{%
\section{Research Vignette}\label{research-vignette-1}}

Our research vignette \citep{lefevor_religious_2017} examines the intersection of religious and sexual identities of clients in therapy. With 12,825 participants from the Center for Collegiate Mental Health 2012-2014 data set, the project is an example of working with \emph{big data.} Because the data is available to members only (and behind a paywall), I simulated the data. In the simulation, categorical variables (e.g., sexual identity, session number, religious identity) were rendered as continuous variables and in the simulation, I needed to transform them back into categorical ones. Inevitably, this will have introduced a great deal of error. Thus, we can expect that the results from the simulated data will be different from those obtained by the authors.

The Method section of the article provides detailed information about the inclusion criteria ofr the study and the coding of the variables. This included data about the religious and sexual identities as well as a minimum of three separate scores on the Counseling Center Assessment of Psychologial Sympsoms \citep[CCAPS,][]{locke_development_2012} measure. For the final dataset, clients attended an average of 10.58 sessions (\emph{SD} = 7.65) and had an average of 5.36 CCAPS administrations (\emph{SD} = 4.04). This means that in the original dataset, each client was represented by a varying number of observations (likely ranging from 3 {[}the minimum required for inclusion{]} and, perhaps as many as 17 {[}adding +3\emph{SD}s to the mean CCAPS administrations{]}). In simulating the data, I specified five observations for each of the 12,825 clients.

Let's take a look at the variables in the study

\begin{itemize}
\item
  \textbf{Anxiety and Depression}: The anxiety and depression ratings were taken from the CCAPS measure \citep{locke_development_2012} that assesses psychological distress across seven domains. Clients rate themselves over the past two weeks on a 5-point Likert-type scale ranging from 0 (\emph{not at all like me}) to 4 (\emph{extremely like me}). Higher scores indicate more distress. The dataset comes from multiple institutions with different procedures around assessment CCAPS there is not a 1:1 correspondence with session number and CCAPS assessment.
\item
  \textbf{Sexual Identity}: Sexual identity was dichotomized into heterosexual (-1, 85.5\%) and LGBQQ (1, 14.5\%).
\item
  \textbf{Relious Identity}: Religious identity was coded into three categories including dominant religious (DR; Christian, Catholic), nondominant religious (NDR; Muslim, Hindu, Buddhist, Jewish), and nondominant unaffiliated (NDU; agnostic, atheist, no preference). The three categories were contrast coded with an orthogonal contrast-coding scheme with two variables. The first variable compared DR(coded as 2) to NDU and NDR (coded as -1); the second variable compared the two nondominant groups (NDU = -1, DR = 0, NDR = 1).
\end{itemize}

\hypertarget{simulating-the-data-from-the-journal-article-1}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200513}\NormalTok{)}
\NormalTok{n\_client }\OtherTok{=} \DecValTok{12825}
\NormalTok{n\_session }\OtherTok{=} \DecValTok{5}
\NormalTok{b0 }\OtherTok{=} \FloatTok{2.03} \CommentTok{\#intercept for anxiety}
\NormalTok{b1 }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{22} \CommentTok{\#b weight for L1 session}
\NormalTok{b2 }\OtherTok{=}\NormalTok{ .}\DecValTok{13} \CommentTok{\#b weight for L2 sexual identity}
\NormalTok{b3 }\OtherTok{=}  \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{03} \CommentTok{\#b weight for L2 Rel1 (D{-}R vs ND{-}R \& ND{-}U)}
\NormalTok{b4 }\OtherTok{=}\NormalTok{ .}\DecValTok{01} \CommentTok{\#b weight for the L2 Rel2 (ND{-}R vs ND{-}U)}
\CommentTok{\#the values used below are the +/{-} 3SD they produce continuous variables which later need to be transformed to categorical ones; admittedly this introduces a great deal of error/noise into the simulation}
\CommentTok{\#the article didn\textquotesingle{}t include a correlation matrix or M/SDs so this was a clunky process }
\NormalTok{( }\AttributeTok{Session =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\SpecialCharTok{{-}}\FloatTok{3.61}\NormalTok{, }\FloatTok{3.18}\NormalTok{)) }\CommentTok{\#calc L1 Session, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{SexualIdentity =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{6.66}\NormalTok{, }\FloatTok{6.92}\NormalTok{)) }\CommentTok{\#calc L2 Sexual Identity, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion1 =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{3.43}\NormalTok{, }\FloatTok{3.37}\NormalTok{)) }\CommentTok{\#calc L2 Religion1, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion2 =} \FunctionTok{rep}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{3.38}\NormalTok{, }\FloatTok{3.41}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Religion2, values are the +/3 3SD}
\NormalTok{mu }\OtherTok{=} \FloatTok{1.76} \CommentTok{\#intercept of empty model }
\NormalTok{sds }\OtherTok{=} \FloatTok{2.264} \CommentTok{\#this is the SD of the DV}
\NormalTok{sd }\OtherTok{=} \DecValTok{1} \CommentTok{\#this is the observation{-}level random effect variance that we set at 1}

\CommentTok{\#( church = rep(LETTERS[1:n\_church], each = n\_mbrs) ) \#this worked in the prior}
\NormalTok{( }\AttributeTok{client =} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_client], }\AttributeTok{each =}\NormalTok{ n\_session) )}
\CommentTok{\#( session = numbers[1:(n\_client*n\_session)] )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rnorm}\NormalTok{(n\_client, }\DecValTok{0}\NormalTok{, sds) )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rep}\NormalTok{(clienteff, }\AttributeTok{each =}\NormalTok{ n\_session) )}
\NormalTok{( }\AttributeTok{sessioneff =} \FunctionTok{rnorm}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\DecValTok{0}\NormalTok{, sd) )}
\NormalTok{( }\AttributeTok{Anxiety =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{Session }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{SexualIdentity }\SpecialCharTok{+}\NormalTok{ b3}\SpecialCharTok{*}\NormalTok{Religion1 }\SpecialCharTok{+}\NormalTok{ b4}\SpecialCharTok{*}\NormalTok{Religion2 }\SpecialCharTok{+}\NormalTok{ clienteff }\SpecialCharTok{+}\NormalTok{ sessioneff)}
\NormalTok{( }\AttributeTok{dat =} \FunctionTok{data.frame}\NormalTok{(client, clienteff, sessioneff, Session, SexualIdentity, Religion1, Religion2, Anxiety) )}

\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())}
\CommentTok{\#moving the ID number to the first column; requires }
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{Lefevor2017 }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, client, Session, SexualIdentity, Religion1, Religion2, Anxiety)}

\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12825}\NormalTok{), }\AttributeTok{each =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#rounded Sexual Identity into dichotomous variable}
\CommentTok{\#85\% were heterosexual, }

\FunctionTok{library}\NormalTok{(robumeta)}
\CommentTok{\#The following variables should be L2, but were simulated as if they were L1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion1,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexualIdentity,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}

\CommentTok{\#Rel2 has contrast codes for dominant religion (DR, 0), nondominant religious (NDR, 1) and nondominant unspecified (NDU, {-}1)}
\CommentTok{\#Strategy is to figure out the raw score associated with the percentile rank of  {-}1 and 0, to set the breakpoints for the coding}
\CommentTok{\#NDU coded as {-}1}
\CommentTok{\#19.2+13.5+9.6}
\CommentTok{\#NDU has bottom 42.3 percent}

\CommentTok{\#DR coded as 0, so quantile cut will be 42.3 + 52.7 = 95th}
\CommentTok{\#33.4 + 19.3}
\CommentTok{\#52.7\% of sample (according to article) was DR}
\CommentTok{\#must look up percentile ranks for 5\% and 57.5\%}

\CommentTok{\#NDR}
\CommentTok{\#2.3+1+1+.7}
\CommentTok{\#NDR has 5\% of sample}
\CommentTok{\#42.3+52.7}
\CommentTok{\#quantile(Lefevor2017$Religion2, probs = c(.423, .95))}
\CommentTok{\#effects coding the second Religion variable so that NDU = {-}1, DR = 0, NDR = 1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
                             \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087} \SpecialCharTok{\&}\NormalTok{ Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \FloatTok{0.9299491}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking work}
\CommentTok{\#Rel2L2\_table \textless{}{-} table(Lefevor2017$Rel2L2)}
\CommentTok{\#prop.table(Rel2L2\_table)}
\CommentTok{\#Lefevor2017 \%\textgreater{}\%}
\CommentTok{\#count(Rel2L2)}

\CommentTok{\#creating the first religion variable where DR is 2 and NDR and NDU are both {-}1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1L2 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{DRel0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking to make sure that 52.7\% are coded 2 (DR)}
\CommentTok{\#Rel1L2\_table \textless{}{-} table(Lefevor2017$Rel1L2)}
\CommentTok{\#prop.table(Rel1L2\_table)}

\CommentTok{\#heterosexual is {-}1}
\CommentTok{\#LGBTQIA+ is 1}
\CommentTok{\#quantile(Lefevor2017$SxID, probs = c(.85))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\SpecialCharTok{\textless{}=} \FloatTok{1.203468}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Het0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\#SexID\_table \textless{}{-} table(Lefevor2017$SexID)}
\CommentTok{\#prop.table(SexID\_table)}

\CommentTok{\#creating a variable representing the session number for each client, in the article up to 20 sessions were allowed. }
\CommentTok{\#install.packages("scales")}
\FunctionTok{library}\NormalTok{(scales)}
\CommentTok{\#Right from the beginning I centered this so that 0 would represent intake}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Session0 }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Session, }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{)))}

\CommentTok{\#creating session waves (1 thru 5) by rank ordering within each person\textquotesingle{}s variable the continuous variable Session that was created in the original simulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{Lefevor2017 }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(ClientID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Index =} \FunctionTok{rank}\NormalTok{(Session))}

\CommentTok{\#selecting the simulated variables}
\NormalTok{Lefevor2017\_sim }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, Session0, Anxiety, DRel0, Het0)}

\CommentTok{\#In the transition from long{-}to{-}wide, it seems like you can only do one L1 variable at a time}
\CommentTok{\#When there are multiple L1 and L2 vars, put all L2 vars on left of tilde}
\CommentTok{\#The wave/index function should come next; this should be finite (like integers of 1,2,3,4) with a maximum}
\CommentTok{\#Put the name of the SINGLE L1 variable in the concatonated list}
\FunctionTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Attaching package: 'data.table'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:xts':

    first, last
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:dplyr':

    between, first, last
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:purrr':

    transpose
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LfvrWp1}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{+}\NormalTok{ DRel0 }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Index"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrWp1}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp1, }\AttributeTok{Index1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Index2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Index3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Index4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Index5 =} \StringTok{"5"}\NormalTok{)}
\NormalTok{LfvrWp2}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Anxiety"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrWp2}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp2, }\AttributeTok{Anx1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Anx2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Anx3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Anx4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Anx5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#For remaining L1 variable, do them one at a time {-}{-} key them from the person{-}level ID and the wave/index.}
\NormalTok{LfvrWp3}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Session0"}\NormalTok{))}
\NormalTok{LfvrWp3}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp3, }\AttributeTok{Sess1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Sess2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Sess3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Sess4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Sess5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#Next, join the dataframes by the person{-}level ID}
\CommentTok{\#Only two can be joined at a time}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrWp1, LfvrWp2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrWide, LfvrWp3,  }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To increase the portability of the OER, this chapter uses simulated data. Here is script for exporting/downloading the data as a .csv file to your local computer and then importing/uploading it again. I find that saving the .csv file (data) in the same place as the .rmd file(s) is essential for R to connect the two.

Because this simulation can take a few minutes, you may wish to do this, even as you work through this chapter, so that resimulations take less time and comuting resources.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(LfvrWide, }\AttributeTok{file=}\StringTok{"LefevorWide.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"LefevorWide.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{longitudinal-exploration}{%
\section{Longitudinal Exploration}\label{longitudinal-exploration}}

\hypertarget{the-structure-of-the-data-file-as-the-first-step-in-understanding-longitudinal-modeling}{%
\subsection{The Structure of the Data File as the First Step in Understanding Longitudinal Modeling}\label{the-structure-of-the-data-file-as-the-first-step-in-understanding-longitudinal-modeling}}

We are accustomed to viewing data in its \emph{wide} format. The wide format is also technically termed the \textbf{person-level} data set or the \emph{multivariate} format of data. It is characterized by the following:

\begin{itemize}
\tightlist
\item
  Each person has one record and multiple variables contain the data from each measurement occasion; a 16-person set has 16 records while a 20,000 person set has 20,000 records.
\item
  As you collect additional waves, a person-level file gains new variables (not new cases).
\item
  In the context of longitudinal modeling, data in this form allows us to visually examing an \emph{empirical growth record} (the temporally sequenced outcomes).
\item
  This wide file arrays each person's empirical growth record horizontally
\end{itemize}

There are \textbf{disadvantages} to the wide format:

\begin{itemize}
\tightlist
\item
  The summmaries are noninformative.
\item
  It omits an explicit time variable.
\item
  It is inefficient/useless when the number and spacing of waves varies
\item
  It cannot handle the presence of time-varying predictors.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(LfvrWide),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars     n    mean      sd  median trimmed     mad   min      max
ClientID    1 12825 6413.00 3702.40 6413.00 6413.00 4753.22  1.00 12825.00
DRel0       2 12825    0.60    0.49    1.00    0.62    0.00  0.00     1.00
Het0        3 12825    0.28    0.45    0.00    0.22    0.00  0.00     1.00
Index1      4 12825    1.00    0.00    1.00    1.00    0.00  1.00     1.00
Index2      5 12825    2.00    0.00    2.00    2.00    0.00  2.00     2.00
Index3      6 12825    3.00    0.00    3.00    3.00    0.00  3.00     3.00
Index4      7 12825    4.00    0.00    4.00    4.00    0.00  4.00     4.00
Index5      8 12825    5.00    0.00    5.00    5.00    0.00  5.00     5.00
Anx1        9 12825    2.58    2.54    2.56    2.58    2.52 -7.88    11.88
Anx2       10 12825    2.32    2.55    2.31    2.33    2.56 -8.13    12.31
Anx3       11 12825    2.08    2.56    2.06    2.08    2.57 -7.36    12.87
Anx4       12 12825    1.82    2.52    1.82    1.83    2.52 -7.29    10.72
Anx5       13 12825    1.58    2.52    1.56    1.58    2.55 -7.58    11.23
Sess1      14 12825    2.66    2.64    2.00    2.27    2.96  0.00    16.00
Sess2      15 12825    5.83    3.37    5.00    5.64    2.96  0.00    17.00
Sess3      16 12825    8.99    3.58    9.00    8.98    4.45  0.00    18.00
Sess4      17 12825   12.15    3.38   12.00   12.33    4.45  0.00    18.00
Sess5      18 12825   15.31    2.68   16.00   15.72    2.96  2.00    19.00
            range  skew kurtosis    se
ClientID 12824.00  0.00    -1.20 32.69
DRel0        1.00 -0.41    -1.83  0.00
Het0         1.00  0.99    -1.02  0.00
Index1       0.00   NaN      NaN  0.00
Index2       0.00   NaN      NaN  0.00
Index3       0.00   NaN      NaN  0.00
Index4       0.00   NaN      NaN  0.00
Index5       0.00   NaN      NaN  0.00
Anx1        19.76  0.03     0.06  0.02
Anx2        20.44 -0.02    -0.01  0.02
Anx3        20.23 -0.01    -0.03  0.02
Anx4        18.01 -0.03    -0.04  0.02
Anx5        18.81  0.00    -0.07  0.02
Sess1       16.00  1.20     1.20  0.02
Sess2       17.00  0.46    -0.34  0.03
Sess3       18.00  0.02    -0.65  0.03
Sess4       18.00 -0.46    -0.36  0.03
Sess5       17.00 -1.21     1.20  0.02
\end{verbatim}

We \emph{could} (but it is not advised in this specific instance) use the wide format to create the multilevel correlation matrix, allowing us to see the correlations between the person-level (L2) variables of religious identity and sexual identity with the repeated measures variable (L1), anxiety.

The bivariate correlations tell us little about change-over-time for individuals or groups. However, in this dataset we can see a strong correlation (they are all \emph{r} = .80) between anxiety at one session and the next..

Thinking of what it takes to get a positive and strong correlation (e.g., relative rankings must stay stable), we learn that the \emph{rank order} of clients (relative to anxiety) remains relatively stable across occasions, but it does not tell us how each person changes over time nor about the direction of change.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Multilevel level correlation matrix}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(LfvrWide[}\FunctionTok{c}\NormalTok{(}
\StringTok{"DRel0"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{, }\StringTok{"Anx1"}\NormalTok{, }\StringTok{"Anx2"}\NormalTok{, }\StringTok{"Anx3"}\NormalTok{, }\StringTok{"Anx4"}\NormalTok{, }\StringTok{"Anx5"}\NormalTok{)], }\AttributeTok{show.conf.interval =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename=}\StringTok{"Lfvr2017\_CorMatrix.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The ability to suppress reporting of reporting confidence intervals has been deprecated in this version.
The function argument show.conf.interval will be removed in a later version.
\end{verbatim}

\begin{verbatim}

Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable M    SD   1           2          3          4          5         
  1. DRel0 0.60 0.49                                                        
                                                                            
  2. Het0  0.28 0.45 -.01                                                   
                     [-.02, .01]                                            
                                                                            
  3. Anx1  2.58 2.54 .00         .05**                                      
                     [-.01, .02] [.03, .07]                                 
                                                                            
  4. Anx2  2.32 2.55 -.00        .05**      .80**                           
                     [-.02, .02] [.03, .07] [.79, .81]                      
                                                                            
  5. Anx3  2.07 2.56 .00         .06**      .80**      .80**                
                     [-.01, .02] [.04, .07] [.80, .81] [.80, .81]           
                                                                            
  6. Anx4  1.82 2.52 .00         .05**      .80**      .80**      .80**     
                     [-.01, .02] [.03, .07] [.79, .80] [.79, .80] [.80, .81]
                                                                            
  7. Anx5  1.58 2.52 -.01        .05**      .80**      .80**      .80**     
                     [-.02, .01] [.03, .07] [.79, .81] [.79, .80] [.79, .81]
                                                                            
  6         
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
  .80**     
  [.79, .80]
            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

I should note that a bivariate matrix created from the wide format is rather useless when the assessments are unevenly spaced (time-unstructured; ours are) and the dataset is unbalanced (ours is).

\hypertarget{job1-is-to-get-our-data-from-person-level-into-person-period}{%
\subsection{Job\#1 is to get our data from person-level into person-period}\label{job1-is-to-get-our-data-from-person-level-into-person-period}}

\textbf{Person-period data set} aka a \emph{long} or \emph{univariate} file:

\begin{itemize}
\tightlist
\item
  Each person has multiple records -- one for each measurement occasion.
\item
  As you collect additional waves,the file gains new records, but no new variables
\item
  This long file arrays each person's empirical growth record vertically
\item
  4 types of variables

  \begin{itemize}
  \tightlist
  \item
    Subject identifier -- typically in the first column and identical across waves; required for sorting and grouping
  \item
    Time indicator -- often labeled AGE, WAVE, or TIME (or something sensible); it is fine to have ``unstructured time'' (e.g., 0.5, 1.2, 3.4 months)
  \item
    Outcome variable(s) -- time-varying, but represented by a single variable/column
  \item
    Predictor variable(s) -- each individual predictor (whether time-covarying or time-invariant) is represented by a single variable/column
  \end{itemize}
\end{itemize}

Let's restructure (shapeshift) our dataset with the technique known as \emph{melting}.

Each set of variables being melted in each set being restructured need to be on the same scale. In this problem, the Anx\# and Sess\# variables should be on the same scale.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(LfvrWide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   12825 obs. of  18 variables:
 $ ClientID: int  1 2 3 4 5 6 7 8 9 10 ...
 $ DRel0   : int  0 1 1 1 0 0 1 1 1 0 ...
 $ Het0    : int  0 0 0 0 0 0 0 1 0 0 ...
 $ Index1  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Index2  : int  2 2 2 2 2 2 2 2 2 2 ...
 $ Index3  : int  3 3 3 3 3 3 3 3 3 3 ...
 $ Index4  : int  4 4 4 4 4 4 4 4 4 4 ...
 $ Index5  : int  5 5 5 5 5 5 5 5 5 5 ...
 $ Anx1    : num  3.71 3.49 1.19 2.94 2.72 ...
 $ Anx2    : num  3.58 3.09 2.21 2.35 4.75 ...
 $ Anx3    : num  3.87 2.02 -1.25 5.11 3.47 ...
 $ Anx4    : num  3.799 2.747 0.338 2.625 5.002 ...
 $ Anx5    : num  1.56 4 -1.48 2.17 4.83 ...
 $ Sess1   : int  0 5 1 2 5 6 3 0 3 6 ...
 $ Sess2   : int  7 6 2 6 6 8 4 2 8 8 ...
 $ Sess3   : int  8 7 12 9 14 8 6 4 13 10 ...
 $ Sess4   : int  13 8 12 13 15 13 8 16 13 10 ...
 $ Sess5   : int  16 10 18 13 17 14 16 18 17 11 ...
\end{verbatim}

They are. Each is \emph{num}eric.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table) }\CommentTok{\#the package for melting (better than reshape2 because it can accommodate multiple repeated measures variables)}

\CommentTok{\#add the name of the wide df after "setDT"}
\CommentTok{\#id.vars are L2 variables that do not change over time}
\CommentTok{\#measure.vars are those that change over time; if there is more than one that is time{-}covarying, add a comma followed by another another concatonated list.}

\NormalTok{LfvrLong }\OtherTok{\textless{}{-}}\NormalTok{ (data.table}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(}\FunctionTok{setDT}\NormalTok{(LfvrWide), }\AttributeTok{id.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{, }\StringTok{"DRel0"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{), }\AttributeTok{measure.vars =}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Anx1"}\NormalTok{, }\StringTok{"Anx2"}\NormalTok{, }\StringTok{"Anx3"}\NormalTok{, }\StringTok{"Anx4"}\NormalTok{, }\StringTok{"Anx5"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Sess1"}\NormalTok{, }\StringTok{"Sess2"}\NormalTok{, }\StringTok{"Sess3"}\NormalTok{, }\StringTok{"Sess4"}\NormalTok{, }\StringTok{"Sess5"}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

Take a peek at LfvrLong:

\begin{itemize}
\tightlist
\item
  Anx1 through Anx5 and Sess1 through Sess5 are gone
\item
  Two new variables have appeared

  \begin{itemize}
  \tightlist
  \item
    \emph{variable} is the former variable name; it represents the \emph{unit} of time (or condition) associated with the repeated measure
  \item
    \emph{value} is the value of that measurement for that person
  \end{itemize}
\item
  We must rename these
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#This process  does not preserve the variable names, so we need to rename them}
\NormalTok{LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong, }\AttributeTok{Index =}\NormalTok{ variable, }\AttributeTok{Anxiety =} \StringTok{"value1"}\NormalTok{, }\AttributeTok{SesNum =} \StringTok{"value2"}\NormalTok{)))}

\CommentTok{\#rearranging variables so that IDs are together}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}}\NormalTok{ LfvrLong}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, SesNum, Anxiety, DRel0, Het0)}
\CommentTok{\#resorting data so that each person is together}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}} \FunctionTok{arrange}\NormalTok{(LfvrLong, ClientID, Index)}
\end{Highlighting}
\end{Shaded}

It can be helpful to have this written to your local file so you can bring it back in without having to re-prep it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(LfvrLong, }\AttributeTok{file=}\StringTok{"LfvrLong.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"LfvrLong.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's peek at the characteristics and descriptives as a result of this restructuring from wide to long.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(LfvrLong)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   64125 obs. of  6 variables:
 $ ClientID: int  1 1 1 1 1 2 2 2 2 2 ...
 $ Index   : int  1 2 3 4 5 1 2 3 4 5 ...
 $ SesNum  : int  0 7 8 13 16 5 6 7 8 10 ...
 $ Anxiety : num  3.71 3.58 3.87 3.8 1.56 ...
 $ DRel0   : int  0 0 0 0 0 1 1 1 1 1 ...
 $ Het0    : int  0 0 0 0 0 0 0 0 0 0 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(LfvrLong),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars     n    mean      sd  median trimmed     mad   min      max
ClientID    1 64125 6413.00 3702.29 6413.00 6413.00 4753.22  1.00 12825.00
Index       2 64125    3.00    1.41    3.00    3.00    1.48  1.00     5.00
SesNum      3 64125    8.99    5.47    9.00    8.98    7.41  0.00    19.00
Anxiety     4 64125    2.08    2.56    2.08    2.08    2.56 -8.13    12.87
DRel0       5 64125    0.60    0.49    1.00    0.62    0.00  0.00     1.00
Het0        6 64125    0.28    0.45    0.00    0.22    0.00  0.00     1.00
         range  skew kurtosis    se
ClientID 12824  0.00    -1.20 14.62
Index        4  0.00    -1.30  0.01
SesNum      19  0.01    -1.20  0.02
Anxiety     21  0.00    -0.01  0.01
DRel0        1 -0.41    -1.83  0.00
Het0         1  0.99    -1.02  0.00
\end{verbatim}

Evaluating longitudinal growth trajectories in MLM means that we invest in substantial preliminary exporation.

\hypertarget{empirical-growth-plots}{%
\subsection{Empirical Growth Plots}\label{empirical-growth-plots}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

Researchers commonly look at individual dataplots to see if there are clear trends/patterns across the individuals. Do they rise? Is there a curve? Do some rise and some fall?

With the \emph{lattice} package we are asking for the anxiety scores to be plotted by session, for each person (noted with ``ClientID'').

Especially in large datasets it is common to create a smaller subset of data for this inspection. The easiest way I found to do this was to grab a set of 30 from the wide file and then quickly turn it to a long file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210515}\NormalTok{)}
\NormalTok{RndmSmpl30 }\OtherTok{\textless{}{-}}\NormalTok{ LfvrWide[}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(LfvrWide), }\DecValTok{30}\NormalTok{,}
   \AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{),] }
\NormalTok{RndmLong }\OtherTok{\textless{}{-}}\NormalTok{ (data.table}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(}\FunctionTok{setDT}\NormalTok{(RndmSmpl30), }\AttributeTok{id.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{, }\StringTok{"DRel0"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{), }\AttributeTok{measure.vars =}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Anx1"}\NormalTok{, }\StringTok{"Anx2"}\NormalTok{, }\StringTok{"Anx3"}\NormalTok{, }\StringTok{"Anx4"}\NormalTok{, }\StringTok{"Anx5"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Sess1"}\NormalTok{, }\StringTok{"Sess2"}\NormalTok{, }\StringTok{"Sess3"}\NormalTok{, }\StringTok{"Sess4"}\NormalTok{, }\StringTok{"Sess5"}\NormalTok{))))}
\NormalTok{RndmLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(RndmLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(RndmLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(RndmLong, }\AttributeTok{Index =}\NormalTok{ variable, }\AttributeTok{Anxiety =} \StringTok{"value1"}\NormalTok{, }\AttributeTok{Session0 =} \StringTok{"value2"}\NormalTok{)))}
\CommentTok{\#resorting data so that each person is together}
\NormalTok{RndmLong }\OtherTok{\textless{}{-}} \FunctionTok{arrange}\NormalTok{(RndmLong, ClientID, Index)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting-a-trajectory-as-summary-of-each-persons-empirical-growth-record}{%
\subsection{Plotting a Trajectory as Summary of Each Person's Empirical Growth Record}\label{plotting-a-trajectory-as-summary-of-each-persons-empirical-growth-record}}

Singer and Willett \citeyearpar{singer_applied_2003} suggest that we do this two ways:

\begin{itemize}
\tightlist
\item
  \emph{nonparametric} models let the ``data speak for themselves'' by smoothing across temporal idiosyncracies without imposing a specific functional form
\item
  \emph{parametric} models impose a researcher-selected common functional form (e.g., linear, quadratic, cubic) and then fit a separate regression model to each person's data, yielding a ``fitted trajectory''
\end{itemize}

While our multilevel modeling will use maximum likelihood, these individual plots are fitted with OLS regression models.

A better ``group-by'' tool:\\
\url{http://r4stats.com/2017/04/18/group-by-modeling-in-r-made-easy/}

\hypertarget{nonparametrical-smoothing-of-the-empirical-growth-trajectory}{%
\subsubsection{Nonparametrical Smoothing of the Empirical Growth Trajectory**}\label{nonparametrical-smoothing-of-the-empirical-growth-trajectory}}

The ``smoothed'' nonparametric trajectory is superimposed on the data.

To evaluate, focus on elevation, shape, and title. Where do scores hover at the low, medium, or high end? Does everyone change over time or do some remain the same? Is there an overall pattern of change: linear, curvilinear, smooth, steplike? Is the rate of change similar or different across people.

Below I have shown how to plot these with variable, Index variable and then again with the variable, Session. Recall that Index is a structured form of counting across the 5 client sessions. In contrast, Session is ``time-unstructured'' because the intervals are unevenly spaced. Our Index variable clocks 1 through 5; Session0 starts at 0.0, providing a better ``intercept'' at the first session. Singer and Willett \citeyearpar{singer_applied_2003} recommend also staring at the entire set together as a group. Notice anything?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(Anxiety}\SpecialCharTok{\textasciitilde{}}\NormalTok{Session0 }\SpecialCharTok{|}\NormalTok{ ClientID, }\AttributeTok{data=}\NormalTok{RndmLong,}
  \AttributeTok{prepanel =} \ControlFlowTok{function}\NormalTok{(x, y) }\FunctionTok{prepanel.loess}\NormalTok{(x, y, }\AttributeTok{family=}\StringTok{"gaussian"}\NormalTok{),}
  \AttributeTok{xlab =} \StringTok{"Session"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Anxiety"}\NormalTok{,}
  \AttributeTok{panel =} \ControlFlowTok{function}\NormalTok{(x, y) \{}
  \FunctionTok{panel.xyplot}\NormalTok{(x, y)}
  \FunctionTok{panel.loess}\NormalTok{(x,y, }\AttributeTok{family=}\StringTok{"gaussian"}\NormalTok{) \},}
   \AttributeTok{as.table=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-63-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(Anxiety}\SpecialCharTok{\textasciitilde{}}\NormalTok{Index }\SpecialCharTok{|}\NormalTok{ ClientID, }\AttributeTok{data=}\NormalTok{RndmLong,}
  \AttributeTok{prepanel =} \ControlFlowTok{function}\NormalTok{(x, y) }\FunctionTok{prepanel.loess}\NormalTok{(x, y, }\AttributeTok{family=}\StringTok{"gaussian"}\NormalTok{),}
  \AttributeTok{xlab =} \StringTok{"Index"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Anxiety"}\NormalTok{,}
  \AttributeTok{panel =} \ControlFlowTok{function}\NormalTok{(x, y) \{}
  \FunctionTok{panel.xyplot}\NormalTok{(x, y)}
  \FunctionTok{panel.loess}\NormalTok{(x,y, }\AttributeTok{family=}\StringTok{"gaussian"}\NormalTok{) \},}
   \AttributeTok{as.table=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-63-2.pdf}

When we examine these, we simply look for patterns:

\begin{itemize}
\tightlist
\item
  Are there \emph{general} trends?

  \begin{itemize}
  \tightlist
  \item
    Seems like anxiety tends to go downward
  \end{itemize}
\item
  Are the levels at the start of the study (Wave 1, intercept) at the same place? Or differenct?

  \begin{itemize}
  \tightlist
  \item
    In this dataset, they are definitely different.
  \end{itemize}
\item
  Is the change-over-time (slope) linear or curvilinear?

  \begin{itemize}
  \tightlist
  \item
    The general trend seems to be linear -- a decline or staying flat
  \item
    Some show ``bump ups'' in anxiety before it declines again
  \item
    One case shows a ``bump down'' in anxiety and then it rises again
  \end{itemize}
\end{itemize}

Because clients (a) start at different points and (b) change differently, MLM is a reasonable approach to analyzing the data. Our choice of L1 (time-covarying) and L2 (person-level) variables may help disentangle these differences.

As we continue the preliminary exploration, I will use the Session0 variable as our representation of time because it is ``truer'' to the data.

\hypertarget{parametric-smoothing-of-the-empirical-growth-trajectory-w-ols-regression}{%
\subsubsection{Parametric Smoothing of the Empirical Growth Trajectory w OLS Regression**}\label{parametric-smoothing-of-the-empirical-growth-trajectory-w-ols-regression}}

Here we fit a separate parametric model to each person's data; OLS is appropriate for this preliminary exploration. Next we can summarize each person's growth trajectory by fitting a separate parametric model to each person's data. Singer and Willett \citeyearpar{singer_applied_2003} indicate that this is ``hardly the most efficient use of longitudinal data\ldots{[}but{]} it connects empirical researchers with their data in a direct and intimate way'' (p.~28).

We must identify a ``common functional form'' (e.g., linear, quadratic, cubic) to fit to all the individuals. Clearly, this is an oversimplification. However, it allows us to easily spot folks for whom the model works and does not. Often the best choice is a straight line and that's what we will do here.

There are three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate a within-person regression model for each person. This means we regress the outcome {[}Anxiety{]} on some representation of time(we'll use Session0 around 0). In order to conduct separate analyses for each person, we conduct the regression analysis ``by Client''
\item
  Use summary statistics from all the within-person regression models into a separate data set. For a linear change model, the intercept and slope summarize their growth trajectory; the \(R^2\) and residual variance statistics summarize the goodness of fit.
\item
  Superimpose each person's fitted regression line on a plot of their empirical growth record.
\end{enumerate}

Let's start with step \#1:

This little script produces individual regression models for each person.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANX\_OLS }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{ (RndmLong)\{}
  \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data =}\NormalTok{ RndmLong))}
\NormalTok{\}}
\FunctionTok{by}\NormalTok{(RndmLong, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID, ANX\_OLS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RndmLong$ClientID: 175

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
       1        2        3        4        5 
 0.05038 -0.29755  0.08804  0.47882 -0.31968 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  6.75056    0.54322  12.427  0.00112 **
Session0    -0.07347    0.04779  -1.537  0.22181   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.3787 on 3 degrees of freedom
Multiple R-squared:  0.4407,    Adjusted R-squared:  0.2542 
F-statistic: 2.363 on 1 and 3 DF,  p-value: 0.2218

------------------------------------------------------------ 
RndmLong$ClientID: 343

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      6       7       8       9      10 
-0.1008  0.2721  0.6444 -1.1712  0.3555 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.49676    0.75416  -0.659    0.557
Session0    -0.07398    0.06735  -1.098    0.352

Residual standard error: 0.816 on 3 degrees of freedom
Multiple R-squared:  0.2868,    Adjusted R-squared:  0.04912 
F-statistic: 1.207 on 1 and 3 DF,  p-value: 0.3523

------------------------------------------------------------ 
RndmLong$ClientID: 755

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     11      12      13      14      15 
 0.2702 -0.5245  0.2561  0.9874 -0.9893 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  2.68707    0.92956   2.891    0.063 .
Session0    -0.04112    0.07372  -0.558    0.616  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8883 on 3 degrees of freedom
Multiple R-squared:  0.09397,   Adjusted R-squared:  -0.208 
F-statistic: 0.3112 on 1 and 3 DF,  p-value: 0.6159

------------------------------------------------------------ 
RndmLong$ClientID: 818

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     16      17      18      19      20 
-1.1235 -0.4274  1.1499  1.6847 -1.2837 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   0.2063     1.0700   0.193    0.859
Session0     -0.2684     0.2140  -1.254    0.299

Residual standard error: 1.555 on 3 degrees of freedom
Multiple R-squared:  0.3439,    Adjusted R-squared:  0.1253 
F-statistic: 1.573 on 1 and 3 DF,  p-value: 0.2986

------------------------------------------------------------ 
RndmLong$ClientID: 1435

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     21      22      23      24      25 
 0.5863 -0.9380 -0.3808  1.2743 -0.5418 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  4.29380    0.80645   5.324   0.0129 *
Session0    -0.05622    0.07818  -0.719   0.5240  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.047 on 3 degrees of freedom
Multiple R-squared:  0.147, Adjusted R-squared:  -0.1373 
F-statistic: 0.5171 on 1 and 3 DF,  p-value: 0.524

------------------------------------------------------------ 
RndmLong$ClientID: 1540

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     26      27      28      29      30 
 1.8238 -1.7020  0.0878 -1.4607  1.2512 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   4.6373     1.6804   2.760   0.0702 .
Session0     -0.2185     0.1838  -1.189   0.3199  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.819 on 3 degrees of freedom
Multiple R-squared:  0.3203,    Adjusted R-squared:  0.09379 
F-statistic: 1.414 on 1 and 3 DF,  p-value: 0.3199

------------------------------------------------------------ 
RndmLong$ClientID: 2039

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     31      32      33      34      35 
 0.5442 -1.1692 -0.6984 -0.1480  1.4715 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  2.09163    1.44704   1.445    0.244
Session0    -0.02286    0.10834  -0.211    0.846

Residual standard error: 1.203 on 3 degrees of freedom
Multiple R-squared:  0.01463,   Adjusted R-squared:  -0.3138 
F-statistic: 0.04454 on 1 and 3 DF,  p-value: 0.8464

------------------------------------------------------------ 
RndmLong$ClientID: 2578

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     36      37      38      39      40 
 0.1239  0.3173 -0.7597  0.6067 -0.2881 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  5.11349    0.46553  10.984  0.00162 **
Session0    -0.08044    0.04164  -1.932  0.14889   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6176 on 3 degrees of freedom
Multiple R-squared:  0.5544,    Adjusted R-squared:  0.4058 
F-statistic: 3.732 on 1 and 3 DF,  p-value: 0.1489

------------------------------------------------------------ 
RndmLong$ClientID: 2983

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      41       42       43       44       45 
-0.16092  0.14607  0.03212  0.53772 -0.55499 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept) 5.127752   0.399302  12.842  0.00102 **
Session0    0.009937   0.046107   0.216  0.84319   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4638 on 3 degrees of freedom
Multiple R-squared:  0.01525,   Adjusted R-squared:  -0.313 
F-statistic: 0.04644 on 1 and 3 DF,  p-value: 0.8432

------------------------------------------------------------ 
RndmLong$ClientID: 3600

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      46       47       48       49       50 
-0.29675  0.03494 -0.97922  1.89035 -0.64932 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  3.39206    1.52325   2.227    0.112
Session0     0.04242    0.13288   0.319    0.771

Residual standard error: 1.297 on 3 degrees of freedom
Multiple R-squared:  0.03285,   Adjusted R-squared:  -0.2895 
F-statistic: 0.1019 on 1 and 3 DF,  p-value: 0.7705

------------------------------------------------------------ 
RndmLong$ClientID: 3748

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      51       52       53       54       55 
-0.45080  0.93546  0.09437 -0.52268 -0.05635 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  1.03581    0.57534   1.800    0.170
Session0    -0.21264    0.09074  -2.343    0.101

Residual standard error: 0.6742 on 3 degrees of freedom
Multiple R-squared:  0.6467,    Adjusted R-squared:  0.5289 
F-statistic: 5.491 on 1 and 3 DF,  p-value: 0.1009

------------------------------------------------------------ 
RndmLong$ClientID: 4017

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      56       57       58       59       60 
 0.26714 -0.15368  0.00644 -0.61559  0.49568 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  2.23595    0.45737   4.889   0.0164 *
Session0    -0.16471    0.03788  -4.349   0.0225 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4898 on 3 degrees of freedom
Multiple R-squared:  0.8631,    Adjusted R-squared:  0.8174 
F-statistic: 18.91 on 1 and 3 DF,  p-value: 0.02246

------------------------------------------------------------ 
RndmLong$ClientID: 4488

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
       61        62        63        64        65 
 0.001669  0.521712 -0.510843 -0.045551  0.033013 

Coefficients:
            Estimate Std. Error t value  Pr(>|t|)    
(Intercept)  8.67916    0.29167  29.757 0.0000834 ***
Session0    -0.05812    0.02776  -2.094     0.127    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4228 on 3 degrees of freedom
Multiple R-squared:  0.5937,    Adjusted R-squared:  0.4582 
F-statistic: 4.383 on 1 and 3 DF,  p-value: 0.1273

------------------------------------------------------------ 
RndmLong$ClientID: 4854

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     66      67      68      69      70 
 0.4984 -0.7896 -0.2480  0.8098 -0.2706 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  6.40250    0.80019   8.001  0.00407 **
Session0    -0.11262    0.06997  -1.610  0.20586   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7444 on 3 degrees of freedom
Multiple R-squared:  0.4634,    Adjusted R-squared:  0.2845 
F-statistic: 2.591 on 1 and 3 DF,  p-value: 0.2059

------------------------------------------------------------ 
RndmLong$ClientID: 5320

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     71      72      73      74      75 
-0.4121  0.2233 -0.1676  0.5102 -0.1538 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) -0.62417    0.33518  -1.862   0.1595  
Session0    -0.14753    0.05135  -2.873   0.0639 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.421 on 3 degrees of freedom
Multiple R-squared:  0.7334,    Adjusted R-squared:  0.6445 
F-statistic: 8.253 on 1 and 3 DF,  p-value: 0.0639

------------------------------------------------------------ 
RndmLong$ClientID: 6114

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      76       77       78       79       80 
-0.04382 -0.32102  2.24396 -0.52170 -1.35743 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   6.3648     2.5328   2.513   0.0867 .
Session0     -0.2398     0.1903  -1.260   0.2966  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.555 on 3 degrees of freedom
Multiple R-squared:  0.3462,    Adjusted R-squared:  0.1283 
F-statistic: 1.589 on 1 and 3 DF,  p-value: 0.2966

------------------------------------------------------------ 
RndmLong$ClientID: 6407

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     81      82      83      84      85 
 0.6233 -0.9159 -0.1976  1.0334 -0.5433 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  1.78203    1.02976   1.731    0.182
Session0    -0.07499    0.12379  -0.606    0.587

Residual standard error: 0.9362 on 3 degrees of freedom
Multiple R-squared:  0.109, Adjusted R-squared:  -0.188 
F-statistic: 0.367 on 1 and 3 DF,  p-value: 0.5874

------------------------------------------------------------ 
RndmLong$ClientID: 6559

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      86       87       88       89       90 
 0.09503 -0.03545 -0.62788  0.27986  0.28844 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  4.49464    0.44869  10.017  0.00212 **
Session0     0.01978    0.03487   0.567  0.61016   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4344 on 3 degrees of freedom
Multiple R-squared:  0.0969,    Adjusted R-squared:  -0.2041 
F-statistic: 0.3219 on 1 and 3 DF,  p-value: 0.6102

------------------------------------------------------------ 
RndmLong$ClientID: 7142

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     91      92      93      94      95 
-0.2880  0.5761  0.2737  0.1724 -0.7343 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.06634    0.56654  -0.117    0.914
Session0    -0.05314    0.04468  -1.189    0.320

Residual standard error: 0.5941 on 3 degrees of freedom
Multiple R-squared:  0.3205,    Adjusted R-squared:  0.09396 
F-statistic: 1.415 on 1 and 3 DF,  p-value: 0.3198

------------------------------------------------------------ 
RndmLong$ClientID: 7767

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     96      97      98      99     100 
 1.0376  0.6314 -2.2630  0.4364  0.1577 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.61480    1.16058   3.115   0.0527 .
Session0    -0.03206    0.10499  -0.305   0.7801  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.507 on 3 degrees of freedom
Multiple R-squared:  0.03014,   Adjusted R-squared:  -0.2931 
F-statistic: 0.09323 on 1 and 3 DF,  p-value: 0.7801

------------------------------------------------------------ 
RndmLong$ClientID: 9066

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    101     102     103     104     105 
-0.8117  0.1204  1.3825  0.2906 -0.9818 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  6.46252    0.82127   7.869  0.00428 **
Session0     0.01625    0.07645   0.213  0.84533   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.1 on 3 degrees of freedom
Multiple R-squared:  0.01483,   Adjusted R-squared:  -0.3136 
F-statistic: 0.04516 on 1 and 3 DF,  p-value: 0.8453

------------------------------------------------------------ 
RndmLong$ClientID: 9097

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    106     107     108     109     110 
 0.1710  0.1344 -1.4475  0.3723  0.7698 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   6.7369     1.2504   5.388   0.0125 *
Session0     -0.1322     0.0901  -1.468   0.2385  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9787 on 3 degrees of freedom
Multiple R-squared:  0.4179,    Adjusted R-squared:  0.2239 
F-statistic: 2.154 on 1 and 3 DF,  p-value: 0.2385

------------------------------------------------------------ 
RndmLong$ClientID: 9814

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    111     112     113     114     115 
 1.5780 -0.1342 -2.2272  0.2243  0.5592 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   3.8789     1.9063   2.035    0.135
Session0     -0.1239     0.1495  -0.829    0.468

Residual standard error: 1.616 on 3 degrees of freedom
Multiple R-squared:  0.1864,    Adjusted R-squared:  -0.08483 
F-statistic: 0.6872 on 1 and 3 DF,  p-value: 0.4679

------------------------------------------------------------ 
RndmLong$ClientID: 9998

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    116     117     118     119     120 
 0.3646 -0.8017  1.1655 -0.5472 -0.1813 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.40739    1.32511   2.571   0.0824 .
Session0     0.01101    0.11264   0.098   0.9283  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9067 on 3 degrees of freedom
Multiple R-squared:  0.003173,  Adjusted R-squared:  -0.3291 
F-statistic: 0.00955 on 1 and 3 DF,  p-value: 0.9283

------------------------------------------------------------ 
RndmLong$ClientID: 10398

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    121     122     123     124     125 
-1.1381  0.9662  1.3901 -1.5399  0.3217 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.53102    1.21714   2.901   0.0624 .
Session0     0.07828    0.15927   0.491   0.6568  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.487 on 3 degrees of freedom
Multiple R-squared:  0.07452,   Adjusted R-squared:  -0.234 
F-statistic: 0.2416 on 1 and 3 DF,  p-value: 0.6568

------------------------------------------------------------ 
RndmLong$ClientID: 11153

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    126     127     128     129     130 
 0.3641  0.1975  0.4548 -2.2425  1.2261 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   1.9946     1.1061   1.803    0.169
Session0     -0.1025     0.1248  -0.821    0.472

Residual standard error: 1.518 on 3 degrees of freedom
Multiple R-squared:  0.1836,    Adjusted R-squared:  -0.08852 
F-statistic: 0.6747 on 1 and 3 DF,  p-value: 0.4716

------------------------------------------------------------ 
RndmLong$ClientID: 11639

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    131     132     133     134     135 
-0.5633  0.3197  1.2701  0.3633 -1.3898 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) -3.35216    0.94537  -3.546   0.0382 *
Session0     0.01803    0.07878   0.229   0.8337  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.169 on 3 degrees of freedom
Multiple R-squared:  0.01716,   Adjusted R-squared:  -0.3104 
F-statistic: 0.05239 on 1 and 3 DF,  p-value: 0.8337

------------------------------------------------------------ 
RndmLong$ClientID: 11713

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
     136      137      138      139      140 
 0.27002  1.08393 -1.06589 -0.38107  0.09301 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  2.46789    0.63760   3.871   0.0305 *
Session0    -0.02542    0.07165  -0.355   0.7462  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9197 on 3 degrees of freedom
Multiple R-squared:  0.04028,   Adjusted R-squared:  -0.2796 
F-statistic: 0.1259 on 1 and 3 DF,  p-value: 0.7462

------------------------------------------------------------ 
RndmLong$ClientID: 12081

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
    141     142     143     144     145 
-1.4042  0.8308  0.6680  1.3966 -1.4913 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.385038   1.021952   0.377    0.731
Session0    -0.001875   0.093447  -0.020    0.985

Residual standard error: 1.558 on 3 degrees of freedom
Multiple R-squared:  0.0001342, Adjusted R-squared:  -0.3332 
F-statistic: 0.0004026 on 1 and 3 DF,  p-value: 0.9853

------------------------------------------------------------ 
RndmLong$ClientID: 12409

Call:
lm(formula = Anxiety ~ Session0, data = RndmLong)

Residuals:
      146       147       148       149       150 
-0.715243  2.248873 -0.389445  0.001022 -1.145207 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.95251    1.53686   2.572   0.0824 .
Session0     0.02669    0.14956   0.178   0.8697  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.531 on 3 degrees of freedom
Multiple R-squared:  0.01051,   Adjusted R-squared:  -0.3193 
F-statistic: 0.03185 on 1 and 3 DF,  p-value: 0.8697
\end{verbatim}

Looking at our data, we might observe:

\begin{itemize}
\tightlist
\item
  \(R^2\) values range from 0 to 74\%
\item
  Level of anxiety starts at different places
\item
  For many, anxiety decreases as a function of session

  \begin{itemize}
  \tightlist
  \item
    but not for all,
  \item
    and for a few it goes negative
  \end{itemize}
\end{itemize}

Next we superimpose each client's fitted OLS trajectory on their empirical growth plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0 }\SpecialCharTok{|}\NormalTok{ ClientID, }\AttributeTok{data=}\NormalTok{RndmLong,}
  \AttributeTok{panel =} \ControlFlowTok{function}\NormalTok{(x, y)\{}
    \FunctionTok{panel.xyplot}\NormalTok{(x, y)}
    \FunctionTok{panel.lmline}\NormalTok{(x, y)}
\NormalTok{  \},  }\AttributeTok{as.table=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-65-1.pdf}

What do we observe? A linear change trajectory

\begin{itemize}
\tightlist
\item
  is ideal for a few members
\item
  reasonable for many others
\item
  The \(R^2\) coincides well with those for whom the line is the best fit
\end{itemize}

Singer and Willett \citeyearpar{singer_applied_2003} can read your minds, ``Don't OLS regression methods assume independence and homoscedastic residuals?'' Why, yes! They do. Singer and Willett indicate OLS estimates are very useful for exploratory purposes in that they provide an unbiased estimate of the intercept and slope of the individual change.

\hypertarget{snapshot-of-the-entire-set-of-smooth-trajectories}{%
\subsubsection{Snapshot of the Entire Set of Smooth Trajectories**}\label{snapshot-of-the-entire-set-of-smooth-trajectories}}

We can plop all our trajectories in a set of smoothed individual trajectories. This first plot is simply the raw data. This first plot is just a smoothed (there is a line for each client) plot of the raw/natural metric of the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plot of raw data for every case}
\CommentTok{\#Session0 provided splotchy data; Index0 gives some indication of change}
\FunctionTok{interaction.plot}\NormalTok{(RndmLong}\SpecialCharTok{$}\NormalTok{Index, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID, RndmLong}\SpecialCharTok{$}\NormalTok{Anxiety)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-66-1.pdf}

The next two sets of code plot our fitted trajectories into a single plot.

First, we fit the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fitting the linear model by ID}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(RndmLong, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID, }
          \ControlFlowTok{function}\NormalTok{(bydata) }\FunctionTok{fitted.values}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{bydata)))}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

Then make the plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plotting the linear fit by ID}
\FunctionTok{interaction.plot}\NormalTok{(RndmLong}\SpecialCharTok{$}\NormalTok{Index, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID, fit, }\AttributeTok{xlab=}\StringTok{"Sessions"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Anxiety"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-68-1.pdf}

What do we observe? Clients present with varying levels of anxiety. On average, change in anxiety declined (at least a little) from the first wave to the fifth.

\hypertarget{examining-intercepts-slopes-and-their-relationship}{%
\subsection{Examining intercepts, slopes, and their relationship}\label{examining-intercepts-slopes-and-their-relationship}}

\emph{Sample means of the estimated intercepts and slopes} (level-1 OLS estimated intercepts and slopes) are unbiased estimates of initial status and rate of change \emph{for each person}. Their sample means are, therefore, unbiased estimates of the key features of the average observed change trajectory.

\emph{Sample variances (SDs) of the estimated intercepts and slopes} quantify the amount of observed interindividual heterogeneity in change.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#obtaining the intercept from the linear model by ClientID}
\NormalTok{ints }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(RndmLong, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID,}
          \ControlFlowTok{function}\NormalTok{(data) }\FunctionTok{coefficients}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{data))[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{ints1 }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(ints)}
\FunctionTok{names}\NormalTok{(ints1) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\FunctionTok{mean}\NormalTok{(ints1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.239564
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(ints1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.688367
\end{verbatim}

Our calculations tell us that at Session 1, anxiety is 3.23 (\emph{SD} = 2.69).

The next two values calculate the slope and its variation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#obtaining the slopes from linear model by id}
\NormalTok{slopes }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(RndmLong, RndmLong}\SpecialCharTok{$}\NormalTok{ClientID,}
            \ControlFlowTok{function}\NormalTok{(data) }\FunctionTok{coefficients}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{data))[[}\DecValTok{2}\NormalTok{]])}
\NormalTok{slopes1 }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(slopes)}
\FunctionTok{names}\NormalTok{(slopes1) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\FunctionTok{mean}\NormalTok{(slopes1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.06980594
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(slopes1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.08853341
\end{verbatim}

Here we learn that the average slope is -0.07 (\emph{SD} = 0.08). On average, anxiety decreases by .07 points per session. Relative to their means, the magnitudes of the \emph{SD}s around the slope and intercept are pretty big, so there is a great deal of variation.

\textbf{Sample correlation between the estimated intercepts and slopes} summarizes the association between the fitted initial status and fitted rate of change. It answers the question, ``Are observed initial status and rate of change related?''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{( ints1, slopes1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.008575535
\end{verbatim}

Not really. \emph{r} = 0.01.

\hypertarget{exploring-the-relationship-between-change-and-time-invariant-predictors}{%
\subsection{Exploring the relationship between Change and Time-Invariant Predictors}\label{exploring-the-relationship-between-change-and-time-invariant-predictors}}

Looking at our level-2 predictors can help uncover \emph{systematic interindividual differences in change}. In this vignette religious affiliation and sexual identity are our L2 predictors.

Let's start with religious affiliation Is there a difference in intercept (initial tolerance) or slope (rate of change) as a function of religious affiliation?

We start by selecting DR, fitting a regression model, and plotting it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fitting the linear model by ID, DR only}
\NormalTok{DR }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(RndmLong, DRel0}\SpecialCharTok{==}\StringTok{"0"}\NormalTok{)}
\NormalTok{fitmlist }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(DR, DR}\SpecialCharTok{$}\NormalTok{ClientID, }\ControlFlowTok{function}\NormalTok{(bydata) }\FunctionTok{fitted.values}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{bydata)))}
\NormalTok{fitDR }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(fitmlist)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#appending the average for the whole group of DR}
\NormalTok{lm.DR }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{( }\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{DR) )}
\FunctionTok{names}\NormalTok{(lm.DR) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{fit.DR2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(fitDR, lm.DR[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\NormalTok{Sess1.DR }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(DR}\SpecialCharTok{$}\NormalTok{Index, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{))}\CommentTok{\#Note that I used Session0 to create the lm, but plotted by Index0}
\NormalTok{id.DR }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(DR}\SpecialCharTok{$}\NormalTok{ClientID, }\FunctionTok{rep}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plotting the linear fit by id, males}
\CommentTok{\#id.m=111 denotes the average value for males}
\FunctionTok{interaction.plot}\NormalTok{(Sess1.DR, id.DR, fit.DR2, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{), }\AttributeTok{xlab=}\StringTok{"Sessions"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Dominant Religious"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-75-1.pdf}
Trajectories for those from dominantreligions stay flat; a few decline.

Now for non-dominant religious (including those that are affiliated and unaffiliated).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fitting the linear model by ID, DR only}
\NormalTok{NDR }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(RndmLong, DRel0}\SpecialCharTok{==}\StringTok{"1"}\NormalTok{)}
\NormalTok{fitmlist }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(NDR, NDR}\SpecialCharTok{$}\NormalTok{ClientID, }\ControlFlowTok{function}\NormalTok{(bydata) }\FunctionTok{fitted.values}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{bydata)))}
\NormalTok{fitNDR }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(fitmlist)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#appending the average for the whole group of males}
\NormalTok{lm.NDR }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{( }\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{NDR) )}
\FunctionTok{names}\NormalTok{(lm.NDR) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{fit.NDR2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(fitNDR, lm.NDR[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\NormalTok{Sess1.NDR }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(NDR}\SpecialCharTok{$}\NormalTok{Index, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{))}\CommentTok{\#Note that I used Session0 to create the lm, but plotted by Index0}
\NormalTok{id.NDR }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(NDR}\SpecialCharTok{$}\NormalTok{ClientID, }\FunctionTok{rep}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plotting the linear fit by id, males}
\CommentTok{\#id.m=111 denotes the average value for males}
\FunctionTok{interaction.plot}\NormalTok{(Sess1.NDR, id.NDR, fit.NDR2, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{), }\AttributeTok{xlab=}\StringTok{"Sessions"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Non{-}Dominant Religious"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-78-1.pdf}

It appears that there is more decline in anxiety for those who claim non-dominant religions.

What about the effects of sexual identity?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fitting the linear model by ID, HET = 0 only}
\NormalTok{HET }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(RndmLong, Het0}\SpecialCharTok{==}\StringTok{"0"}\NormalTok{)}
\NormalTok{fitmlist }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(HET, HET}\SpecialCharTok{$}\NormalTok{ClientID, }\ControlFlowTok{function}\NormalTok{(bydata) }\FunctionTok{fitted.values}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{bydata)))}
\NormalTok{fitHET }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(fitmlist)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#appending the average for the whole group of males}
\NormalTok{lm.HET }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{( }\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{HET) )}
\FunctionTok{names}\NormalTok{(lm.HET) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{fit.HET }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(fitHET, lm.HET[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\NormalTok{Sess1.HET }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(HET}\SpecialCharTok{$}\NormalTok{Index, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{))}\CommentTok{\#Note that I used Session0 to create the lm, but plotted by Index0}
\NormalTok{id.HET }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(HET}\SpecialCharTok{$}\NormalTok{ClientID, }\FunctionTok{rep}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plotting the linear fit by id, males}
\CommentTok{\#id.m=111 denotes the average value for males}
\FunctionTok{interaction.plot}\NormalTok{(Sess1.HET, id.HET, fit.HET, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{), }\AttributeTok{xlab=}\StringTok{"Sessions"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"Heterosexual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-81-1.pdf}

Among clients who identify as heterosexual, there is a mixed profile. Clients start with varying degrees of anxiety and we see it stay the same, increase, and decrease.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fitting the linear model by ID, DR only}
\NormalTok{LGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(RndmLong, Het0}\SpecialCharTok{==}\StringTok{"1"}\NormalTok{)}
\NormalTok{fitmlist }\OtherTok{\textless{}{-}} \FunctionTok{by}\NormalTok{(LGBQQ, LGBQQ}\SpecialCharTok{$}\NormalTok{ClientID, }\ControlFlowTok{function}\NormalTok{(bydata) }\FunctionTok{fitted.values}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{bydata)))}
\NormalTok{fitLGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(fitmlist)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#appending the average for the whole group of males}
\NormalTok{lm.LGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{( }\FunctionTok{lm}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Session0, }\AttributeTok{data=}\NormalTok{LGBQQ) )}
\FunctionTok{names}\NormalTok{(lm.LGBQQ) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{fit.LGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(fitLGBQQ, lm.LGBQQ[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\NormalTok{Sess1.LGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(LGBQQ}\SpecialCharTok{$}\NormalTok{Index, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{)) }\CommentTok{\#Note that I used Session0 to create the lm, but plotted by Index0}
\NormalTok{id.LGBQQ }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(LGBQQ}\SpecialCharTok{$}\NormalTok{ClientID, }\FunctionTok{rep}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plotting the linear fit by id, males}
\CommentTok{\#id.m=111 denotes the average value for males}
\FunctionTok{interaction.plot}\NormalTok{(Sess1.LGBQQ, id.LGBQQ, fit.LGBQQ, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{), }\AttributeTok{xlab=}\StringTok{"Sessions"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{1}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"LGBQQ"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-84-1.pdf}

What do we observe? Clients who identify as LGBQQ start at different levels of anxiety; most decline somewhere in the middle and then maintain in a consistent/level manner.

\hypertarget{the-relationship-between-ols-estimated-trajectories-and-substantive-predictors}{%
\subsection{The Relationship between OLS-Estimated Trajectories and Substantive Predictors}\label{the-relationship-between-ols-estimated-trajectories-and-substantive-predictors}}

Below are two plots (and corresponding correlation coefficients) for religious affiliation and sexual identity regarding intercept (or initial status). They help us answer the question, is initial status different as a function of gender (then, exposure level).

\emph{NOTE} that these are calculated from the the wide-format.

\textbf{First, is anxiety at Session0 (our initial, or intercept) different for those from dominant and nondominant religions?}

For these analyses of intercepts, the fitted model that we are using for the plot and correlation keeps the assessment at the Session0 intercept.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Using the slopes and intercepts from the linear model fitted by id}
\CommentTok{\#generated for use in table 2.3}
\FunctionTok{plot}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{DRel0, ints1, }\AttributeTok{xlab=}\StringTok{"Religion"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Fitted initial status"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-85-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{DRel0, ints1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1587707
\end{verbatim}

For religion, looking at the dots on 0 and 1 and the correlation, it looks like the anxiety intercept is lower for those from the dominant religion.

\textbf{Next, is Anxiety at Session 1 different as a function of level of sexual identity?}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{Het0, ints1, }\AttributeTok{xlab=}\StringTok{"Sexual Identity"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Fitted initial status"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-87-1.pdf}
The plot of this random sample of data emphasizes that those who identify as LGBQQ are, proportionately, much smaller. Their range of anxiety is more restricted, but higher.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{Het0, ints1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1309987
\end{verbatim}

Looking at the correlation of intercepts and plot together, wee see that anxiety is higher for those who identify as LGBQQ.

\textbf{Next, we look at 2 more plots and correlations for religion and sexual identity regarding slope/rate of change.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{DRel0, slopes1, }\AttributeTok{xlab=}\StringTok{"Religion"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Fitted rate of change"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-89-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{DRel0, slopes1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.09496023
\end{verbatim}

The plot is the rate of change or slope. It's maybe a little more difficult to plot. We see that dominant religions have a slower rate of change than those who claim a nondominant religion (or no religion).

\textbf{What about change in anxiety as a function of sexual identity?}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{Het0, slopes1, }\AttributeTok{xlab =} \StringTok{"Sexual Identity"}\NormalTok{, }\AttributeTok{ylab =}
  \StringTok{"Fitted rate of change"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-91-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(RndmSmpl30}\SpecialCharTok{$}\NormalTok{Het0, slopes1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1113146
\end{verbatim}

Those with who identify as LGBQQ have sharper rates of change.

\hypertarget{apa-style-writeup-1}{%
\subsection{APA Style Writeup}\label{apa-style-writeup-1}}

\textbf{Method/Analytic Strategy}

COMING SOON: Will be completed in next lesson

\textbf{Results}

\textbf{Preliminary Analyses}

Preliminary analysis involved the creation and visual inspection of empirical growth plots with parametrical and nonparmetrical smoothing. We also calculated and plotted intercepts, slopes, and their relationship. Results suggested that intercepts and slopes differed across the clients. Thus, utilizing multi-level modeling as the framework for analyzing the data is justified.

\textbf{Primary Analyses}

COMING SOON: Will be completed as we work the problem in the next lesson(s).

\hypertarget{observations-about-the-social-and-cultural-responsivity-of-the-project}{%
\section{Observations about the Social and Cultural Responsivity of the Project}\label{observations-about-the-social-and-cultural-responsivity-of-the-project}}

\begin{itemize}
\tightlist
\item
  Accessing and analyzing \emph{big data} is a strength; it is through these collaborative endeavors that we get greater access to sample sizes representing populations that are marginalized in numbers for which it is possible to analyze.
\item
  Heterosexual and dominant religions are still the basis for comparison. Is this a strength or limitation? It depends on the goal of the project. This dataset, though, may have sufficient representation among marginalized groups for within-group analysis without comparison.
\item
  In the simulation, nondominant religions disappeared. They had been only 5\% of the original sample. The authors describe how they ran the models with and without this subgroup and chose to leave them in the dataset.
\end{itemize}

\hypertarget{residual-and-related-questions}{%
\section{Residual and Related Questions\ldots{}}\label{residual-and-related-questions}}

..that you might have; or at least I had, but if had answered them earlier it would have disrupt the flow.

\hypertarget{practice-problems-1}{%
\section{Practice Problems}\label{practice-problems-1}}

The assignment is designed to span several lessons. Therefore, at this stage, please select a longitudinal dataset that will allow you to engage in the preliminary exploring, model building (including both exploration of an unconditional growth model and adding at least one L2 variables), and writing up a complete multilevel model for change (as specified below). Minimally, you should have a time-changing dependent variable and corresponding time-covarying (L1) predictor with a minimum of three waves each; our time-covarying predictor is Session. Variables will be clocked with a ``sensible time metric.'' You should also have a time invariant L2 predictor.

FROM THIS LESSON

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Restructure the dataset from wide to long.
\item
  Provide three examples of data exploration

  \begin{itemize}
  \tightlist
  \item
    An unfitted model
  \item
    A model fitted with a linear growth trajectory
  \item
    The fitted (or unfitted) data identified by the L2 predictor
  \end{itemize}
\end{enumerate}

FROM SUBSEQUENT LESSONS

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Using a staged approach to model development, report on at least four models, these must include

  \begin{itemize}
  \tightlist
  \item
    An unconditional means model
  \item
    An unconditional growth model
  \item
    An intermediary model (please test both a time variable and an L2 variable)
  \item
    A final model
  \end{itemize}
\item
  Write up the Results as demonstrated in the lecture
\item
  Table (use the tab\_model outfile) and Figure are required
\end{enumerate}

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-1}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-1}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation, then rework the problem. This should provide minor changes to the data (maybe in the second or third decimal point), but the results will likely be very similar.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Restructure the dataset from wide to long (or from long to wide) & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Provide a write-up of what you found in this process & 5 & \_\_\_\_\_ \\
6. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 20 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-2-rework-the-research-vignette-using-the-second-set-of-simulated-data-where-depression-is-the-outcome-variable}{%
\subsection{Problem \#2: Rework the research vignette using the second set of simulated data where depression is the outcome variable}\label{problem-2-rework-the-research-vignette-using-the-second-set-of-simulated-data-where-depression-is-the-outcome-variable}}

Use the simulated data, but select one of the other models that was evaluated in the Lewis et al. \citep{lewis_applying_2017} study. Compare your results to those reported in the mansucript.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Restructure the dataset from wide to long (or from long to wide) & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Provide a write-up of what you found in this process & 5 & \_\_\_\_\_ \\
6. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & & \\
\bottomrule
\end{longtable}

\hypertarget{problem-3-use-other-data-that-is-available-to-you-1}{%
\subsection{Problem \#3: Use other data that is available to you}\label{problem-3-use-other-data-that-is-available-to-you-1}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete the exploratory analyses.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Restructure the dataset from wide to long (or from long to wide) & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Provide a write-up of what you found in this process & 5 & \_\_\_\_\_ \\
6. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & & \\
\bottomrule
\end{longtable}

\hypertarget{bonus-track-1}{%
\section{Bonus Track:}\label{bonus-track-1}}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.45833in,height=2.19792in]{images/film-strip-1.jpg}
\caption{Image of a filmstrip}\label{id}
}
\end{figure}

\hypertarget{simulated-data-when-depression-is-the-outcome}{%
\subsection{Simulated Data when Depression is the Outcome}\label{simulated-data-when-depression-is-the-outcome}}

One suggestion for practice is to work the second MLM example in the Lefevor et al. \citep{lefevor_religious_2017} example. The code below will simulate the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200513}\NormalTok{)}
\NormalTok{n\_client }\OtherTok{=} \DecValTok{12825}
\NormalTok{n\_session }\OtherTok{=} \DecValTok{5}
\NormalTok{b0 }\OtherTok{=} \FloatTok{1.84} \CommentTok{\#intercept for depression}
\NormalTok{b1 }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{28} \CommentTok{\#b weight for L1 session}
\NormalTok{b2 }\OtherTok{=}\NormalTok{ .}\DecValTok{15} \CommentTok{\#b weight for L2 sexual identity}
\NormalTok{b3 }\OtherTok{=}  \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{06} \CommentTok{\#b weight for L2 Rel1 (D{-}R vs ND{-}R \& ND{-}U)}
\NormalTok{b4 }\OtherTok{=} \FloatTok{0.04} \CommentTok{\#b weight for the L2 Rel2 (ND{-}R vs ND{-}U)}
\CommentTok{\#the values used below are the +/{-} 3SD they produce continuous variables which later need to be transformed to categorical ones; admittedly this introduces a great deal of error/noise into the simulation}
\CommentTok{\#the article didn\textquotesingle{}t include a correlation matrix or M/SDs so this was a clunky process }
\NormalTok{( }\AttributeTok{Session =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\SpecialCharTok{{-}}\FloatTok{3.67}\NormalTok{, }\FloatTok{3.12}\NormalTok{)) }\CommentTok{\#calc L1 Session, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{SexualIdentity =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{6.64}\NormalTok{, }\FloatTok{6.94}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Sexual Identity, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion1 =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{3.46}\NormalTok{, }\FloatTok{3.34}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Religion1, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion2 =} \FunctionTok{rep}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{3.44}\NormalTok{, }\FloatTok{3.36}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Religion2, values are the +/3 3SD}
\NormalTok{mu }\OtherTok{=} \FloatTok{1.49} \CommentTok{\#intercept of empty model }
\NormalTok{sds }\OtherTok{=} \FloatTok{2.264} \CommentTok{\#this is the SD of the DV}
\NormalTok{sd }\OtherTok{=} \DecValTok{1} \CommentTok{\#this is the observation{-}level random effect variance that we set at 1}

\CommentTok{\#( church = rep(LETTERS[1:n\_church], each = n\_mbrs) ) \#this worked in the prior}
\NormalTok{( }\AttributeTok{client =} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_client], }\AttributeTok{each =}\NormalTok{ n\_session) )}
\CommentTok{\#( session = numbers[1:(n\_client*n\_session)] )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rnorm}\NormalTok{(n\_client, }\DecValTok{0}\NormalTok{, sds) )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rep}\NormalTok{(clienteff, }\AttributeTok{each =}\NormalTok{ n\_session) )}
\NormalTok{( }\AttributeTok{sessioneff =} \FunctionTok{rnorm}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\DecValTok{0}\NormalTok{, sd) )}
\NormalTok{( }\AttributeTok{Depression =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{Session }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{SexualIdentity }\SpecialCharTok{+}\NormalTok{ b3}\SpecialCharTok{*}\NormalTok{Religion1 }\SpecialCharTok{+}\NormalTok{ b4}\SpecialCharTok{*}\NormalTok{Religion2 }\SpecialCharTok{+}\NormalTok{ clienteff }\SpecialCharTok{+}\NormalTok{ sessioneff)}
\NormalTok{( }\AttributeTok{dat =} \FunctionTok{data.frame}\NormalTok{(client, clienteff, sessioneff, Session, SexualIdentity, Religion1, Religion2, Depression) )}

\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())}
\CommentTok{\#moving the ID number to the first column; requires }
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{Lefevor2017D }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, client, Session, SexualIdentity, Religion1, Religion2, Depression)}

\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{ClientID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12825}\NormalTok{), }\AttributeTok{each =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#rounded Sexual Identity into dichotomous variable}
\CommentTok{\#85\% were heterosexual, }

\CommentTok{\#Rel2 has contrast codes for dominant religion (DR, 0), nondominant religious (NDR, 1) and nondominant unspecified (NDU, {-}1)}
\CommentTok{\#Strategy is to figure out the raw score associated with the percentile rank of  {-}1 and 0, to set the breakpoints for the coding}
\CommentTok{\#NDU coded as {-}1}
\CommentTok{\#19.2+13.5+9.6}
\CommentTok{\#NDU has bottom 42.3 percent}

\CommentTok{\#DR coded as 0, so quantile cut will be 42.3 + 52.7 = 95th}
\CommentTok{\#33.4 + 19.3}
\CommentTok{\#52.7\% of sample (according to article) was DR}
\CommentTok{\#must look up percentile ranks for 5\% and 57.5\%}

\CommentTok{\#NDR}
\CommentTok{\#2.3+1+1+.7}
\CommentTok{\#NDR has 5\% of sample}
\CommentTok{\#42.3+52.7}
\FunctionTok{quantile}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{Religion2, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(.}\DecValTok{423}\NormalTok{, .}\DecValTok{95}\NormalTok{))}
\CommentTok{\#effects coding the second Religion variable so that NDU = {-}1, DR = 0, NDR = 1}
\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{Rel2L2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{0.3304528}\NormalTok{ , }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
                             \FunctionTok{ifelse}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\FloatTok{0.3304529} \SpecialCharTok{\&}\NormalTok{ Lefevor2017D}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \FloatTok{2.4446784}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\CommentTok{\#creating the  religion variable where DR is 0 and NDR and NDU are both 1}
\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{DRel0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The following `from` values were not present in `x`: 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#checking }
\CommentTok{\#DRel0\_table \textless{}{-} table(Lefevor2017D$DRel0)}
\CommentTok{\#prop.table(DRel0\_table)}
\CommentTok{\#heterosexual is {-}1}
\CommentTok{\#LGBQQ is 1}
\CommentTok{\#quantile(Lefevor2017D$SexualIdentity, probs = c(.85))}
\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{SexID }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{SexualIdentity }\SpecialCharTok{\textless{}=} \FloatTok{5.747946}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{Het0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{SexID, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\#Het0\_table \textless{}{-} table(Lefevor2017D$Het0)}
\CommentTok{\#prop.table(Het0\_table)\#to make sure that 85\% are coded 0 for Het}

\CommentTok{\#creating a variable representing the session number for each client, in the article up to 20 sessions were allowed. }
\CommentTok{\#install.packages("scales")}
\FunctionTok{library}\NormalTok{(scales)}
\CommentTok{\#Right from the beginning I centered this so that 0 would represent intake}
\NormalTok{Lefevor2017D}\SpecialCharTok{$}\NormalTok{Session0 }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(Lefevor2017D}\SpecialCharTok{$}\NormalTok{Session, }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{)))}

\CommentTok{\#creating session waves (1 thru 5) by rank ordering within each person\textquotesingle{}s variable the continuous variable Session that was created in the original simulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{Lefevor2017D }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017D}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(ClientID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Index =} \FunctionTok{rank}\NormalTok{(Session))}

\CommentTok{\#selecting the simulated variables}
\NormalTok{Lefevor2017D\_sim }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017D}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, Session0, Depression, DRel0, Het0)}

\CommentTok{\#In the transition from long{-}to{-}wide, it seems like you can only do one L1 variable at a time}
\CommentTok{\#When there are multiple L1 and L2 vars, put all L2 vars on left of tilde}
\CommentTok{\#The wave/index function should come next; this should be finite (like integers of 1,2,3,4) with a maximum}
\CommentTok{\#Put the name of the SINGLE L1 variable in the concatonated list}
\FunctionTok{library}\NormalTok{(data.table)}
\NormalTok{DLfvrWp1}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017D\_sim, ClientID }\SpecialCharTok{+}\NormalTok{ DRel0 }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Index"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{DLfvrWp1}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(DLfvrWp1, }\AttributeTok{Index1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Index2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Index3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Index4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Index5 =} \StringTok{"5"}\NormalTok{)}
\NormalTok{DLfvrWp2}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017D\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Depression"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{DLfvrWp2}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(DLfvrWp2, }\AttributeTok{Dep1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Dep2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Dep3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Dep4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Dep5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#For remaining L1 variable, do them one at a time {-}{-} key them from the person{-}level ID and the wave/index.}
\NormalTok{DLfvrWp3}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017D\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Session0"}\NormalTok{))}
\NormalTok{DLfvrWp3}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(DLfvrWp3, }\AttributeTok{Sess1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Sess2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Sess3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Sess4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Sess5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#Next, join the dataframes by the person{-}level ID}
\CommentTok{\#Only two can be joined at a time}
\NormalTok{DLfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(DLfvrWp1, DLfvrWp2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\NormalTok{DLfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(DLfvrWide, DLfvrWp3,  }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The parallel dataset with depression as the outcome is called: DLfvrWide Here is script to save it as an outfile and then import it back into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(DLfvrWide, }\AttributeTok{file=}\StringTok{"DLfvrWide.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{DLfvrWide }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"DLfvrWide.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{LongMod}{%
\chapter{A Basic Longitudinal Model}\label{LongMod}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=dedb85c8-e442-49bf-8f92-ad31018b1dd8}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen=}\DecValTok{999}\NormalTok{)}\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

The focus of this lecture is to introduce longitudinal modeling with the multilevel modeling approach. Longitudinal MLM is a complicated statistic with decisions-to-be-made at every point. We will continue with the Lefevor et al. \citeyearpar{lefevor_religious_2017} example that served as the research vignette for the preliminary investigation that preceded model building. In this lesson's example, we will engage in model building that sequentially specifies and analyzes a within-persons (Level 1 {[}L1{]}) predictor of time, two between-persons (Level 2 {[}L2{]}) predictors of sexual identity and religious affiliation, and cross-level actions. We will also trim non-significant effects. In-so-doing, we examine the conceptual and technical aspects of multilevel modeling and assemble the model (and explore the data) in a systematic and sequential manner.

\hypertarget{navigating-this-lesson-2}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-2}}

There is about 1 hour and 45 minutes of lecture. If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the lesson, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_CPA}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-2}{%
\subsection{Learning Objectives}\label{learning-objectives-2}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  In the taxonomy of model building, define and distinguish between unconditional means models and unconditional growth models.
\item
  List differences between FML and RML. Specifically, what effects do they cover/not-cover and how does that impact our interpretation of deviance statistics?
\item
  Define and interpret the intraclass correlation (ICC).
\item
  Articulate a strategy for model specification and evaluation (building and trimming).
\item
  Given a taxonomy of model comparisons (such as we grow in the Viewer pane), be able to interpret both fixed and random effects. Be able to calculate pseudo R2 for the three primary variance components (\(\sigma^2\), \(\tau _{00}\), \(\tau _{11}\))
\item
  In terms of overall model evaluation, interpret deviance and AIC statistics (and know when such interpretation is appropriate).
\end{itemize}

\hypertarget{planning-for-practice-2}{%
\subsection{Planning for Practice}\label{planning-for-practice-2}}

This assignment grows from the \protect\hyperlink{MLMexplore}{lesson} on preliminary exploration of longitudinal data and extends it with model specification and evaluation. The suggested practice problem for this lesson is to conduct a longitudinal MLM that includes at least one L1 predictor (ideally one that clocks time), at least one L2 predictor, and a cross-level interaction. Specific details are provided at the end of the lesson.

As is typical for this OER, the suggestions are practice are graded in complexity. I encourage you to select one or more options that provides some stretch.

\begin{itemize}
\tightlist
\item
  Rework the problem in the lesson by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar.
\item
  In this (and the prior) lesson(s) we work the problem with anxiety as the dependent variable. In the previous lesson's \protect\hyperlink{MLMexplore}{Bonus Track} there is a parallel dataset that is simulated with depression as the DV. Use that dataset for the homework. one in the lesson (and/or oNe you mimicked in the journal article).
\item
  Work through the steps in the assignment with data to which you have access. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources-2}{%
\subsection{Readings \& Resources}\label{readings-resources-2}}

In preparing this lesson, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Singer, J. D., \& Willett, J. B. (2003). Doing data analysis with the multilevel model for change/Chapter 4 in \emph{Applied longitudinal data analysis: Modeling change and event occurrence.} Oxford University Press. \url{https://doi-org.ezproxy.spu.edu/10.1093/acprof:oso/9780195152968.001.0001}

  \begin{itemize}
  \tightlist
  \item
    The \href{https://stats.idre.ucla.edu/other/examples/alda/}{UCLA IDRE website} hosts R solutions (as well as SPSS, SAS, MPlus, and HLM) to many of the examples in this text.
  \end{itemize}
\item
  Lefevor, G. T., Janis, R. A., \& Park, S. Y. (2017). Religious and sexual identities: An intersectional, longitudinal examination of change in therapy. \emph{The Counseling Psychologist, 45}(3), 387--413. \url{https://doi-org.ezproxy.spu.edu/10.1177/0011000017702721}
\end{itemize}

\hypertarget{packages-3}{%
\subsection{Packages}\label{packages-3}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#will install the package if not already installed}
\CommentTok{\#if(!require(lme4))\{install.packages("lme4")\}}
\CommentTok{\#if(!require(sjStats))\{install.packages("sjStats")\}}
\CommentTok{\#if(!require(sjPlot))\{install.packages("sjPlot")\}}
\CommentTok{\#if(!require(formattable))\{install.packages("formattable")\}}
\CommentTok{\#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{\#if(!require(robumeta))\{install.packages("robumeta")\}}
\CommentTok{\#if(!require(scales))\{install.packages("scales")\}}
\CommentTok{\#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{\#if(!require(data.table))\{install.packages("data.table")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-basic-longitudinal-multilevel-model}{%
\section{The Basic, Longitudinal, Multilevel Model}\label{the-basic-longitudinal-multilevel-model}}

\hypertarget{the-definitional-and-conceptual}{%
\subsection{The definitional and conceptual}\label{the-definitional-and-conceptual}}

Singer and Willett \citeyearpar{singer_applied_2003} tell us that the focus of this er is on ``real-world issues of analysis'' (p.~75). To that end, they introduce some complexities:

\emph{Composite model}: Combines the

\begin{itemize}
\tightlist
\item
  L1 how each person changes over time, and
\item
  L2 relates interindividual differences in change to predictors submodels into a single equation.
\end{itemize}

This composite model ``leads naturally'' to consideration of alternate methods of analysis

\begin{itemize}
\tightlist
\item
  generalized least squares (GLS)
\item
  iterative generalized least squares (IGLS)
\end{itemize}

And within maximum likelihood
* full
* restricted

In addition to the exploration demonstrated the prior \protect\hyperlink{MLMexplore}{lesson}, longitudinal model building should always start by fitting two models that provide essential baselines for subsequent comparison:

\begin{itemize}
\tightlist
\item
  unconditional means model
\item
  unconditional growth model
\end{itemize}

There are multiple ways to add time-invariant (L2) predictors, to test complex hypotheses, and examine model assumptions and residuals. Additionally, we can get ``model-based'' estimates of individual growth trajectories that improve upon the exploratory OLS estimates we did completed in the prior \protect\hyperlink{MLMexplore}{lesson}. In this example, the approach is somewhat streamlined because we are (a) specifying \emph{linear} (not curvilinear) growth model, (b) using data that is balanced (everyone has the same number of waves), (c) using data with no missingness. We are using time-unstructured data. That is, while everyone has five waves, the amount of time (sessions) represented, ranges from 0 to 19.

\hypertarget{workflow-for-a-basic-longitudinal-multilevel-model}{%
\section{Workflow for a Basic, Longitudinal, Multilevel Model}\label{workflow-for-a-basic-longitudinal-multilevel-model}}

Multilevel modeling is a complex statistic with a number of considerations. At the risk of being an oversimplification, this workflow provides a rough map the steps involved in preliminary exploration and model building.

\begin{figure}
\centering
\includegraphics{images/LongExpl/WrkFlowLong.jpg}
\caption{Image of a workflow through a longitudinal multilevel model}
\end{figure}

\hypertarget{research-vignette-2}{%
\section{Research Vignette}\label{research-vignette-2}}

We continue with the same research vignette used in the chapter that reviewed preliminary \protect\hyperlink{MLMexplore}{exploration of longitudinal data}.

Our research vignette \citep{lefevor_religious_2017} examines the intersection of religious and sexual identities of clients in therapy. With 12,825 participants from the Center for Collegiate Mental Health 2012-2014 data set, the project is an example of working with \emph{big data.} Because the data is available to members only (and behind a paywall), I simulated the data. In the simulation, categorical variables (e.g., sexual identity, session number, religious affiliation) were rendered as continuous variables and in the simulation, I needed to transform them back into categorical ones. Inevitably, this will have introduced a great deal of error. Thus, we can expect that the results from the simulated data will be different from those obtained by the authors.

The Method section of the article provides detailed information about the inclusion criteria ofr the study and the coding of the variables. This included data about the religious and sexual identities as well as a minimum of three separate scores on the Counseling Center Assessment of Psychologial Sympsoms \citep[CCAPS,][]{locke_development_2012} measure. For the final dataset, clients attended an average of 10.58 sessions (\emph{SD} = 7.65) and had an average of 5.36 CCAPS administrations (\emph{SD} = 4.04). This means that in the original dataset, each client was represented by a varying number of observations (likely ranging from 3 {[}the minimum required for inclusion{]} and, perhaps as many as 17 {[}adding +3\emph{SD}s to the mean CCAPS administrations{]}). In simulating the data, I specified five observations for each of the 12,825 clients.

Let's take a look at the variables in the study

\begin{itemize}
\item
  \textbf{Anxiety and Depression}: The anxiety and depression ratings were taken from the CCAPS measure \citep{locke_development_2012} that assesses psychological distress across seven domains. Clients rate themselves over the past two weeks on a 5-point Likert-type scale ranging from 0 (\emph{not at all like me}) to 4 (\emph{extremely like me}). Higher scores indicate more distress. The dataset comes from multiple institutions with different procedures around assessment CCAPS there is not a 1:1 correspondence with session number and CCAPS assessment.
\item
  \textbf{Sexual Identity}: The researchers used effects coding for their analysis. We used dummy coding into heterosexual (0, 85.5\%) and LGBQQ (1, 14.5\%).
\item
  \textbf{Religious Affiliation}: Religious affiliation was coded into three categories including dominant religious (DR; Christian, Catholic), nondominant religious (NDR; Muslim, Hindu, Buddhist, Jewish), and nondominant unaffiliated (NDU; agnostic, atheist, no preference). The researchers used contrast coded with an orthogonal contrast-coding scheme with two variables (to represent the three variables). We used dummy coding with dominant religion as 0 and the combined nondominant and nondominant affiliated as 1.
\end{itemize}

\hypertarget{simulating-the-data-from-the-journal-article-2}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article-2}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200513}\NormalTok{)}
\NormalTok{n\_client }\OtherTok{=} \DecValTok{12825}
\NormalTok{n\_session }\OtherTok{=} \DecValTok{5}
\NormalTok{b0 }\OtherTok{=} \FloatTok{2.03} \CommentTok{\#intercept for anxiety}
\NormalTok{b1 }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{22} \CommentTok{\#b weight for L1 session}
\NormalTok{b2 }\OtherTok{=}\NormalTok{ .}\DecValTok{13} \CommentTok{\#b weight for L2 sexual identity}
\NormalTok{b3 }\OtherTok{=}  \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{03} \CommentTok{\#b weight for L2 Rel1 (D{-}R vs ND{-}R \& ND{-}U)}
\NormalTok{b4 }\OtherTok{=}\NormalTok{ .}\DecValTok{01} \CommentTok{\#b weight for the L2 Rel2 (ND{-}R vs ND{-}U)}
\CommentTok{\#the values used below are the +/{-} 3SD they produce continuous variables which later need to be transformed to categorical ones; admittedly this introduces a great deal of error/noise into the simulation}
\CommentTok{\#the article didn\textquotesingle{}t include a correlation matrix or M/SDs so this was a clunky process }
\NormalTok{( }\AttributeTok{Session =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\SpecialCharTok{{-}}\FloatTok{3.61}\NormalTok{, }\FloatTok{3.18}\NormalTok{)) }\CommentTok{\#calc L1 Session, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{SexualIdentity =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{6.66}\NormalTok{, }\FloatTok{6.92}\NormalTok{)) }\CommentTok{\#calc L2 Sexual Identity, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion1 =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{3.43}\NormalTok{, }\FloatTok{3.37}\NormalTok{)) }\CommentTok{\#calc L2 Religion1, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion2 =} \FunctionTok{rep}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{3.38}\NormalTok{, }\FloatTok{3.41}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Religion2, values are the +/3 3SD}
\NormalTok{mu }\OtherTok{=} \FloatTok{1.76} \CommentTok{\#intercept of empty model }
\NormalTok{sds }\OtherTok{=} \FloatTok{2.264} \CommentTok{\#this is the SD of the DV}
\NormalTok{sd }\OtherTok{=} \DecValTok{1} \CommentTok{\#this is the observation{-}level random effect variance that we set at 1}

\CommentTok{\#( church = rep(LETTERS[1:n\_church], each = n\_mbrs) ) \#this worked in the prior}
\NormalTok{( }\AttributeTok{client =} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_client], }\AttributeTok{each =}\NormalTok{ n\_session) )}
\CommentTok{\#( session = numbers[1:(n\_client*n\_session)] )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rnorm}\NormalTok{(n\_client, }\DecValTok{0}\NormalTok{, sds) )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rep}\NormalTok{(clienteff, }\AttributeTok{each =}\NormalTok{ n\_session) )}
\NormalTok{( }\AttributeTok{sessioneff =} \FunctionTok{rnorm}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\DecValTok{0}\NormalTok{, sd) )}
\NormalTok{( }\AttributeTok{Anxiety =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{Session }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{SexualIdentity }\SpecialCharTok{+}\NormalTok{ b3}\SpecialCharTok{*}\NormalTok{Religion1 }\SpecialCharTok{+}\NormalTok{ b4}\SpecialCharTok{*}\NormalTok{Religion2 }\SpecialCharTok{+}\NormalTok{ clienteff }\SpecialCharTok{+}\NormalTok{ sessioneff)}
\NormalTok{( }\AttributeTok{dat =} \FunctionTok{data.frame}\NormalTok{(client, clienteff, sessioneff, Session, SexualIdentity, Religion1, Religion2, Anxiety) )}

\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())}
\CommentTok{\#moving the ID number to the first column; requires }
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{Lefevor2017 }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, client, Session, SexualIdentity, Religion1, Religion2, Anxiety)}

\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12825}\NormalTok{), }\AttributeTok{each =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#rounded Sexual Identity into dichotomous variable}
\CommentTok{\#85\% were heterosexual, }

\FunctionTok{library}\NormalTok{(robumeta)}
\CommentTok{\#The following variables should be L2, but were simulated as if they were L1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion1,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexualIdentity,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}

\CommentTok{\#Rel2 has contrast codes for dominant religion (DR, 0), nondominant religious (NDR, 1) and nondominant unspecified (NDU, {-}1)}
\CommentTok{\#Strategy is to figure out the raw score associated with the percentile rank of  {-}1 and 0, to set the breakpoints for the coding}
\CommentTok{\#NDU coded as {-}1}
\CommentTok{\#19.2+13.5+9.6}
\CommentTok{\#NDU has bottom 42.3 percent}

\CommentTok{\#DR coded as 0, so quantile cut will be 42.3 + 52.7 = 95th}
\CommentTok{\#33.4 + 19.3}
\CommentTok{\#52.7\% of sample (according to article) was DR}
\CommentTok{\#must look up percentile ranks for 5\% and 57.5\%}

\CommentTok{\#NDR}
\CommentTok{\#2.3+1+1+.7}
\CommentTok{\#NDR has 5\% of sample}
\CommentTok{\#42.3+52.7}
\CommentTok{\#quantile(Lefevor2017$Religion2, probs = c(.423, .95))}
\CommentTok{\#effects coding the second Religion variable so that NDU = {-}1, DR = 0, NDR = 1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
                             \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087} \SpecialCharTok{\&}\NormalTok{ Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \FloatTok{0.9299491}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking work}
\CommentTok{\#Rel2L2\_table \textless{}{-} table(Lefevor2017$Rel2L2)}
\CommentTok{\#prop.table(Rel2L2\_table)}
\CommentTok{\#Lefevor2017 \%\textgreater{}\%}
\CommentTok{\#count(Rel2L2)}

\CommentTok{\#creating the first religion variable where DR is 2 and NDR and NDU are both {-}1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1L2 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{DRel0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking to make sure that 52.7\% are coded 2 (DR)}
\CommentTok{\#Rel1L2\_table \textless{}{-} table(Lefevor2017$Rel1L2)}
\CommentTok{\#prop.table(Rel1L2\_table)}

\CommentTok{\#heterosexual is {-}1}
\CommentTok{\#LGBTQIA+ is 1}
\CommentTok{\#quantile(Lefevor2017$SxID, probs = c(.85))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\SpecialCharTok{\textless{}=} \FloatTok{1.203468}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Het0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\#SexID\_table \textless{}{-} table(Lefevor2017$SexID)}
\CommentTok{\#prop.table(SexID\_table)}

\CommentTok{\#creating a variable representing the session number for each client, in the article up to 20 sessions were allowed. }
\CommentTok{\#install.packages("scales")}
\FunctionTok{library}\NormalTok{(scales)}
\CommentTok{\#Right from the beginning I centered this so that 0 would represent intake}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Session0 }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Session, }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{)))}

\CommentTok{\#creating session waves (1 thru 5) by rank ordering within each person\textquotesingle{}s variable the continuous variable Session that was created in the original simulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{Lefevor2017 }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(ClientID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Index =} \FunctionTok{rank}\NormalTok{(Session))}

\CommentTok{\#selecting the simulated variables}
\NormalTok{Lefevor2017\_sim }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, Session0, Anxiety, DRel0, Het0)}

\CommentTok{\#In the transition from long{-}to{-}wide, it seems like you can only do one L1 variable at a time}
\CommentTok{\#When there are multiple L1 and L2 vars, put all L2 vars on left of tilde}
\CommentTok{\#The wave/index function should come next; this should be finite (like integers of 1,2,3,4) with a maximum}
\CommentTok{\#Put the name of the SINGLE L1 variable in the concatonated list}
\FunctionTok{library}\NormalTok{(data.table)}
\NormalTok{LfvrWp1}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{+}\NormalTok{ DRel0 }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Index"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrWp1}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp1, }\AttributeTok{Index1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Index2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Index3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Index4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Index5 =} \StringTok{"5"}\NormalTok{)}
\NormalTok{LfvrWp2}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Anxiety"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrWp2}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp2, }\AttributeTok{Anx1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Anx2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Anx3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Anx4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Anx5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#For remaining L1 variable, do them one at a time {-}{-} key them from the person{-}level ID and the wave/index.}
\NormalTok{LfvrWp3}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(Lefevor2017\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Session0"}\NormalTok{))}
\NormalTok{LfvrWp3}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrWp3, }\AttributeTok{Sess1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Sess2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Sess3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Sess4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Sess5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#Next, join the dataframes by the person{-}level ID}
\CommentTok{\#Only two can be joined at a time}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrWp1, LfvrWp2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrWide, LfvrWp3,  }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To increase the portability of the OER, this lesson uses simulated data. Here is script for exporting/downloading the data as a .csv file to your local computer and then importing/uploading it again. I find that saving the .csv file (data) in the same place as the .rmd file(s) is essential for R to connect the two.

Because this simulation can take a few minutes, you may wish to do this, even as you work through this lesson, so that resimulations take less time and comuting resources.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Exporting and importing the wide df}
\FunctionTok{write.table}\NormalTok{(LfvrWide, }\AttributeTok{file=}\StringTok{"LefevorWide.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{LfvrWide }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"LefevorWide.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As before we need to restructure it into a long format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}}\NormalTok{ (data.table}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(}\FunctionTok{setDT}\NormalTok{(LfvrWide), }\AttributeTok{id.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{, }\StringTok{"DRel0"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{), }\AttributeTok{measure.vars =}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Anx1"}\NormalTok{, }\StringTok{"Anx2"}\NormalTok{, }\StringTok{"Anx3"}\NormalTok{, }\StringTok{"Anx4"}\NormalTok{, }\StringTok{"Anx5"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"Sess1"}\NormalTok{, }\StringTok{"Sess2"}\NormalTok{, }\StringTok{"Sess3"}\NormalTok{, }\StringTok{"Sess4"}\NormalTok{, }\StringTok{"Sess5"}\NormalTok{))))}
\CommentTok{\#This process  does not preserve the variable names, so we need to rename them}
\NormalTok{LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrLong, }\AttributeTok{Index =}\NormalTok{ variable, }\AttributeTok{Anxiety =} \StringTok{"value1"}\NormalTok{, }\AttributeTok{SesNum =} \StringTok{"value2"}\NormalTok{)))}

\CommentTok{\#rearanging variables so that IDs are together}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}}\NormalTok{ LfvrLong}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, SesNum, Anxiety, DRel0, Het0)}
\CommentTok{\#resorting data so that each person is together}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}} \FunctionTok{arrange}\NormalTok{(LfvrLong, ClientID, Index)}
\end{Highlighting}
\end{Shaded}

And here is code for writing it as an outfile and bringing it back in. This means that you can just import the file and start working from here (without taking time to resimulate the data and convert it again).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(LfvrLong, }\AttributeTok{file=}\StringTok{"LfvrLong.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{LfvrLong }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"LfvrLong.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-longitudinal-multilevel-model}{%
\section{Working the Longitudinal, Multilevel Model}\label{working-the-longitudinal-multilevel-model}}

Our preliminary exploration in the \protect\hyperlink{MLMexplore}{previous lesson} supported positing an L1 individual growth model that anxiety is \emph{linear} with number of sessions. Recall we centered session number around zero:

\[Y_{ij}=\beta_{0i}+\beta _{1i}(SesNum_{ij})+\varepsilon _{ij}\]
What are we saying? We are predicting each client's anxiety at a given time \(Y_{ij}\). This prediction includes anxiety at the first session (\(\beta_{0i}\), the intercept) and the rate of change, centered around zero (\(\beta _{1i}\)). Of course, each equation isn't perfect, and so a little residual remains: \(\varepsilon _{ij}\).

Just a little more recap:

\(\beta _{0i}\) is the individual \(i\)'s \emph{true} initial status, the value of the outcome when \(SesNum_{ij}\) is zero.

\(\beta _{1i}\) is the individual \(i\)'s \emph{true} rate of change during the period under study.

\(\varepsilon _{ij}\) represents that portion of the individual \(i\)'s outcome that is unpredicted on occasion \(j\).

We \emph{assume} that the \(\varepsilon _{ij}\)'s are independently drawn from a normal distribution with a mean of 0 and a variance of \(\sigma _{\varepsilon }^{2}\).

\hypertarget{a-moment-on-estimators}{%
\subsection{A Moment on Estimators}\label{a-moment-on-estimators}}

Although we tend to use maximum likelihood estimators (ML) in MLMs, two OLS approaches (generalized least squares {[}GLS{]} and iterative generalized least squares {[}IGLS{]}) are also used. They are described in more depth in Singer and Willett's text \citeyearpar{singer_applied_2003} and demonstrated in their chapter four.

ML estimators fall into two categories: \emph{full} (FML) and \emph{restricted}(RML) types. The essential difference is in how the likelihood function is formed. It, in turn, affects parameter estimation and the strategies used to test hypotheses. It is most proper to select the ML method prior to fitting models. All stats packages have defaults; it is important to know what they are and how to change them. \emph{Note. We are no longer in lavaan, so a decision between FIML and REML in MLM doesn't help with missingness.}

\hypertarget{full-maximum-likelihood}{%
\subsubsection{Full maximum likelihood**}\label{full-maximum-likelihood}}

\begin{itemize}
\tightlist
\item
  FML assesses the joint probability of simultaneously observing all the sample data actually obtained
\item
  The sampling likelihood is a function of all the data and hypothesized model and its assumptions. It includes all the unknown parameters, both the fixed effects (the gammas) and the variance components (the 4 sigmas).
\item
  The computer computes those estimates of these population parameters that jointly maximize this likelihood
\item
  The criticism of FML is that it ignores uncertainty about the fixed effects when estimating the variance components, treating their values as known. FML overstates the degrees of freedom left for estimating variance components and underestimates the variance components themselves, leading to biased estimates when samples are small.
\end{itemize}

\hypertarget{restricted-maximum-likelihood}{%
\subsubsection{Restricted maximum likelihood}\label{restricted-maximum-likelihood}}

\begin{itemize}
\tightlist
\item
  RML estimates of the variance components are those values that maximize the likelihood of observing the sample \emph{residuals} (not the sample data).
\item
  Like FML, it's an iterative process beginning with the fixed effects (the gammas) using OLS or GLS. Next, the predicted gammas are used to estimate a residual for each person on each occasion. A likelihood of observing this particular collection (residuals and the unknown variance components that govern their distribution) is noted. The logarithm of the restricted likelihood is maximized to yield RML estimates of variance components -- the only known parameters remaining.
\end{itemize}

\textbf{Which to use when?}
There has been controversy for decades and neither is clearly better than the other.

\begin{itemize}
\tightlist
\item
  Goodness of fit statistics from FML can be used to test hypotheses about any type of parameter (either a fixed effect or variance component)
\item
  Goodness of fit statistics from RML can only be used to test hypotheses about variance components (\emph{not} fixed effects)
\item
  Thus, when we compare models that differ in both fixed and variance components we need to use FML; when they differ only in variance components, we can use either.
\item
  In today's example we will use FML.
\end{itemize}

\hypertarget{two-unconditional-multilevel-models-for-change}{%
\subsection{Two Unconditional Multilevel Models for Change}\label{two-unconditional-multilevel-models-for-change}}

Subsequent to identifying the research questions, restructuring the dataset from wide to long, conducting the exploratory analyses, and choosing the estimation approach, we start small by fitting first an \emph{unconditional means model} and then an \emph{unconditional growth model.}

These \emph{unconditional} (no moderators) partition and quantify the outcome variation in two ways:

\begin{itemize}
\tightlist
\item
  across people without regard to time (unconditional means model)
\item
  across both time and people (unconditional growth model)
\end{itemize}

This allows us to establish

\begin{itemize}
\tightlist
\item
  whether there is systematic variation in the outcome that is worth modeling
\item
  \emph{where} variation resides (within or between people)
\item
  baselines for evaluating the successive models (long live Joreskog's \emph{model generating} approach)
\end{itemize}

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

\hypertarget{model-1-the-unconditional-means-model-aka-the-empty-model-or-intercept-only-model}{%
\subsection{\texorpdfstring{Model 1: The \emph{unconditional} means model (aka the ``empty model'' or intercept-only model)}{Model 1: The unconditional means model (aka the ``empty model'' or intercept-only model)}}\label{model-1-the-unconditional-means-model-aka-the-empty-model-or-intercept-only-model}}

Always fit the empty model first. With zero predictors at every level it is our starting point for describing and partitioning the outcome \emph{variation}.

\[Y_{ij}=\beta _{0i}+\varepsilon _{ij}\]
\[\beta _{0i}=\gamma _{00}+\zeta _{0i}\]

The unconditional means model (with no slope parameter) stipulates that at L1, the true individual change trajectory for person \emph{i} is flat, sitting at elevation \(\beta _{0i}\). Plus, the single part of the L2 model means that while these flat trajectories may differ in elevation, their average elevation across everyone in the population is \(\gamma _{00}\). There is no link to interindividual variation and predictors.

In sum: this model is not about change. It really just gives you a \emph{mean}. However, it is a necessary first step because it partitions the total variation in the outcome. Harkening back to ANOVA:

\(Y_{ij}\) is the person-specific mean
\(\beta _{0i}\) is the grand mean

The unconditional means model postulates that the observed value of \emph{Y} for individual \emph{i} on occasion \emph{j} is composed of deviations about these means. On occasion \emph{j}, \(Y_{ij}\) deviates from individual \emph{i}'s true mean (\(\beta _{0i}\)) by \(\varepsilon _{ij}\). Thus, the L1 residual is a ``within-person'' deviation that assesses the distance between \(Y_{ij}\) and\(\beta _{0i}\).

THEN, for person \emph{i}, his or her true mean (\(\beta _{0i}\)) deviates from the population average true mean \(\gamma _{00}\) by \(\zeta _{0i}\). This L2 residual is a ``between person'' deviation that assesses distance between \(\beta _{0i}\) and \(\gamma _{00}\).

Model A output (the unconditional means model) will provide us with two variance components:

\(\sigma^2\) is the within-person variance -- the pooled scatter of each person's data around his/her own mean
\(\tau_{00}\) is the between-person variance -- the pooled scatter of the person-specific means around the grand mean.

The purpose of fitting the unconditioned means model is to get these two variance components so we can estimate the amount of variation that exists at each level. We can use employ associated hypothesis tests to help determine whether there is sufficient variation at that level to continue modeling. If a variation component is\ldots{}

\begin{itemize}
\tightlist
\item
  Zero, there is no point in trying to predict outcome variation \emph{at that level}
\item
  Non-zero, there is some variation \emph{at that level} that could potentially be explained
\end{itemize}

\hypertarget{a-moment-on-lmer-syntax}{%
\subsection{\texorpdfstring{A moment on \emph{lmer()} syntax}{A moment on lmer() syntax}}\label{a-moment-on-lmer-syntax}}

\emph{lmer()} script is not so different from the \emph{lm()} function; it just has the additional random effect portion added in.

We can think of random effects as those things that are outside our control. For example, assignment to a treatment or control condition is in our control; it's a \emph{fixed effect}. However, anxiety at the first session (intercept) is going to vary from individual to individual -- a \emph{random effect}. We can model this random effect (i.e., that individuals have different anxiety intercepts). We can also create models with random slopes (i.e., anxiety growth trajectories).

To model a random intercept, we use this basic formula: DV \textasciitilde{} IV + (1 \textbar{} rand.int)

\begin{itemize}
\tightlist
\item
  DV is the dependent variable
\item
  IV represents independent variables
\item
  1 represents the coefficients (or slope) of the independent variables
\item
  rand.int is the variable acting as a random intercept (usually this is the column of participant IDs)
\end{itemize}

To model a random slope we use: DV \textasciitilde{} IV + (rand.slope \textbar{} rand.int).

In the model below (the goal of which is really to just get a mean and understand our within/between variance), we put a ``1'' in front of the ``\textbar{}'' because we are fixing slope to a single value -- it will not vary. But we are interested in knowing how our intercepts are different as a function of person (ID).

RML is the default for \emph{lme4}, we switch to FML with the statement, ``REML = FALSE''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{Mod1a }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{(}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ ClientID), LfvrLong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{M1a\_obj }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(Mod1a)}
\NormalTok{reM1a\_obj }\OtherTok{\textless{}{-}} \FunctionTok{ranef}\NormalTok{(Mod1a)}
\NormalTok{M1a\_obj}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: Anxiety ~ 1 + (1 | ClientID)
   Data: LfvrLong

      AIC       BIC    logLik  deviance  df.resid 
 243113.5  243140.7 -121553.7  243107.5     64122 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.6752 -0.6104  0.0018  0.6120  3.9085 

Random effects:
 Groups   Name        Variance Std.Dev.
 ClientID (Intercept) 5.118    2.262   
 Residual             1.444    1.202   
Number of obs: 64125, groups:  ClientID, 12825

Fixed effects:
               Estimate  Std. Error          df t value            Pr(>|t|)    
(Intercept)     2.07564     0.02053 12824.99885   101.1 <0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{tab\_model}\NormalTok{(Mod1a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Predictors

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

Random Effects

2

1.44

00 ClientID

5.12

ICC

0.78

N ClientID

12825

Observations

64125

Marginal R2 / Conditional R2

0.000 / 0.780

Deviance

243107.470

AIC

243119.404

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

With the \emph{plot\_model()} function in \emph{sjPlot}, we can plot the random effects. For this intercept-only model, we see the mean and range of the anxiety variable

With an intercept only, there is not much to plot. We can, though, look at the residuals to see about outliers, non-normality, and heteroscedasticity. They look reasonable to me.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod1a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-107-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-107-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-107-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-107-4.pdf}

\textbf{Some interpretation}

\(\gamma_{00}\), the intercept, estimates the grand mean across all occasions and individuals. The value, 2.076 (\emph{p} \textless{} 0.001) confirms that the average anxiety of the average client between 0 and 19 sessions is non-zero.

\(\sigma^{2}\), within-person variance, is 1.44 (\emph{SD} = 1.202). Find this in the lme4* output labeled RANDOM EFECTS: RESIDUAL

\(\tau_{00}\), between-person variance, is 5.12 (\emph{SD}= 2.26). In addition to the tab\_model output, this is located in the \emph{lme4} output labeled RANDOM EFFECTS: ClientID

The \emph{intraclass correlation coeffient (ICC)}, \$\rho \$, describes the proportion of variance that lies \emph{between} people. It is the essential piece of data we need from this model. Because the total variation in \emph{Y} is just the sum of the within and between-person variance components, the population intraclass correlation is:

\[\rho =\frac{\tau_{00}}{\sigma^{2}+\tau_{00}}\]

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{5.12}\SpecialCharTok{/}\NormalTok{(}\FloatTok{5.12+1.44}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7804878
\end{verbatim}

In our case this translates:\\
\[\rho =\frac{5.12}{1.44 + 5.12} = 0.78\]
This means that 78\% of the total variation in anxiety use is attributable to differences among clients. Within the context of the unconditional means model, the ICC also summarizes the size of the \emph{residual (or error) autocorrelation}. Practically speaking, this means that we estimate that for each person, the average correlation between any pair of composite residuals (e.g., between occasions 1 and 2, 2 and 3, or 1 and 3) is 0.78. This is considered to be large and far away from the zero residual autocorrelation that an OLS analysis would assume (require).

In addition to hand-calculating the ICC we spot it in the tab\_model/Viewer pane when we ask for a table from the \emph{lme4} model.

\hypertarget{model-2-the-unconditional-growth-model}{%
\subsection{Model 2: The unconditional growth model}\label{model-2-the-unconditional-growth-model}}

The second model is introduces TIME into the L1 submodel. Because a linear model made sense in our exploratory analyses, we specify a trajectory of linear change. We do not include any other substantive predictors.Because there are no other predictors, it is an \emph{unconditional} growth model.

\[Y_{ij}=\beta _{0i}+\beta _{1i}SesNum_{ij}+\varepsilon _{ij}\]
\[\beta _{0i}=\gamma _{00} + \zeta _{0i}\]
\[\beta _{1i}=\gamma _{10} + \zeta _{1j}\]

\hypertarget{another-moment-on-lmer-syntax}{%
\subsection{\texorpdfstring{Another moment on \emph{lmer()} syntax}{Another moment on lmer() syntax}}\label{another-moment-on-lmer-syntax}}

To model a random intercept, we use this basic formula: DV \textasciitilde{} IV + (1 \textbar{} rand.int), where

\begin{itemize}
\tightlist
\item
  DV is the dependent variable
\item
  IV represents independent variables
\item
  1 represents the coefficients (or slope) of the independent variables
\item
  rand.int is the variable acting as a random intercept (usually this is the column of participant IDs)
\end{itemize}

To model a random slope we use: DV \textasciitilde{} IV + (rand.slope \textbar{} rand.int).

\textbf{For comparison}, this was our prior model; we can see how the model building occurs.
Mod1a \textless- lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), LfvrLong, REML = FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{Mod2a }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ SesNum }\SpecialCharTok{+}\NormalTok{(SesNum }\SpecialCharTok{|}\NormalTok{ ClientID), LfvrLong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Mod2a )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: Anxiety ~ SesNum + (SesNum | ClientID)
   Data: LfvrLong

      AIC       BIC    logLik  deviance  df.resid 
 236029.0  236083.4 -118008.5  236017.0     64119 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8724 -0.6074 -0.0034  0.6132  3.5660 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.181154032 2.27621       
          SesNum      0.000001932 0.00139  -1.00
 Residual             1.259247013 1.12216       
Number of obs: 64125, groups:  ClientID, 12825

Fixed effects:
                 Estimate    Std. Error            df t value
(Intercept)     2.7787360     0.0221083 12887.3758540  125.69
SesNum         -0.0782356     0.0008979 52401.2308073  -87.13
                       Pr(>|t|)    
(Intercept) <0.0000000000000002 ***
SesNum      <0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
       (Intr)
SesNum -0.377
optimizer (nloptwrap) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(Mod1a, Mod2a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.78

0.02

\textless0.001

SesNum

-0.08

0.00

\textless0.001

Random Effects

2

1.44

1.26

00

5.12 ClientID

5.18 ClientID

11

~

0.00 ClientID.SesNum

01

~

-1.00 ClientID

ICC

0.78

~

N

12825 ClientID

12825 ClientID

Observations

64125

64125

Marginal R2 / Conditional R2

0.000 / 0.780

0.127 / NA

Deviance

243107.470

236017.007

AIC

243119.404

236047.140

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

A plot of predicted values illustrates the decrease in anxiety as sessions continue.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{plot\_model}\NormalTok{ (Mod2a, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{, }\AttributeTok{vars=}\StringTok{"SesNum"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$SesNum
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-111-1.pdf}

Diagnostic plots continue to check for outliers, non-normality, and heteroscedasticity. In my mind they continue to look reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod2a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-112-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-112-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-112-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-112-4.pdf}

\textbf{Interpreting Fixed Effects}
Looking at the \emph{tab\_model()} in the viewer, we can see that

At Session \#1 (which was rescaled so that the value of 0 is the first session), the average client has a non-zero anxiety level of 2.78 (\emph{p} \textless{} 0.001; \(\gamma _{00}\)). On average, anxiety decreases by 0.08 units per session. Using these values together, at Session \#3, the average client could be expected to have an anxiety level of 1.92 (2.08 + (-.08) + (-.08)).

\textbf{Interpreting Variance Components}

Focusing on what this change means to the L1 residuals:

In Mod1 we postulated that individual \(i\)'s observed score on occasion \(j\) (i.e,. \(Y_{ij}\)) deviates by \(\varepsilon _{ij}\) from his or her \emph{person-specific mean}

In Mod2, we postulate that \(Y_{ij}\) deviates by \(\varepsilon _{ij}\) from his or her \emph{true change trajectory}.

Comparing the Mod1 to Mod2 means that we also have a second part to the L2 submodel that depicts interindividual variation in the rates of change (\(\beta _{1i}\)).

The L1 residual variance, \(\sigma^{2}\) now summarizes the scatter of each person's data \emph{around their own linear change trajectory} (not around their own specific mean). Our value of 1.26 is a decrease of 0.18 from Mod1. While we don't get \emph{p} values to let us know whether/not within-person variation remains, the value is non-zero. Explaining more within-person variance would require another, \emph{substantive} time-covarying (L1) predictor. This example only has \emph{time-invariant} predictors.

The L2 residual variances (\(\sigma_{0 }^{2}\), \(\sigma_{1 }^{2}\)) now summarize between-person variability in initial status and rates of change.

The value of \(\tau _{00}\) is 5.12; this is variance remaining around the intercept (anxiety at Session \#1). Stated another way, it is the scatter of \(\beta _{_{0i}}\) around \(\hat{\gamma }_{00}\). While we would not expect that our L1 addition (SesNum) to have decreased between-person variance, it is a little surprising (but not uncommon) that it increased.

The value of \(\tau _{11}\) is 0.00; this is variance remaining around the slope (rate of growth). Stated another way, it is the scatter of \(\beta _{_{1i}}\) around \(\hat{\gamma }_{10}\). This value of 0.00 suggests that there is no remaining variance to explain in the slope.

Because the introduction of the time variable, SesNum, changes the meaning of the L2 variance components, we do not compare the \(\tau _{00}\)) values between Models 1 and 2. As we move forward (keeping SesNum in the model) we will use the Model 2 estimates as benchmarks for comparison.

Together, these 3 variance components allow us to distinguish L1 variation from two different kinds of L2 variation and determine whether interindividual differences in change are due to interindividual differences in true initial status or true rate of change.

\(\rho_{01}\) is the population correlation of the L2 residuals that quantifies the relationship between true initial status and true change. The value of -1.00 is negative and strong meaning that the less anxious the individual was at Session \#1, the slower that anxiety subsided.

\textbf{Proportion of Variance Explained}

In OLS regression, a simple way of computing a summary \(R^2\) statistic is to square the sample correlation between observed and predicted values of the outcome.

A similar approach can be used in the multilevel model for change. We need to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute a predicted outcome value for each person on each occasion of measurement; and
\item
  Square the sample correlation between observed and predicted values.
\end{enumerate}

The result is the \(pseudo-R^2\) statistic -- an assessment of the proportion of total outcome variation \emph{explained} by the multilevel model's specific combination of predictors.

In \emph{lme4} output, \(pseudo-R^2\) is labeled as ``Marginal\(R^2\)'' and is shown to be 0.127. That is, 13\% of the total variability in anxiety is associated with linear time. As we add substantive predictors to this model, we examine whether, and by how much, this \(pseudo-R^2\) statistic increases.

We can also compute \(pseudo-R^2\) statistics for the variance components.

\textbf{Residual variation} is the portion of the outcome variation \emph{unexplained} by the model's predictors. We can use these to further explain the model. When we evaluate a series of models we hope that adding predictors further explains unexplained outcome variation. Thus, residual variation should decline. The magnitude of the decline quantifies improvement in fit. If the decline is relatively large -- we've made a large difference. To assess these declines on a common scale we compute the \emph{proportional reduction in residual variance} as we add the predictors.

\emph{Unconditional models} provide a baseline for comparison: the unconditional means model provides a baseline estimate of \(\sigma ^{2}\) (within-person variance); the unconditional growth model provides baseline estimates of both \(\tau _{00}\) and \(\tau _{11}\) (between person variance around the intercept and slope .

We'll start by examining the decrease in within-person residual variance between the unconditional means model and unconditional growth model. We are comparing \(\sigma ^{2}\). We have to do this by hand, here's the formula:

\[Pseudo R_{\varepsilon }^{2} = \frac{\sigma^{2}(unconditional. means. model) - \sigma^{2}(unconditional. growth. model)}{\sigma ^{2}(unconditional. means. model)}\]

We calculate it manually:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{1.44} \SpecialCharTok{{-}} \FloatTok{1.26}\NormalTok{)}\SpecialCharTok{/}\FloatTok{1.44}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.125
\end{verbatim}

We conclude that 13\% of the within-person variation in anxiety is explained by number of sessions. The only way to further reduce this variance component is to add time-covarying predictors to the L1 submodel.

Singer and Willett \citeyearpar{singer_applied_2003} warn us about the flaws of \(pseudo-R^2\). Specifically, these stats can go wonky! In the multilevel model for change, additional predictors \emph{generally} reduce variance components and increase \(pseudo-R^2\) statistics.

However, because of explicit links among the model's several parts, there are times when the addition of predictors \emph{increases} the variance components' magnitude. This is most likely to happen when all, or most, of the outcome variation is exclusively within- or between-. Then, a predictor added at one level reduces the residual variance at that level, but potentially increases the residual at the other level. This results in a \emph{negative} \(pseudo-R^2\). In summary: be cautious in interpreting these.

\hypertarget{a-taxonomy-of-statistical-models}{%
\subsection{A Taxonomy of Statistical Models}\label{a-taxonomy-of-statistical-models}}

The Singer and Willett \citeyearpar{singer_applied_2003} version of Jorsekog's \citeyearpar{bollen_testing_1993} ``model generating'' and Hayes' \citeyearpar{hayes_introduction_2018} ``piecewise'' approach might be their taxonomical approach to model building. Specifically:

\begin{itemize}
\tightlist
\item
  ``Each model in the taxonomy extends a prior model in some sensible way; inspection and comparison of its elements tell the story of predictors' individual and joint effects. Most data analysis iterate toward a meaningful path; good analysis does not proceed in a rigidly predetermined order'' \citep[p.~105]{singer_applied_2003}.
\end{itemize}

How do you approach model specification: logic, theory, prior research\ldots supplemented by hypothesis testing and comparison of model fit.

A possible order:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Examine the effect of each predictor, individually.
\item
  Focus on predictors of primary interest (while including others whose effects you want to control).

  \begin{itemize}
  \tightlist
  \item
    You can add predictors individually, or in groups
  \item
    You can address issues of functional form with interactions and transformations
  \end{itemize}
\item
  Progression toward a ``final model'' whose interpretation will address your research questions
  *the quotations mean that no statistical model is ever final; it's merely a placeholder until a better model is found
\item
  Longitudinal modeling brings complexities

  \begin{itemize}
  \tightlist
  \item
    Multiple L2 outcomes can each be related to predictors
  \item
    Multiple kinds of effects (fixed effects and variance components)
  \end{itemize}
\item
  The simplest strategy is to initially include each L2 predictor (simultaneously) in all L2 submodels, but as demonstrated in later examples, they may be trimmed.

  \begin{itemize}
  \tightlist
  \item
    Each individual growth parameter can have its own predictors and one goal of model building is to identify which predictors are important for which L1 parameters.
  \item
    Although each L2 submodel can contain fixed and random effects, both are not necessarily required; sometimes a model with fewer random effects will provide a more parsimonious representation and clearer substantive insights
  \end{itemize}
\end{enumerate}

Singer and Willett \citeyearpar{singer_applied_2003} distinguish between two types of predictors:

\begin{itemize}
\tightlist
\item
  \textbf{Question} predictors are those whose effects are of primary, substantive, interest.
\item
  \textbf{Control} predictors are those whose effects you would like to remove.
\end{itemize}

Not surprisingly, different approaches may lead to the same ``final model.'' The process Lefevor and colleagues \citeyearpar{lefevor_religious_2017} follow echoes the Singer and Willett \citeyearpar{singer_applied_2003} approach, so that is what we will do.

As we procede:

\begin{itemize}
\tightlist
\item
  Model 3 includes sexual identity (and its cross-level interaction with session number) as predictor of both initial status and change.
\item
  Model 4 adds religious affiliation to both L2 models. Its addition also includes interaction terms with session number and sexual identity.
\item
  Model 5 trims non-significant effects.
\end{itemize}

\textbf{How much of this goes in the ms?}

Singer and Willett\citeyearpar{singer_applied_2003} indicate that we ``identify a manageable subset of models that, taken together, tells a persuasive story parsimoniously'' (p.~106). Minimally, include the (a) unconditional means model, (b) unconditional growth model, (c) ``final model.'' Intermediary models may be included if they provide important steps and/or tell interesting stories in their own right.
Tables (easily produced with the \emph{tab\_model()} function) should always be included (again, not all models tested are required) because they allow comparison of fitted models in a systematic way.

\hypertarget{model-3-the-uncontrolled-effects-of-sexual-identity}{%
\subsection{Model 3: The uncontrolled effects of sexual identity}\label{model-3-the-uncontrolled-effects-of-sexual-identity}}

\textbf{For comparison}:
Mod1a \textless- lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod2a \textless- lmer(Anxiety \textasciitilde{} SesNum +(SesNum \textbar{} ClientID), LfvrLong, REML = FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{Mod3a }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ SesNum}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{(SesNum }\SpecialCharTok{|}\NormalTok{ ClientID), LfvrLong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv, :
Model failed to converge with max|grad| = 0.0936369 (tol = 0.002, component 1)
\end{verbatim}

\begin{verbatim}
Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Mod3a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: Anxiety ~ SesNum * Het0 + (SesNum | ClientID)
   Data: LfvrLong

      AIC       BIC    logLik  deviance  df.resid 
 235991.7  236064.3 -117987.9  235975.7     64117 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8707 -0.6073 -0.0033  0.6135  3.5663 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.165293363 2.272728      
          SesNum      0.000002378 0.001542 -0.92
 Residual             1.259228931 1.122154      
Number of obs: 64125, groups:  ClientID, 12825

Fixed effects:
                Estimate   Std. Error           df t value             Pr(>|t|)
(Intercept)     2.700371     0.025994 12869.461063 103.886 < 0.0000000000000002
SesNum         -0.078559     0.001058 42697.704011 -74.284 < 0.0000000000000002
Het0            0.281411     0.049261 12882.202380   5.713         0.0000000114
SesNum:Het0     0.001143     0.002002 42673.127403   0.571                0.568
               
(Intercept) ***
SesNum      ***
Het0        ***
SesNum:Het0    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) SesNum Het0  
SesNum      -0.378              
Het0        -0.528  0.199       
SesNum:Het0  0.200 -0.528 -0.379
optimizer (nloptwrap) convergence code: 0 (OK)
Model failed to converge with max|grad| = 0.0936369 (tol = 0.002, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(Mod1a, Mod2a,Mod3a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.78

0.02

\textless0.001

2.70

0.03

\textless0.001

SesNum

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

SesNum * Het0

0.00

0.00

0.568

Random Effects

2

1.44

1.26

1.26

00

5.12 ClientID

5.18 ClientID

5.17 ClientID

11

~

0.00 ClientID.SesNum

0.00 ClientID.SesNum

01

~

-1.00 ClientID

-0.92 ClientID

ICC

0.78

~

0.80

N

12825 ClientID

12825 ClientID

12825 ClientID

Observations

64125

64125

64125

Marginal R2 / Conditional R2

0.000 / 0.780

0.127 / NA

0.030 / 0.808

Deviance

243107.470

236017.007

235975.714

AIC

243119.404

236047.140

236024.777

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Diagnostic plots continue to check for outliers, non-normality, and heteroscedasticity. In my mind they continue to look reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod3a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-117-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-117-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-117-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-117-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{plot\_model}\NormalTok{ (Mod3a, }\AttributeTok{type=}\StringTok{"int"}\NormalTok{, }\AttributeTok{terms =} \FunctionTok{c}\NormalTok{(}\StringTok{"SesNum"}\NormalTok{, }\StringTok{"Het0 [0,1]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-118-1.pdf}

\textbf{Interpreting Fixed Effects}
The estimated anxiety at Session \#1 for heterosexual individuals is 2.70 (\emph{p} \textless{} .001). If the client is LGBQQ, the average Session \#1 anxiety level is higher: 2.98 (\emph{p} \textless{} .001; 2.70 + .28).

The estimated rate of change in anxiety for the average client who is heterosexual is -.08 (\emph{p} \textless{} .001) units per session. The estimated differential in the rate of change in anxiety between clients who are heterosexual and LGBQQ is nondistinguishable (\(\beta\) = 0.00, \emph{p} = .568). This is evident in the interaction plot produced by the \emph{plot\_model()} function.

\textbf{Interpreting Variance Components}

Not surprisingly, the \(\sigma ^{2}\) value (1.26) stayed the same from Mod2a to Mod3a. This is because we did not add a within-subjects (time covarying) predictor. If it had changed, we would have conducted the proportionate reduction in variance evaluation.

\(\tau _{00}\) decreased from 5.18 (Mod2a) to 5.17 (Mod3a). We can apply the proportionate reduction in variance formula to determine the proportion of L2 intercept variance accounted for by the Het0 addition.

\[Pseudo R_{\zeta }^{2} = \frac{\tau _{00} (unconditional. growth. model) - \tau _{00}(subsequent. model)}{\tau _{00}(unconditional. growth. model)}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{5.18{-}5.17}\NormalTok{)}\SpecialCharTok{/}\FloatTok{5.18}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.001930502
\end{verbatim}

The \(\tau _{00}\) variance component decreases by less than 1\% from the unconditional growth model (Mod2a)

\(\tau _{11}\) is unchanged. Both Mod2a and Mod3a are 0.00; adding sexual identity did not change between-subjects' slopes. Similarly, if it had changed, we would have conducted the proportionate reduction in variance evaluation.

These variance components are now considered \emph{partial} or \emph{conditional} variances because they quantify the interindividual differences in change that remain unexplained by the model's predictors.

Here's what we might observe

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It appears there is residual around the intercept (\(\tau _{00}\)) remaining, so exploring the contribution of religious affiliation seems justifiable.
\item
  While Lefevor and colleagues \citeyearpar{lefevor_religious_2017} immediately trim the interaction term (leaving only the main effect of sexual identity), Singer and Willett \citeyearpar{singer_applied_2003} caution against immediately trimming effects because they fail to predict rate-of-change variance. Because it is a focal predictor, it is worth retaining it until we have investigated the full spectrum of its effects. We will try this approach.
\end{enumerate}

\hypertarget{model-4-the-effects-of-religious-affiliation}{%
\subsection{Model 4: The effects of religious affiliation}\label{model-4-the-effects-of-religious-affiliation}}

Model 4 evaluates the effects of religious affiliation on initial status and rates of change in anxiety, controlling for the effects of sexual identity on initial status and rate of change.

\textbf{For comparison}:
Mod1a \textless- lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod2a \textless- lmer(Anxiety \textasciitilde{} SesNum +(SesNum \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod3a \textless- lmer(Anxiety \textasciitilde{} SesNum*Het0 +(SesNum \textbar{} ClientID), LfvrLong, REML = FALSE)

Note that in this \emph{lmer()} specification, I have added a control statement: \emph{control = lmerControl(optimizer = ``bobyq'')}. This is because I was getting a ``failure to converge'' warning. Although it is more complicated than I will present, trying different \emph{engines} can sometimes force a convergence. There are a variety of different optimizers (e.g., bobyqa, Nelder-Mead, optimx, nlminb). There is much to learn about optimizers and so internet searching and evaluating will probably be required if/when you encounter convergence issues.

Another troubleshooting solution would be to trim the model of the non-significant effects. At this point, I will try to get the full model to run and then trim at the end.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{Mod4a }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ SesNum}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{ SesNum}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ Het0}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ (SesNum }\SpecialCharTok{|}\NormalTok{ ClientID), LfvrLong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Mod4a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: Anxiety ~ SesNum * Het0 + SesNum * DRel0 + Het0 * DRel0 + (SesNum |  
    ClientID)
   Data: LfvrLong
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid 
 235995.6  236095.3 -117986.8  235973.6     64114 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8761 -0.6073 -0.0032  0.6131  3.5787 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.165064172 2.272678      
          SesNum      0.000002011 0.001418 -1.00
 Residual             1.259186847 1.122135      
Number of obs: 64125, groups:  ClientID, 12825

Fixed effects:
                 Estimate   Std. Error           df t value
(Intercept)      2.676170     0.040708 13340.083398  65.741
SesNum          -0.077048     0.001528 52370.311106 -50.438
Het0             0.309204     0.074189 13819.649006   4.168
DRel0            0.040209     0.052031 13570.792578   0.773
SesNum:Het0      0.001122     0.002002 52428.230140   0.561
SesNum:DRel0    -0.002512     0.001832 52398.919720  -1.371
Het0:DRel0      -0.046310     0.092939 12825.615011  -0.498
                         Pr(>|t|)    
(Intercept)  < 0.0000000000000002 ***
SesNum       < 0.0000000000000002 ***
Het0                    0.0000309 ***
DRel0                       0.440    
SesNum:Het0                 0.575    
SesNum:DRel0                0.171    
Het0:DRel0                  0.618    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) SesNum Het0   DRel0  SsN:H0 SN:DR0
SesNum      -0.349                                   
Het0        -0.514  0.092                            
DRel0       -0.770  0.236  0.377                     
SesNum:Het0  0.129 -0.371 -0.252 -0.002              
SesNum:DRl0  0.252 -0.722  0.000 -0.327  0.007       
Het0:DRel0   0.384  0.001 -0.748 -0.500  0.001 -0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(Mod1a, Mod2a, Mod3a, Mod4a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{, }\StringTok{"Mod4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Mod4

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.78

0.02

\textless0.001

2.70

0.03

\textless0.001

2.68

0.04

\textless0.001

SesNum

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

0.31

0.07

\textless0.001

SesNum * Het0

0.00

0.00

0.568

0.00

0.00

0.575

DRel0

0.04

0.05

0.440

SesNum * DRel0

-0.00

0.00

0.171

Het0 * DRel0

-0.05

0.09

0.618

Random Effects

2

1.44

1.26

1.26

1.26

00

5.12 ClientID

5.18 ClientID

5.17 ClientID

5.17 ClientID

11

~

0.00 ClientID.SesNum

0.00 ClientID.SesNum

0.00 ClientID.SesNum

01

~

-1.00 ClientID

-0.92 ClientID

-1.00 ClientID

ICC

0.78

~

0.80

~

N

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

Observations

64125

64125

64125

64125

Marginal R2 / Conditional R2

0.000 / 0.780

0.127 / NA

0.030 / 0.808

0.137 / NA

Deviance

243107.470

236017.007

235975.714

235973.577

AIC

243119.404

236047.140

236024.777

236046.835

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Diagnostic plots continue to check for outliers, non-normality, and heteroscedasticity. In my mind they continue to look reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod4a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-122-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-122-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-122-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-122-4.pdf}

A plot of predicted values illustrates that anxiety continues decrease as sessions increase, even though we have added additional predictors. So far, this suggests stability in that result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{plot\_model}\NormalTok{ (Mod4a, }\AttributeTok{type=}\StringTok{"int"}\NormalTok{, }\AttributeTok{terms =} \FunctionTok{c}\NormalTok{(}\StringTok{"SesNum"}\NormalTok{, }\StringTok{"Het0 [0,1]"}\NormalTok{, }\StringTok{"DRel[0,1]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-123-1.pdf}

\begin{verbatim}
[[2]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-123-2.pdf}

\begin{verbatim}
[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-123-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plot\_model (Mod3a, type="int", terms = c("SesNum", "Het0 [0,1]")) \#this produces the same result, I included it as an example of how to add further specifications}
\CommentTok{\#sjPlot::plot\_model (Mod4a, type="pred",terms=c("SesNum", "Het0", "DRel0"))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod4a, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"SesNum"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{, }\StringTok{"DRel0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-124-1.pdf}

\textbf{Interpreting Fixed Effects}

The additions are all nonsignificant. That is:

\begin{itemize}
\tightlist
\item
  The prior predictors of SesNum and Het0 continue to have significant (and stable) effects.
\item
  We previously considered trimming the SesNum*Het0 interaction effect (but left it in).
\item
  DRel0 as an individual predictor, and also in moderating relations with SesNum and Het0 were all non-significant.
\end{itemize}

\textbf{Interpreting Variance Components}
Similarly, we see little change in the variance components. In fact, because the \(\sigma ^{2}\), \(\tau _{00}\) (intercept), and \(\tau _{11}\) (slope) remained constant, there is no point in conducting the proportionate reduction in variance tests.

\hypertarget{model-5-model-trimming}{%
\subsection{Model 5: Model trimming}\label{model-5-model-trimming}}

In this model, we trim the non-significant effects. Researchers will sometimes retain focal predictors, even if they are non-significant. Lefevor and colleagues \citep{lefevor_religious_2017} appear to do this when by retaining the non-significant sexual identity/religious affiliation interaction.

Reviewing my code will show that in trimming, I tried different combinations of trimming, ``just to make sure'' nothing became significant when something else was trimmed. Alas, nothing became significant. None-the-less, hashtagging it out allows me to retain a record of what I tried, while not walking through the entire process of taking notes about what I found.

\textbf{For comparison}
Mod1a \textless- lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod2a \textless- lmer(Anxiety \textasciitilde{} SesNum +(SesNum \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod3a \textless- lmer(Anxiety \textasciitilde{} SesNum\emph{Het0 +(SesNum \textbar{} ClientID), LfvrLong, REML = FALSE)
Mod4a \textless- lmer(Anxiety \textasciitilde{} SesNum}Het0 + SesNum\emph{DRel0 + Het0}DRel0 + (SesNum \textbar{} ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= ``bobyqa''))

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Mod5a\textless{}{-} lmer(Anxiety \textasciitilde{}  SesNum*DRel0 + Het0*DRel0 + (SesNum | ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))}
\CommentTok{\#Mod5a\textless{}{-} lmer(Anxiety \textasciitilde{} SesNum*Het0 +  Het0*DRel0 + (SesNum | ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))\#Mod5a\textless{}{-} lmer(Anxiety \textasciitilde{} SesNum*Het0 + SesNum*DRel0 + (SesNum | ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))}
\CommentTok{\#Mod5a\textless{}{-} lmer(Anxiety \textasciitilde{} SesNum + Het0*DRel0 + (SesNum | ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))}
\CommentTok{\#Mod5a \textless{}{-} lmer(Anxiety \textasciitilde{} Het0 + SesNum + DRel0 + (SesNum | ClientID), LfvrLong, REML = FALSE, control = lmerControl(optimizer= "bobyqa"))}

\NormalTok{Mod5a}\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ SesNum }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{+}\NormalTok{ (SesNum }\SpecialCharTok{|}\NormalTok{ ClientID), LfvrLong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Mod5a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's
  method [lmerModLmerTest]
Formula: Anxiety ~ SesNum + Het0 + (SesNum | ClientID)
   Data: LfvrLong
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid 
 235990.0  236053.5 -117988.0  235976.0     64118 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8692 -0.6073 -0.0039  0.6137  3.5690 

Random effects:
 Groups   Name        Variance   Std.Dev. Corr 
 ClientID (Intercept) 5.16508886 2.272683      
          SesNum      0.00000201 0.001418 -1.00
 Residual             1.25924483 1.122161      
Number of obs: 64125, groups:  ClientID, 12825

Fixed effects:
                 Estimate    Std. Error            df t value
(Intercept)     2.6974082     0.0254699 13568.2762227 105.906
SesNum         -0.0782400     0.0008979 52402.5696211 -87.136
Het0            0.2920586     0.0455930 12825.8954758   6.406
                        Pr(>|t|)    
(Intercept) < 0.0000000000000002 ***
SesNum      < 0.0000000000000002 ***
Het0              0.000000000155 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
       (Intr) SesNum
SesNum -0.327       
Het0   -0.498 -0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(Mod1a, Mod2a, Mod3a, Mod4a, Mod5a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{, }\StringTok{"Mod4"}\NormalTok{, }\StringTok{"Mod5"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Mod4

Mod5

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.78

0.02

\textless0.001

2.70

0.03

\textless0.001

2.68

0.04

\textless0.001

2.70

0.03

\textless0.001

SesNum

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

0.31

0.07

\textless0.001

0.29

0.05

\textless0.001

SesNum * Het0

0.00

0.00

0.568

0.00

0.00

0.575

DRel0

0.04

0.05

0.440

SesNum * DRel0

-0.00

0.00

0.171

Het0 * DRel0

-0.05

0.09

0.618

Random Effects

2

1.44

1.26

1.26

1.26

1.26

00

5.12 ClientID

5.18 ClientID

5.17 ClientID

5.17 ClientID

5.17 ClientID

11

~

0.00 ClientID.SesNum

0.00 ClientID.SesNum

0.00 ClientID.SesNum

0.00 ClientID.SesNum

01

~

-1.00 ClientID

-0.92 ClientID

-1.00 ClientID

-1.00 ClientID

ICC

0.78

~

0.80

~

~

N

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

Observations

64125

64125

64125

64125

64125

Marginal R2 / Conditional R2

0.000 / 0.780

0.127 / NA

0.030 / 0.808

0.137 / NA

0.137 / NA

Deviance

243107.470

236017.007

235975.714

235973.577

235976.039

AIC

243119.404

236047.140

236024.777

236046.835

236012.513

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

Diagnostic plots continue to check for outliers, non-normality, and heteroscedasticity. In my mind they continue to look reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod5a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-126-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-126-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-126-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-126-4.pdf}
Because of the significant trimming (of non-significant interaction effects), our plot resemble those we have observed throughout the model building process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (Mod5a, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"SesNum"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-127-1.pdf}

Summary interpretations:

\textbf{Fixed Effects}: Model trimming resulted in the retention of two significant predictor variables. Controlling for sexual identity, anxiety decreased .08 points (\emph{p} \textless{} 0.001) from one session to the next. In turn, controlling for sexual identity, LGBQQ clients were .29 points higher on anxiety (\emph{p} \textless{} 0.001). All interaction terms were trimmed from the model. The figures illustrate the main effects of session number and sexual identity.

\textbf{Variance Components}: The variance components did change from Mod4a to Mod5a. Given that the effects we trimmed were \emph{non-significant}, it is not surprising that it did not have a significant effect on the model. We should,though, compare our the final model's \(\sigma ^{2}\) back to the appropriate unconditional means model. This asks, ``What within-person's variance does Mod5a account for, relative to the within-person variance observed in Mod1a?'' In parallel, we compare the final model's \(\tau _{00}\) and \(\tau _{11}\) back to the unconditional growth model (Mod2a).This asks, ``What between-person's intercept and slope variance does Mod5a account for, relative wo those variances observed in Mod2a?''

Here's the general formula that we can apply across the \(\sigma ^{2}\)(within subjects {[}L1{]} variance), \(\tau _{00}\) (between-subjects {[}L2{]} intercepts), and \(\tau _{11}\) (between-subjects {[}L2{]} slopes)

\[Pseudo R^{2} = \frac{\sigma^2 (unconditional. means(or.growth)model) - \sigma^2(final. model)}{\sigma^{2}(unconditional.means(or.growth)model)}\]

First we assess the relative proportion of within-persons variance accounted for by the final model. Using the \(\sigma ^{2}\) (L1/Within-subjects variance) we compare Mod5a (final, trimmed model) compared to Mod1a (unconditioned means model). In calculating this (all of these, actually), I recognize it did not decrease after Mod2a, but I want to demonstrate it as practice.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#for within{-}subjects variance}
\NormalTok{(}\FloatTok{1.44{-}1.26}\NormalTok{)}\SpecialCharTok{/}\FloatTok{1.44}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.125
\end{verbatim}

Relative to the empty model, the final, trimmed model (Mod5a) explains 13\% of the within-subjects variance.

Next we assess the relative proportion of between-persons intercept variance accounted for by the final model. Using the \(\tau _{00}\) (L2/intercept variance) we compare Mod5a (final, trimmed model) to Mod2a (unconditional growth model).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#for bewteen subjects intercept}
\NormalTok{(}\FloatTok{5.18{-}5.17}\NormalTok{)}\SpecialCharTok{/}\FloatTok{5.18}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.001930502
\end{verbatim}

Relative to the unconditional growth model (Mod2a), the final model (Mod5a) explains less than 1\% of the variance.

Finally, we assess the relative proportion of between-persons slope variance accounted for by the final model. Using the \(\tau _{11}\) (L2/slope variance), we compare Mod5a (final, trimmed model) to Mod2a (unconditional growth model).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#for for between subjects slope}
\DecValTok{0{-}0}\SpecialCharTok{/}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] NaN
\end{verbatim}

With values of zero, the calculation returns an impossible solution. The point, though, is that there has been no claim of variance in the between-persons (L2) slopes.

\hypertarget{evaluating-the-tenability-quality-of-the-final-model}{%
\section{Evaluating the ``Tenability'' (quality) of the Final Model}\label{evaluating-the-tenability-quality-of-the-final-model}}

\begin{itemize}
\item
  \textbf{Graphs of fitted trajectories} are essential to communicating results. The dichotomous coding made it possible to plot the growth trajectories according to our focal predictors: SesNum (by session), Het0 (as 0,1 coding), and DRel0 (as 0,1 coding). O
\item
  \textbf{Centering predictors} often improves interpretation.

  \begin{itemize}
  \tightlist
  \item
    We centered SesNum, so that 0 would represent the initial value/intercept.
  \item
    Centering time-invariant predictors (L2) can also be useful. Much has been written around ``grand mean centering'' (centering around the entire sample mean) and ``group mean centering'' (centering around an individual's mean).\\
  \item
    Although this example did not include time-covarying predictors, I strongly recommend the \emph{compositional effects} \citep{enders_centering_2007} approach where the L1 time-covarying variable is group-mean centered (centered within context) at L1 and then its group aggregate is entered again at L2. I demonstrated this in the lesson on the \protect\hyperlink{wGroups}{Nested Within Groups lesson}.
  \end{itemize}
\end{itemize}

\hypertarget{the-deviance-statistics}{%
\subsection{The Deviance Statistics}\label{the-deviance-statistics}}

The output at the bottom of the tab-model are a cluster of relative fit indices.

The \textbf{deviance statistic} compares log-likelihood statistics for two models at a time: (a) the current model and (b) a saturated model (e.g., a more general model that fits the sample data perfectly). Deviance quantifies \emph{how much worse} the current model is in comparison to the best possible model. The deviance is identical to the residual sums of squares used in regression. While you cannot directly interpret any particular deviance statistic, you can compare \emph{nested} models; the smaller value ``wins.''

Two requirements to use the deviance statistic:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The dataset must be identical. If you have a difference in 1 record in the person-period dataset that is missing or any variable in either model, then the comparison is invalidated.
\item
  A reduced model must be \emph{nested} within the other. Every parameter must be in both models. The difference is the \emph{constraints}
\item
  But it's even more complicated. Remember the FML v. REML distinction/decision?

  \begin{itemize}
  \tightlist
  \item
    If you used FML, the underlying estimation made use of the \emph{full} sample data. Consequently, the FML deviance statistic describes the fit of the entire model (both fixed and random effects). Thus, you can use the deviance statistics to test hypotheses about any combination of parameters, fixed effects, or variance components.
  \item
    If you used RML, the underlying estimation \emph{restricted} itself fo the sample \emph{residuals}. Consequently, the the deviance statistic describes only the stochastic portion of the model. Thus, you can use deviance statistics to test hypotheses only about variance components.
  \end{itemize}
\end{enumerate}

We can see the deviance values in the tab-Model and observe if they increase or decrease. The \emph{anova()} function can formally conduct a Chi-square difference test to tell us if the models are statistically significantly different from each other.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devALL }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod1a, Mod2a, Mod3a, Mod4a, Mod5a) }
\NormalTok{devALL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: LfvrLong
Models:
Mod1a: Anxiety ~ 1 + (1 | ClientID)
Mod2a: Anxiety ~ SesNum + (SesNum | ClientID)
Mod5a: Anxiety ~ SesNum + Het0 + (SesNum | ClientID)
Mod3a: Anxiety ~ SesNum * Het0 + (SesNum | ClientID)
Mod4a: Anxiety ~ SesNum * Het0 + SesNum * DRel0 + Het0 * DRel0 + (SesNum | ClientID)
      npar    AIC    BIC  logLik deviance     Chisq Df            Pr(>Chisq)
Mod1a    3 243113 243141 -121554   243107                                   
Mod2a    6 236029 236083 -118009   236017 7090.4634  3 < 0.00000000000000022
Mod5a    7 235990 236054 -117988   235976   40.9677  1       0.0000000001548
Mod3a    8 235992 236064 -117988   235976    0.3253  1                0.5684
Mod4a   11 235996 236095 -117987   235974    2.1375  3                0.5444
         
Mod1a    
Mod2a ***
Mod5a ***
Mod3a    
Mod4a    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In a manner consistent with the fixed effects, we see that models 2a and 5a are improvements over the more simple models that precede them. If we wanted specific comparisons we could list pairs, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devM4M5 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(Mod4a, Mod5a) }
\NormalTok{devM4M5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Data: LfvrLong
Models:
Mod5a: Anxiety ~ SesNum + Het0 + (SesNum | ClientID)
Mod4a: Anxiety ~ SesNum * Het0 + SesNum * DRel0 + Het0 * DRel0 + (SesNum | ClientID)
      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)
Mod5a    7 235990 236054 -117988   235976                     
Mod4a   11 235996 236095 -117987   235974 2.4628  4     0.6513
\end{verbatim}

With \(\chi^2\)(4) = 2.46, \(p\) = 0.651, we see that these Mod4a and Mod5a are not statistically significantly different than each other.

Interpreting deviance stats means you are paying attention to what is different in the model. The effects that are changed are what are different. Recall that if you used RML, you can only use the deviance tests for comparing models where the \emph{only} diffs are those that occur in the random effects.

\hypertarget{comparing-nonnested-models-with-information-criteria}{%
\subsection{Comparing Nonnested Models with Information Criteria}\label{comparing-nonnested-models-with-information-criteria}}

The AIC (Akaike Information Criteria) allow the comparison of the relative \emph{goodness of fit} of models that are not nested. That is, they can involve different sets of parameters.

Like the deviance statistic, the AIC is based on the log-likelihood statistic. Instead of using the LL itself, the AIC penalizes (e.g., decreases) the LL based on the number of parameters. Why? Adding parameters (even if they have no effect) \emph{will} increase the LL statistic and decrease the deviance statistic.

\emph{As long as two models are fit to the identical same set of data}, the AICs and BICs can be compared. The model with the smaller information critera ``wins.'' There are no established criteria for determining how large the difference is for it to matter. Across our models M5a has the lowest AIC.

\hypertarget{evaluating-the-models-assumptions}{%
\subsection{Evaluating the Model's Assumptions}\label{evaluating-the-models-assumptions}}

Especially with the proliferation of so many models and comparisons, this can quickly get unwieldy. Singer and Willett \citeyearpar{singer_applied_2003} recommed that you examine the assumptions of several initial models (making sure things are looking ok), and then again in any model you explicitly write up. We briefly look at 3 sets of assumptions: functional form, normality, homoschedasticity.

\hypertarget{checking-functional-form}{%
\subsubsection{Checking functional form}\label{checking-functional-form}}

\textbf{Level 1} We checked these early in the process by examining the empirical growth plots with superimposed OLS-estimated individual change trajectories. We saw that the linear model seemed quite reasonable for many; less so for others.

\textbf{Level 2} We plotted OLS-estimated individual growth parameters against the two substantive predictors. Similarly, we observed that a linear model would be appropriate for the Het0 and DRel0 predictors.

\hypertarget{checking-normality}{%
\subsubsection{Checking normality}\label{checking-normality}}

MLMers really like visual inspection of residual distributions. For each raw residual (the one at L1 and two at L2) you can plot values against the associated normal scores. If the distribution is normal, the points form a line. Departures from linearity indicates a departure from normality. We observed these in the diagnostic plots at the testing of each new model.

\hypertarget{checking-homoscedasticity}{%
\subsubsection{Checking homoscedasticity}\label{checking-homoscedasticity}}

This is evaluated by plotting raw residuals against predictors: the L1 residuals against the L1 predictor; the L2 residuals against the L2 predictors. If the assumption holds, residual variability will be approximately equal at every predictor. Similarly, we observed this in the diagnostic plots as we evaluated each model.

\hypertarget{apa-style-writeup-2}{%
\section{APA Style Writeup}\label{apa-style-writeup-2}}

There are a variety of ways to write this up. The Lefevor et al. \citeyearpar{lefevor_religious_2017} writeup is excellent and one worth modeling. I like to collect and consult a variety of write-ups when I draft my own. Here is an example of how I might write it up.

\textbf{Method/Analytic Strategy}

Longitudinal studies produce data with a hierarchical structure in which the repeated measures (level 1 {[}L1{]}) are clustered within individuals (level 2 {[}L2{]}). Multilevel modeling (MLM) with the R package \emph{lme4} (v. 1.1-26) was appropriate to use in this analysis because it allows for the dependent nature of the repeated measures data. Thus, we simultaneously estimated within- and between-person effects. We had one L1 variable, session number, which was centered on the client's first sesion. Thus, intercepts (i.e., the initial level of a variable) were anxiety levels at the first session (centered on the client's first sesion) and slopes represented changes in the dependent variable as the session count increased.

L2 variables included sexual and religious identities. Given the large proportion of individuals who identified as heterosexual, dummy coding was used such that heterosexual was coded 0 and LGBQQ was coded 1. Similarly, those who claimed a dominant religion were 1; non-dominant religious affiliations (both named and unnamed) were coded 0. Data was modeled with a linear mixed effects approach with full information maximum likelihood (FML).

A model building approach was used to specify the model \citep{hancock_hierarchical_2010, singer_applied_2003}. Model 1 was an unconditioned growth model, primarily used to calculate the ICC. We entered the time variable, session number in Model 2. In Model 3 we entered the L2 variable sexual identity as an L2 predictor and cross-level interaction. In Model 4 we entered the L2 variable religious affiliation as an L2 predictor and in cross-level interactions with the other variables. In Model 5 we trimmed nonsignificant predictors. At each stage we tracked the proportionate variance accounted for in our within- and between-subjects variance components.

\textbf{Preliminary Analyses}

\emph{Data Preparation ad Missing Data} (a mangy draft) Our data set included 64125 observations across five sessions for 12825 clients of university counseling centers. Although our dataset was balanced (five sessions were available for all clients) we used an unstructured form of time (unevenly spaced counseling sessions counted by session number

\emph{Distributional characteristics, assumptions, etc.} (another mangy draft) Our preliminary analysis included extensive exploratory analyses recommended by Singer and Willett (2003). Not reported here (but available in \emph{whatever OSF frame or as a supplement to the journal}) this included (a) plotting empirical growth plots for a random sample (\emph{20\%}n* = 30) of the the sample, (b) creating within-person regression models for each person in the dataset and superimposing each person's fitted regression line on the plot of his/her empircal growth record, and (c) examining the fit of these regressions via the \(R^2\) statistic and an individual estimated residual variance. Further analysis explored differences in the trajectories as a function of sexual identity and religious affiliation. Specifically, we plotted raw and fitted trajectories separately comparing heterosexual and LGBQQ clients and comparing those claiming dominant and nondominant religions. Our observations indicated that it would be appropriate to proceed by specifying a linear growth trajectory.

\textbf{Primary Analyses}
We followed Singer and Willett's \citep{singer_applied_2003} recommendations by creating and reporting on taxonomy of statistical models, where each model in the taxonomy extends a prior model in some sensible way. As shown in Table 1, Model 1 was an unconditional means model where we learned that the average anxiety level at the first session (all participants across all waves) was 2.08 (\emph{p} \textless{} .001). Further, the intraclass correlation suggested that 78\% of the variance was between subjects. This supported our decision to use a multilevel approach.

Model 2 was an unconditional growth model; simply adding session number (centered so that the first session equalled 0). In Model 3 we added a focal predictor, sexual identity to predict both intercept and slope at the submodels of the L2 equation. Results suggested that LGBQQ individuals were 0.28 (\emph{P} \textless{} .008) units higher on anxiety; there was no interaction with session number. In Model 4 we added religious affiliation and allowed it to interact with the other variables. There was no additional significance. In Model 5 we trimmed all non-signifciant effects.

Results, depicted in Figure 1, suggested that, controlling for sexual identity, the anxiety level at the first sesion w as 2.70 (\emph{p} \textless{} .001) and decreased by .08 (\emph{p} \textless{} .001) units each session. Controlling for sexual identity, LGBQQ clients' anxiety was, on average 0.29 units higher (\emph{p} \textless{} .001) throughout the counseling process.

With regard to the model as a whole, 13\% of the within-subjects variance in anxiety was explained session number. Relative to the unconditional growth model almost no variance at session one and no variance in the rate of change was explained by sexual identity or religious affiliation. Further, the deviance and AIC statistics supported our decision for retaining only session number and sexual identity.

\hypertarget{residual-and-related-questions-1}{%
\section{Residual and Related Questions\ldots{}}\label{residual-and-related-questions-1}}

..that you might have; or at least I had, but if had answered them earlier it would have disrupt the flow.

\begin{itemize}
\item
  \textbf{Why are the results different in the Lefevor et al. \citeyearpar{lefevor_religious_2017} article and should we worry?} Recall that the data is simulated. My simulation was different enough (e.g., I used dummy coding, whereas they used effects coding; I chunked religious affiliation into two categories and not three) that it likely introduced more noise into the data than even the simple simulation.
\item
  \textbf{What if my data does not include a time variable (or one that is suitable for modeling)?} Repeated measures studies can be longitudinal without clocking time. If this particular study did not clock time, the researcher could ask, ``Is anxiety different as a function of sexual identity, religious affiliation, and/or its interaction?'' In this particular study, both depression and anxiety were measured. Researchers could ask, ``Does anxiety fluctuate (increase or decrease) predictably with levels of depression?'' A lesson is forthcoming that will demonstrate MLM repeated measures without time.
\end{itemize}

\hypertarget{practice-problems-2}{%
\section{Practice Problems}\label{practice-problems-2}}

This assignment grows from the \protect\hyperlink{MLMexplore}{lesson} on preliminary exploration of longitudinal data and extends it with model specification and evaluation. The suggested practice problem for this lesson is to conduct a longitudinal MLM that includes at least one L1 predictor (ideally one that clocks time), at least one L2 predictor, and a cross-level interaction. The dependent variable should be time-varying (i.e., one of the L1, repeated measures) and the measurement scale should be continuous. If the L1 predictor is time, it should be on a sensible metric as described in the lecture (e.g., should have a zero/start point and never go backwards). The L2 variable can be categorical or continuous. Specific steps are identified in the grading rubric.

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-2}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-2}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation, then rework the problem. This should provide minor changes to the data (maybe in the second or third decimal point), but the results will likely be very similar.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Model 1: unconditional means model (i.e., empty model) & 5 & \_\_\_\_\_ \\
4. Model 2: unconditional growth model (i.e., includes the time variable) & 5 & \_\_\_\_\_ \\
5. Model 3: focal predictor and cross-level interaction & 5 & \_\_\_\_\_ \\
6. Model 4 and/or 5: more predictors and/or trimming (if no more predictors) & 5 & \_\_\_\_\_ \\
7. Calculate the proportionate reduction of variance (\(pseudoR^2\)) for each progression & & \\
8. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
9. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
10. APA Style writeup & 5 & \_\_\_\_\_ \\
11. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-2-rework-the-research-vignette-but-use-the-depression-variable-as-an-outcome}{%
\subsection{Problem \#2: Rework the research vignette, but use the depression variable as an outcome}\label{problem-2-rework-the-research-vignette-but-use-the-depression-variable-as-an-outcome}}

In the Bonus Track of the prior \protect\hyperlink{MLMexplore}{lesson} I provided the code to simulate this data where depression was the dependent variable. Repeat this analysis with that data.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Model 1: unconditional means model (i.e., empty model) & 5 & \_\_\_\_\_ \\
4. Model 2: unconditional growth model (i.e., includes the time variable) & 5 & \_\_\_\_\_ \\
5. Model 3: focal predictor and cross-level interaction & 5 & \_\_\_\_\_ \\
6. Model 4 and/or 5: more predictors and/or trimming (if no more predictors) & 5 & \_\_\_\_\_ \\
7. Calculate the proportionate reduction of variance (\(pseudoR^2\)) for each progression & & \\
8. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
9. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
10. APA Style writeup & 5 & \_\_\_\_\_ \\
11. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-3-use-other-data-that-is-available-to-you-2}{%
\subsection{Problem \#3: Use other data that is available to you}\label{problem-3-use-other-data-that-is-available-to-you-2}}

Conduct a multilevel model with data to which you have access. This could include data you simulate on your own or from a published article.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7642}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1220}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1138}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Assign each variable to the L1 or L2 roles & 5 & \_\_\_\_\_ \\
2. Provide three examples of data exploration: an unfitted model, a model fitted with a linear growth trajectory, and the fitted (or unfitted) data identified by the L2 predictor & 5 & \_\_\_\_\_ \\
3. Model 1: unconditional means model (i.e., empty model) & 5 & \_\_\_\_\_ \\
4. Model 2: unconditional growth model (i.e., includes the time variable) & 5 & \_\_\_\_\_ \\
5. Model 3: focal predictor and cross-level interaction & 5 & \_\_\_\_\_ \\
6. Model 4 and/or 5: more predictors and/or trimming (if no more predictors) & 5 & \_\_\_\_\_ \\
7. Calculate the proportionate reduction of variance (\(pseudoR^2\)) for each progression & & \\
8. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
9. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
10. APA Style writeup & 5 & \_\_\_\_\_ \\
11. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{TimeLord}{%
\chapter{Calendrical Time (and Missingness) in MLMs}\label{TimeLord}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=5ea38416-c682-49f8-ba71-ad390025e36e}{Screencasted Lecture Link}

Because much longitudinal research involves date or time stamps (and multi-level analyses can be improved by using unstructured time), the primary focus of this lecture is on working with \emph{calendrical time.} We will consider what is the proper ``metric'' for clocking time, engage in ``date math,'' and then rework the Lefevor et al.\citeyearpar{lefevor_religious_2017} with this alternate metric for time. The bonus reel of the chapter asks a number of ``What if\ldots?'' questions (i.e., what if we used the Index (time structured) variable as an indicator of time; what if our dataset was unbalanced) and provides quick answers and offers possible R script solutions.

\hypertarget{navigating-this-lesson-3}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-3}}

There is about 1 hour and 20 minutes of lecture. If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_CPA}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-3}{%
\subsection{Learning Objectives}\label{learning-objectives-3}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Select proper ``metric'' of time (selecting between days, weeks, months, year).
\item
  Conduct ``date math'':

  \begin{itemize}
  \tightlist
  \item
    add or subtract time to a date
  \item
    create intervals between dates
  \end{itemize}
\item
  Compare results when different time metrics are used
\item
  Compare the process and results when designs differ on balance/unbalance (i.e., varying numbers of observations per case)

  \begin{itemize}
  \tightlist
  \item
    Generate ideas for problems that may arise from imbalance
  \end{itemize}
\end{itemize}

\hypertarget{planning-for-practice-3}{%
\subsection{Planning for Practice}\label{planning-for-practice-3}}

The suggestions for practice in this lesson are quite flexible. I encourage you to try one or more that is relevant to the type of data you collect and the challenges you have when you use it. There are three basic suggestions:

\begin{itemize}
\tightlist
\item
  Rework the problem in the lesson but change the metric by which time is clocked from weeks to something else (e.g., hours, months). Compare the results. For more of a challenge, use the simulated data where depression (not anxiety) is the outcome and work through the entire process of preparing and analyzing calendrical time with that data.
\item
  Using the data in this chapter or the depression-outcome data from the earlier lesson, randomly delete data from the long file. Compare the results of this unbalanced design to results with the complete data.
\item
  Using data to which you have access, experiment with time and balance in a manner that makes sense ot you.
\end{itemize}

\hypertarget{readings-resources-3}{%
\subsection{Readings \& Resources}\label{readings-resources-3}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Singer, J. D., \& Willett, J.B. (2003). Chapter 5: Treating time more flexibly. In, Applied longitudinal data analysis: Modeling change and event occurrence. New York, NY: Oxford University Press.
\item
  Ford, C. (2017). Working with dates and time in R using the lubridate package. University of Virginia Library: Research Data Services + Sciences. \url{https://data.library.virginia.edu/working-with-dates-and-time-in-r-using-the-lubridate-package/}
\item
  Lefevor, G. T., Janis, R. A., \& Park, S. Y. (2017). Religious and sexual identities: An intersectional, longitudinal examination of change in therapy. \emph{The Counseling Psychologist, 45}(3), 387--413. \url{https://doi-org.ezproxy.spu.edu/10.1177/0011000017702721}
\end{itemize}

\hypertarget{packages-4}{%
\subsection{Packages}\label{packages-4}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#will install the package if not already installed}
\CommentTok{\#if(!require(lme4))\{install.packages("lme4")\}}
\CommentTok{\#if(!require(sjStats))\{install.packages("sjStats")\}}
\CommentTok{\#if(!require(sjPlot))\{install.packages("sjPlot")\}}
\CommentTok{\#if(!require(formattable))\{install.packages("formattable")\}}
\CommentTok{\#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{\#if(!require(robumeta))\{install.packages("robumeta")\}}
\CommentTok{\#if(!require(scales))\{install.packages("scales")\}}
\CommentTok{\#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{\#if(!require(data.table))\{install.packages("data.table")\}}
\CommentTok{\#if(!require(ggplot2))\{install.packages("ggplot2")\}}
\CommentTok{\#if(!require(lubridate))\{install.packages("lubridate")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploring-variants-of-time-and-balance}{%
\section{Exploring Variants of Time and Balance}\label{exploring-variants-of-time-and-balance}}

Datasets present their unique challenges. In longitudinal designs where time is a focal predictor, the ``clock'' used to mark time is a critical choice. Further, there are a number of considerations when converting dates to markers of time.

Another challenge has to do with balance. A highlight of MLM is the hope that individuals can have missing observations and varying numbers of observations per case. In this chapter, we randomly delete a number of cases from the otherwise perfectly balanced dataset to see how it challenges (or doesn't) the process and the results.

Given that this chapter does not introduce a new statistic, let's get right to the example.

\hypertarget{research-vignette-3}{%
\section{Research Vignette}\label{research-vignette-3}}

Our research vignette \citep{lefevor_religious_2017} examines the intersection of religious and sexual identities of clients in therapy. With 12,825 participants from the Center for Collegiate Mental Health 2012-2014 data set, the project is an example of working with \emph{big data.} Because the data is available to members only (and behind a paywall), I simulated the data. In the simulation, categorical variables (e.g., sexual identity, session number, religious identity) were rendered as continuous variables and in the simulation, I needed to transform them back into categorical ones. Inevitably, this will have introduced a great deal of error. Thus, we can expect that the results from the simulated data will be different from those obtained by the authors.

The Method section of the article provides detailed information about the inclusion criteria ofr the study and the coding of the variables. This included data about the religious and sexual identities as well as a minimum of three separate scores on the Counseling Center Assessment of Psychologial Sympsoms \citep[CCAPS,][]{locke_development_2012} measure. For the final dataset, clients attended an average of 10.58 sessions (\emph{SD} = 7.65) and had an average of 5.36 CCAPS administrations (\emph{SD} = 4.04). This means that in the original dataset, each client was represented by a varying number of observations (likely ranging from 3 {[}the minimum required for inclusion{]} and, perhaps as many as 17 {[}adding +3\emph{SD}s to the mean CCAPS administrations{]}). In simulating the data, I specified five observations for each of the 12,825 clients.

Let's take a look at the variables in the study

\begin{itemize}
\item
  \textbf{Anxiety and Depression}: The anxiety and depression ratings were taken from the CCAPS measure \citep{locke_development_2012} that assesses psychological distress across seven domains. Clients rate themselves over the past two weeks on a 5-point Likert-type scale ranging from 0 (\emph{not at all like me}) to 4 (\emph{extremely like me}). Higher scores indicate more distress. The dataset comes from multiple institutions with different procedures around assessment CCAPS there is not a 1:1 correspondence with session number and CCAPS assessment.
\item
  \textbf{Sexual Identity}: Sexual identity was dichotomized into heterosexual (-1, 85.5\%) and LGBQQ (1, 14.5\%).
\item
  \textbf{Relious Identity}: Religious identity was coded into three categories including dominant religious (DR; Christian, Catholic), nondominant religious (NDR; Muslim, Hindu, Buddhist, Jewish), and nondominant unaffiliated (NDU; agnostic, atheist, no preference). The three categories were contrast coded with an orthogonal contrast-coding scheme with two variables. The first variable compared DR(coded as 2) to NDU and NDR (coded as -1); the second variable compared the two nondominant groups (NDU = -1, DR = 0, NDR = 1).
\item
  \textbf{Time}: Time was a variable in the study. In the article, Lefevor et al. \citeyearpar{lefevor_religious_2017}clocked time with session number. Conceptualizing session as an indicator of ``dose'', each participants' observation included a session variable representing a value ranging from the first to the twentieth session. In the prior lessons where we used this vignette, we centered the first session at 0 so that in the data simulation, session was an integer-level variable ranging from 1 to 19. In the simulation for this lesson (where the goal is to manipulate time), I have further transformed session so that it is a date (year, month, date).
\end{itemize}

\hypertarget{simulating-the-data-from-the-journal-article-3}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article-3}}

This simulation is very similar (but not identical) to the simulation used in the MLM longitudinal exploration and model building chapters. The primary difference is that when I created the SessionT variable, I did not require the values to be integers. Instead, the number of sessions could be fractional as they still ranged from 0 to 19. The session variable becomes our key in creating a variable representing \emph{calendrical time}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200513}\NormalTok{)}
\NormalTok{n\_client }\OtherTok{=} \DecValTok{12825}
\NormalTok{n\_session }\OtherTok{=} \DecValTok{5}
\NormalTok{b0 }\OtherTok{=} \FloatTok{2.03} \CommentTok{\#intercept for anxiety}
\NormalTok{b1 }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{22} \CommentTok{\#b weight for L1 session}
\NormalTok{b2 }\OtherTok{=}\NormalTok{ .}\DecValTok{13} \CommentTok{\#b weight for L2 sexual identity}
\NormalTok{b3 }\OtherTok{=}  \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{03} \CommentTok{\#b weight for L2 Rel1 (D{-}R vs ND{-}R \& ND{-}U)}
\NormalTok{b4 }\OtherTok{=}\NormalTok{ .}\DecValTok{01} \CommentTok{\#b weight for the L2 Rel2 (ND{-}R vs ND{-}U)}
\CommentTok{\#the values used below are the +/{-} 3SD they produce continuous variables which later need to be transformed to categorical ones; admittedly this introduces a great deal of error/noise into the simulation}
\CommentTok{\#the article didn\textquotesingle{}t include a correlation matrix or M/SDs so this was a clunky process }
\NormalTok{( }\AttributeTok{Session =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\SpecialCharTok{{-}}\FloatTok{3.61}\NormalTok{, }\FloatTok{3.18}\NormalTok{)) }\CommentTok{\#calc L1 Session, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{SexualIdentity =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{6.66}\NormalTok{, }\FloatTok{6.92}\NormalTok{)) }\CommentTok{\#calc L2 Sexual Identity, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion1 =} \FunctionTok{runif}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{Session, }\SpecialCharTok{{-}}\FloatTok{3.43}\NormalTok{, }\FloatTok{3.37}\NormalTok{)) }\CommentTok{\#calc L2 Religion1, values are the +/3 3SD}
\NormalTok{( }\AttributeTok{Religion2 =} \FunctionTok{rep}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n\_session, }\SpecialCharTok{{-}}\FloatTok{3.38}\NormalTok{, }\FloatTok{3.41}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n\_session)) }\CommentTok{\#calc L2 Religion2, values are the +/3 3SD}
\NormalTok{mu }\OtherTok{=} \FloatTok{1.76} \CommentTok{\#intercept of empty model }
\NormalTok{sds }\OtherTok{=} \FloatTok{2.264} \CommentTok{\#this is the SD of the DV}
\NormalTok{sd }\OtherTok{=} \DecValTok{1} \CommentTok{\#this is the observation{-}level random effect variance that we set at 1}

\CommentTok{\#( church = rep(LETTERS[1:n\_church], each = n\_mbrs) ) \#this worked in the prior}
\NormalTok{( }\AttributeTok{client =} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_client], }\AttributeTok{each =}\NormalTok{ n\_session) )}
\CommentTok{\#( session = numbers[1:(n\_client*n\_session)] )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rnorm}\NormalTok{(n\_client, }\DecValTok{0}\NormalTok{, sds) )}
\NormalTok{( }\AttributeTok{clienteff =} \FunctionTok{rep}\NormalTok{(clienteff, }\AttributeTok{each =}\NormalTok{ n\_session) )}
\NormalTok{( }\AttributeTok{sessioneff =} \FunctionTok{rnorm}\NormalTok{(n\_client}\SpecialCharTok{*}\NormalTok{n\_session, }\DecValTok{0}\NormalTok{, sd) )}
\NormalTok{( }\AttributeTok{Anxiety =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{Session }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{SexualIdentity }\SpecialCharTok{+}\NormalTok{ b3}\SpecialCharTok{*}\NormalTok{Religion1 }\SpecialCharTok{+}\NormalTok{ b4}\SpecialCharTok{*}\NormalTok{Religion2 }\SpecialCharTok{+}\NormalTok{ clienteff }\SpecialCharTok{+}\NormalTok{ sessioneff)}
\NormalTok{( }\AttributeTok{dat =} \FunctionTok{data.frame}\NormalTok{(client, clienteff, sessioneff, Session, SexualIdentity, Religion1, Religion2, Anxiety) )}

\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{row\_number}\NormalTok{())}
\CommentTok{\#moving the ID number to the first column; requires }
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{select}\NormalTok{(ID, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{Lefevor2017 }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, client, Session, SexualIdentity, Religion1, Religion2, Anxiety)}

\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12825}\NormalTok{), }\AttributeTok{each =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#rounded Sexual Identity into dichotomous variable}
\CommentTok{\#85\% were heterosexual, }

\FunctionTok{library}\NormalTok{(robumeta)}
\CommentTok{\#The following variables should be L2, but were simulated as if they were L1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion1,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{group.mean}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexualIdentity,Lefevor2017}\SpecialCharTok{$}\NormalTok{ClientID))}\CommentTok{\#aggregated at group mean}

\CommentTok{\#Rel2 has contrast codes for dominant religion (DR, 0), nondominant religious (NDR, 1) and nondominant unspecified (NDU, {-}1)}
\CommentTok{\#Strategy is to figure out the raw score associated with the percentile rank of  {-}1 and 0, to set the breakpoints for the coding}
\CommentTok{\#NDU coded as {-}1}
\CommentTok{\#19.2+13.5+9.6}
\CommentTok{\#NDU has bottom 42.3 percent}

\CommentTok{\#DR coded as 0, so quantile cut will be 42.3 + 52.7 = 95th}
\CommentTok{\#33.4 + 19.3}
\CommentTok{\#52.7\% of sample (according to article) was DR}
\CommentTok{\#must look up percentile ranks for 5\% and 57.5\%}

\CommentTok{\#NDR}
\CommentTok{\#2.3+1+1+.7}
\CommentTok{\#NDR has 5\% of sample}
\CommentTok{\#42.3+52.7}
\CommentTok{\#quantile(Lefevor2017$Religion2, probs = c(.423, .95))}
\CommentTok{\#effects coding the second Religion variable so that NDU = {-}1, DR = 0, NDR = 1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
                             \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\FloatTok{3.0877087} \SpecialCharTok{\&}\NormalTok{ Lefevor2017}\SpecialCharTok{$}\NormalTok{Religion2 }\SpecialCharTok{\textless{}=} \FloatTok{0.9299491}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking work}
\CommentTok{\#Rel2L2\_table \textless{}{-} table(Lefevor2017$Rel2L2)}
\CommentTok{\#prop.table(Rel2L2\_table)}
\CommentTok{\#Lefevor2017 \%\textgreater{}\%}
\CommentTok{\#count(Rel2L2)}

\CommentTok{\#creating the first religion variable where DR is 2 and NDR and NDU are both {-}1}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel1L2 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{DRel0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Rel2L2, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{\#checking to make sure that 52.7\% are coded 2 (DR)}
\CommentTok{\#Rel1L2\_table \textless{}{-} table(Lefevor2017$Rel1L2)}
\CommentTok{\#prop.table(Rel1L2\_table)}

\CommentTok{\#heterosexual is {-}1}
\CommentTok{\#LGBTQIA+ is 1}
\CommentTok{\#quantile(Lefevor2017$SxID, probs = c(.85))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SxID }\SpecialCharTok{\textless{}=} \FloatTok{1.203468}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{Het0 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{mapvalues}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{SexID, }\AttributeTok{from =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\#SexID\_table \textless{}{-} table(Lefevor2017$SexID)}
\CommentTok{\#prop.table(SexID\_table)}

\CommentTok{\#creating a variable representing the session number for each client, in the article up to 20 sessions were allowed. }
\CommentTok{\#install.packages("scales")}
\FunctionTok{library}\NormalTok{(scales)}
\CommentTok{\#Right from the beginning I centered this so that 0 would represent intake}
\CommentTok{\#Lefevor2017$Session0 \textless{}{-} as.integer(scales::rescale(Lefevor2017$Session, to = c(0, 19)))}
\NormalTok{Lefevor2017}\SpecialCharTok{$}\NormalTok{SessionT }\OtherTok{\textless{}{-}}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(Lefevor2017}\SpecialCharTok{$}\NormalTok{Session, }\AttributeTok{to =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{))}

\CommentTok{\#creating session waves (1 thru 5) by rank ordering within each person\textquotesingle{}s variable the continuous variable Session that was created in the original simulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{LefevorTIME }\OtherTok{\textless{}{-}}\NormalTok{ Lefevor2017}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(ClientID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Index =} \FunctionTok{rank}\NormalTok{(Session))}

\CommentTok{\#selecting the simulated variables}
\NormalTok{LefevorTIME\_sim }\OtherTok{\textless{}{-}}\NormalTok{ LefevorTIME}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, SessionT, Anxiety, DRel0, Het0)}

\CommentTok{\#rearranging variables so that IDs are together}
\NormalTok{LefevorTIME\_sim }\OtherTok{\textless{}{-}}\NormalTok{ LefevorTIME\_sim}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, SessionT, Anxiety, DRel0, Het0)}
\CommentTok{\#resorting data so that each person is together}
\NormalTok{LefevorTIME\_sim }\OtherTok{\textless{}{-}} \FunctionTok{arrange}\NormalTok{(LefevorTIME\_sim, ClientID, Index)}

\CommentTok{\#In the transition from long{-}to{-}wide, it seems like you can only do one L1 variable at a time}
\CommentTok{\#When there are multiple L1 and L2 vars, put all L2 vars on left of tilde}
\CommentTok{\#The wave/index function should come next; this should be finite (like integers of 1,2,3,4) with a maximum}
\CommentTok{\#Put the name of the SINGLE L1 variable in the concatonated list}
\FunctionTok{library}\NormalTok{(data.table)}
\NormalTok{LfvrTWp1}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(LefevorTIME\_sim, ClientID }\SpecialCharTok{+}\NormalTok{ DRel0 }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Index"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrTWp1}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrTWp1, }\AttributeTok{Index1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Index2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Index3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Index4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Index5 =} \StringTok{"5"}\NormalTok{)}
\NormalTok{LfvrTWp2}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(LefevorTIME\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"Anxiety"}\NormalTok{))}
\CommentTok{\#rename the anxiety variable}
\NormalTok{LfvrTWp2}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrTWp2, }\AttributeTok{Anx1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Anx2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Anx3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Anx4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Anx5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#For remaining L1 variable, do them one at a time {-}{-} key them from the person{-}level ID and the wave/index.}
\NormalTok{LfvrTWp3}\OtherTok{\textless{}{-}}\NormalTok{reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(LefevorTIME\_sim, ClientID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index, }\AttributeTok{value.var =} \FunctionTok{c}\NormalTok{(}\StringTok{"SessionT"}\NormalTok{))}
\NormalTok{LfvrTWp3}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(LfvrTWp3, }\AttributeTok{Sess1 =} \StringTok{"1"}\NormalTok{, }\AttributeTok{Sess2 =} \StringTok{"2"}\NormalTok{, }\AttributeTok{Sess3 =} \StringTok{"3"}\NormalTok{, }\AttributeTok{Sess4 =} \StringTok{"4"}\NormalTok{, }\AttributeTok{Sess5 =} \StringTok{"5"}\NormalTok{)}
\CommentTok{\#Next, join the dataframes by the person{-}level ID}
\CommentTok{\#Only two can be joined at a time}
\NormalTok{LfvrTWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrTWp1, LfvrTWp2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\NormalTok{LfvrTWide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(LfvrTWide, LfvrTWp3,  }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To increase the portability of the OER, this lesson uses simulated data. Here is script for exporting/downloading the long and wide forms of the data as a .csv files to your local computer and then importing/uploading it again. I find that saving the .csv file (data) in the same place as the .rmd file(s) is essential for R to connect the two.

Once you write the document to your file, you only need to run the piece of script that reads it back in.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Once you simulate the data, the "write.table" script saves it {-}{-} ideally to the folder where your .rmd file is located}
\CommentTok{\#write.table(LefevorTIME\_sim , file="LefevorTLong.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{\#If you have used the above code, you can clear your environment, then simply run this line of code to bring the data back in}
\CommentTok{\#TIMElong \textless{}{-} read.csv ("LefevorTLong.csv", head = TRUE, sep = ",")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Once you simulate the data, the "write.table" script saves it {-}{-} ideally to the folder where your .rmd file is located}
\FunctionTok{write.table}\NormalTok{(LfvrTWide, }\AttributeTok{file=}\StringTok{"LefevorTWide.csv"}\NormalTok{, }\AttributeTok{sep=}\StringTok{","}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names=}\ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#If you have used the above code, you can clear your environment, then simply run this line of code to bring the data back in}
\CommentTok{\#TIMEwide \textless{}{-} read.csv ("LefevorTWide.csv", head = TRUE, sep = ",")}
\end{Highlighting}
\end{Shaded}

\hypertarget{more-simulation-appointment-dates}{%
\section{More Simulation -- Appointment Dates}\label{more-simulation-appointment-dates}}

Lefevor et al. \citeyearpar{lefevor_religious_2017} treated sessions as a dosage marker and clocked time by the number of sessions. But what if we wanted to use calendrical time, as time? In this lesson we will get practice at working with dates and calculating time intervals (e.g., days, weeks, or months) with those dates. To demonstrate another possible way of working with time, I have decided to simulate the dates as part of this lesson.

If you simulated the data and saved it locally, you should be able to bring it into the R Environment as a file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TIMEwide }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{ (}\StringTok{"LefevorTWide.csv"}\NormalTok{, }\AttributeTok{head =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{)}
\FunctionTok{head}\NormalTok{(TIMEwide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ClientID DRel0 Het0 Index1 Index2 Index3 Index4 Index5     Anx1       Anx2
1        1     0    0      1      2      3      4      5 3.707621 3.57984960
2        2     1    0      1      2      3      4      5 3.493704 3.09390261
3        3     1    0      1      2      3      4      5 1.186357 2.21117371
4        4     1    0      1      2      3      4      5 2.942570 2.34895407
5        5     0    0      1      2      3      4      5 2.717222 4.74703344
6        6     0    0      1      2      3      4      5 2.487774 0.01596648
       Anx3      Anx4      Anx5     Sess1    Sess2     Sess3     Sess4    Sess5
1  3.874312 3.7994900  1.556770 0.1845726 7.670686  8.539096 13.273797 16.01546
2  2.022628 2.7468959  4.003687 5.5126599 6.002033  7.781499  8.367488 10.90543
3 -1.246828 0.3381523 -1.475709 1.1219976 2.529710 12.279964 12.624917 18.73150
4  5.112765 2.6254631  2.171254 2.5158497 6.317120  9.876657 13.103456 13.93061
5  3.471733 5.0021070  4.828522 5.0965424 6.040165 14.066772 15.946901 17.50296
6  1.484700 0.8271757  2.537089 6.1330161 8.275050  8.628714 13.328045 14.03743
\end{verbatim}

If we examine the head of the data, we see five variables (Sess1 to Sess5) that were resimulated to be fractional values between 0 and 19). Thinking this simulation through, we have five repeated measures from each client that correspond with when they took the CCAPS measure.Because counseling centers differed on the frequency and scheduling for the CCAPS, there is not a 1:1 correspondence with CCAPS administration and session number. I have kept this in mind in creating a ``dates'' based time metric.

First I must create the Intake date. In this particular simulation, this is simply a marker/baseline for clocking time. I am only using it to create dates for the weekly counseling sessions. In my imagination this might have been the date that the client called to schedule an appointment.

In the script below, I am creating the variable ``Intake.'' I have assigned all the clients the same intake date (May 28, 2021). I need to tell R that the format is ``year/month/date.'' The \emph{ymd()} function does that for me.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Attaching package: 'lubridate'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:data.table':

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    date, intersect, setdiff, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TIMEwide}\SpecialCharTok{$}\NormalTok{Intake }\OtherTok{\textless{}{-}} \FunctionTok{ymd}\NormalTok{(}\StringTok{"2021{-}05{-}28"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can confirm that our Intake variable is a date by checking the structure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TIMEwide}\SpecialCharTok{$}\NormalTok{Intake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 Date[1:12825], format: "2021-05-28" "2021-05-28" "2021-05-28" "2021-05-28" "2021-05-28" ...
\end{verbatim}

In the next step of the data simulation I need to create Weeks\# variables that have a date. The date must consistently increase from Week1 through Week5. I am going to make the assumption that the simulated values for the Sess\# variables (recall, they are fractional values that range from 0 to 19) are a reasonable estimate of \emph{weekly} time. Therefore, I can add it to the Intake date to create new dates for each of the five observations for each client.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{TIMEwide }\OtherTok{\textless{}{-}}\NormalTok{ TIMEwide}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{ (}\AttributeTok{Weeks1 =}\NormalTok{ Intake }\SpecialCharTok{+} \FunctionTok{dweeks}\NormalTok{(Sess1))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{ (}\AttributeTok{Weeks2 =}\NormalTok{ Intake }\SpecialCharTok{+} \FunctionTok{dweeks}\NormalTok{(Sess2))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{ (}\AttributeTok{Weeks3 =}\NormalTok{ Intake }\SpecialCharTok{+} \FunctionTok{dweeks}\NormalTok{(Sess3))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{ (}\AttributeTok{Weeks4 =}\NormalTok{ Intake }\SpecialCharTok{+} \FunctionTok{dweeks}\NormalTok{(Sess4))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{ (}\AttributeTok{Weeks5 =}\NormalTok{ Intake }\SpecialCharTok{+} \FunctionTok{dweeks}\NormalTok{(Sess5))}

\FunctionTok{head}\NormalTok{(TIMEwide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ClientID DRel0 Het0 Index1 Index2 Index3 Index4 Index5     Anx1       Anx2
1        1     0    0      1      2      3      4      5 3.707621 3.57984960
2        2     1    0      1      2      3      4      5 3.493704 3.09390261
3        3     1    0      1      2      3      4      5 1.186357 2.21117371
4        4     1    0      1      2      3      4      5 2.942570 2.34895407
5        5     0    0      1      2      3      4      5 2.717222 4.74703344
6        6     0    0      1      2      3      4      5 2.487774 0.01596648
       Anx3      Anx4      Anx5     Sess1    Sess2     Sess3     Sess4    Sess5
1  3.874312 3.7994900  1.556770 0.1845726 7.670686  8.539096 13.273797 16.01546
2  2.022628 2.7468959  4.003687 5.5126599 6.002033  7.781499  8.367488 10.90543
3 -1.246828 0.3381523 -1.475709 1.1219976 2.529710 12.279964 12.624917 18.73150
4  5.112765 2.6254631  2.171254 2.5158497 6.317120  9.876657 13.103456 13.93061
5  3.471733 5.0021070  4.828522 5.0965424 6.040165 14.066772 15.946901 17.50296
6  1.484700 0.8271757  2.537089 6.1330161 8.275050  8.628714 13.328045 14.03743
      Intake              Weeks1              Weeks2              Weeks3
1 2021-05-28 2021-05-29 07:00:29 2021-07-20 16:40:31 2021-07-26 18:34:05
2 2021-05-28 2021-07-05 14:07:36 2021-07-09 00:20:29 2021-07-21 11:17:30
3 2021-05-28 2021-06-04 20:29:44 2021-06-14 16:59:28 2021-08-21 23:02:01
4 2021-05-28 2021-06-14 14:39:45 2021-07-11 05:16:34 2021-08-05 03:16:42
5 2021-05-28 2021-07-02 16:13:08 2021-07-09 06:44:52 2021-09-03 11:13:03
6 2021-05-28 2021-07-09 22:20:48 2021-07-24 22:12:30 2021-07-27 09:37:25
               Weeks4              Weeks5
1 2021-08-28 21:59:52 2021-09-17 02:35:52
2 2021-07-25 13:44:16 2021-08-12 08:06:43
3 2021-08-24 08:59:09 2021-10-06 02:53:31
4 2021-08-27 17:22:50 2021-09-02 12:20:35
5 2021-09-16 15:04:45 2021-09-27 12:29:50
6 2021-08-29 07:06:41 2021-09-03 06:17:18
\end{verbatim}

Let's peek at the structure of one of the Weeks\# variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TIMEwide}\SpecialCharTok{$}\NormalTok{Weeks1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 POSIXct[1:12825], format: "2021-05-29 07:00:29" "2021-07-05 14:07:36" "2021-06-04 20:29:44" ...
\end{verbatim}

The POSIXct format stores a date (complete with time) and an associated timezone (i.e., the default time zone is the one your computer is set to). The metric is seconds beginning from January 1, 1970. Any dates before that are in negative numbers. This format makes it possible to do calculate with dates/times and then extract intervals.

To this point I have only been using some of the \emph{lubridate} functions to simulate the data. This is generally the point where we would receive the data -- that point where each event of interest has a time stamp.

\hypertarget{reworking-lefevor-et-al.--lefevor_religious_2017-using-calendrical-time-and-unbalanced-data}{%
\section{\texorpdfstring{Reworking Lefevor et al. \citeyearpar{lefevor_religious_2017} using Calendrical Time (and Unbalanced Data)}{Reworking Lefevor et al. {[}-@lefevor\_religious\_2017{]} using Calendrical Time (and Unbalanced Data)}}\label{reworking-lefevor-et-al.--lefevor_religious_2017-using-calendrical-time-and-unbalanced-data}}

\hypertarget{creating-time-intervals}{%
\subsection{Creating Time Intervals}\label{creating-time-intervals}}

On their own, datestamps are not useful as predictors in a regression equation. Therefore, we must manipulate or transform them to represent some ``sensible metric for time'' \citep{singer_applied_2003}; that is, we need to think about how we wish to clock time. In making this consideration, we also want to think about how regression works. Time will be a focal predictor and so we are interested in the meaningfulness and interpretability of a ``one-unit change.'' Using \emph{days} as a metric seems too small because we would not expect to detect change in 1-day increments. Months feels reasonable to me, but with a maximum of 19 weeks, we would have a maximum of just under five units of the \emph{months} metric. Because the span of time is only 19 session and sessions tend to occur weekly, let's create a metric of weeks.

First, I need to create the \emph{time intervals} from which I will extract the durations. In regression, it is important to have a meaningful zero point. We want Week1 to be that zero.

Calculating intervals of time requires using the \%--\% operator. This results in a data column that lists the two dates being compared. In our example we want to count time forward by listing the Intake variable before the \%--\% and the Appointment variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TIMEwide }\OtherTok{\textless{}{-}}\NormalTok{ TIMEwide }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TimeInterval1 =}\NormalTok{ Weeks1 }\SpecialCharTok{\%{-}{-}\%}\NormalTok{ Weeks1)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TimeInterval2 =}\NormalTok{ Weeks1 }\SpecialCharTok{\%{-}{-}\%}\NormalTok{ Weeks2)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TimeInterval3 =}\NormalTok{ Weeks1 }\SpecialCharTok{\%{-}{-}\%}\NormalTok{ Weeks3)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TimeInterval4 =}\NormalTok{ Weeks1 }\SpecialCharTok{\%{-}{-}\%}\NormalTok{ Weeks4)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TimeInterval5 =}\NormalTok{ Weeks1 }\SpecialCharTok{\%{-}{-}\%}\NormalTok{ Weeks5)}

\FunctionTok{head}\NormalTok{(TIMEwide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ClientID DRel0 Het0 Index1 Index2 Index3 Index4 Index5     Anx1       Anx2
1        1     0    0      1      2      3      4      5 3.707621 3.57984960
2        2     1    0      1      2      3      4      5 3.493704 3.09390261
3        3     1    0      1      2      3      4      5 1.186357 2.21117371
4        4     1    0      1      2      3      4      5 2.942570 2.34895407
5        5     0    0      1      2      3      4      5 2.717222 4.74703344
6        6     0    0      1      2      3      4      5 2.487774 0.01596648
       Anx3      Anx4      Anx5     Sess1    Sess2     Sess3     Sess4    Sess5
1  3.874312 3.7994900  1.556770 0.1845726 7.670686  8.539096 13.273797 16.01546
2  2.022628 2.7468959  4.003687 5.5126599 6.002033  7.781499  8.367488 10.90543
3 -1.246828 0.3381523 -1.475709 1.1219976 2.529710 12.279964 12.624917 18.73150
4  5.112765 2.6254631  2.171254 2.5158497 6.317120  9.876657 13.103456 13.93061
5  3.471733 5.0021070  4.828522 5.0965424 6.040165 14.066772 15.946901 17.50296
6  1.484700 0.8271757  2.537089 6.1330161 8.275050  8.628714 13.328045 14.03743
      Intake              Weeks1              Weeks2              Weeks3
1 2021-05-28 2021-05-29 07:00:29 2021-07-20 16:40:31 2021-07-26 18:34:05
2 2021-05-28 2021-07-05 14:07:36 2021-07-09 00:20:29 2021-07-21 11:17:30
3 2021-05-28 2021-06-04 20:29:44 2021-06-14 16:59:28 2021-08-21 23:02:01
4 2021-05-28 2021-06-14 14:39:45 2021-07-11 05:16:34 2021-08-05 03:16:42
5 2021-05-28 2021-07-02 16:13:08 2021-07-09 06:44:52 2021-09-03 11:13:03
6 2021-05-28 2021-07-09 22:20:48 2021-07-24 22:12:30 2021-07-27 09:37:25
               Weeks4              Weeks5
1 2021-08-28 21:59:52 2021-09-17 02:35:52
2 2021-07-25 13:44:16 2021-08-12 08:06:43
3 2021-08-24 08:59:09 2021-10-06 02:53:31
4 2021-08-27 17:22:50 2021-09-02 12:20:35
5 2021-09-16 15:04:45 2021-09-27 12:29:50
6 2021-08-29 07:06:41 2021-09-03 06:17:18
                                     TimeInterval1
1 2021-05-29 07:00:29 UTC--2021-05-29 07:00:29 UTC
2 2021-07-05 14:07:36 UTC--2021-07-05 14:07:36 UTC
3 2021-06-04 20:29:44 UTC--2021-06-04 20:29:44 UTC
4 2021-06-14 14:39:45 UTC--2021-06-14 14:39:45 UTC
5 2021-07-02 16:13:08 UTC--2021-07-02 16:13:08 UTC
6 2021-07-09 22:20:48 UTC--2021-07-09 22:20:48 UTC
                                     TimeInterval2
1 2021-05-29 07:00:29 UTC--2021-07-20 16:40:31 UTC
2 2021-07-05 14:07:36 UTC--2021-07-09 00:20:29 UTC
3 2021-06-04 20:29:44 UTC--2021-06-14 16:59:28 UTC
4 2021-06-14 14:39:45 UTC--2021-07-11 05:16:34 UTC
5 2021-07-02 16:13:08 UTC--2021-07-09 06:44:52 UTC
6 2021-07-09 22:20:48 UTC--2021-07-24 22:12:30 UTC
                                     TimeInterval3
1 2021-05-29 07:00:29 UTC--2021-07-26 18:34:05 UTC
2 2021-07-05 14:07:36 UTC--2021-07-21 11:17:30 UTC
3 2021-06-04 20:29:44 UTC--2021-08-21 23:02:01 UTC
4 2021-06-14 14:39:45 UTC--2021-08-05 03:16:42 UTC
5 2021-07-02 16:13:08 UTC--2021-09-03 11:13:03 UTC
6 2021-07-09 22:20:48 UTC--2021-07-27 09:37:25 UTC
                                     TimeInterval4
1 2021-05-29 07:00:29 UTC--2021-08-28 21:59:52 UTC
2 2021-07-05 14:07:36 UTC--2021-07-25 13:44:16 UTC
3 2021-06-04 20:29:44 UTC--2021-08-24 08:59:09 UTC
4 2021-06-14 14:39:45 UTC--2021-08-27 17:22:50 UTC
5 2021-07-02 16:13:08 UTC--2021-09-16 15:04:45 UTC
6 2021-07-09 22:20:48 UTC--2021-08-29 07:06:41 UTC
                                     TimeInterval5
1 2021-05-29 07:00:29 UTC--2021-09-17 02:35:52 UTC
2 2021-07-05 14:07:36 UTC--2021-08-12 08:06:43 UTC
3 2021-06-04 20:29:44 UTC--2021-10-06 02:53:31 UTC
4 2021-06-14 14:39:45 UTC--2021-09-02 12:20:35 UTC
5 2021-07-02 16:13:08 UTC--2021-09-27 12:29:50 UTC
6 2021-07-09 22:20:48 UTC--2021-09-03 06:17:18 UTC
\end{verbatim}

Don't let the result of this script scare you! In the second step, we convert the TimeInterval\# variable to weeks. The ``d'' in the ``dweeks'' function tells R that we are calculating a duration. The ``x=1'' indicates that we are counting 1 week at a time. If we wanted to create 2-week intervals, we simply write, ``x = 2''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TIMEwide }\OtherTok{\textless{}{-}}\NormalTok{ TIMEwide }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Wks1 =} \FunctionTok{as.duration}\NormalTok{(TimeInterval1)}\SpecialCharTok{/}\FunctionTok{dweeks}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Wks2 =} \FunctionTok{as.duration}\NormalTok{(TimeInterval2)}\SpecialCharTok{/}\FunctionTok{dweeks}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Wks3 =} \FunctionTok{as.duration}\NormalTok{(TimeInterval3)}\SpecialCharTok{/}\FunctionTok{dweeks}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Wks4 =} \FunctionTok{as.duration}\NormalTok{(TimeInterval4)}\SpecialCharTok{/}\FunctionTok{dweeks}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Wks5 =} \FunctionTok{as.duration}\NormalTok{(TimeInterval5)}\SpecialCharTok{/}\FunctionTok{dweeks}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{))}

\FunctionTok{head}\NormalTok{(TIMEwide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ClientID DRel0 Het0 Index1 Index2 Index3 Index4 Index5     Anx1       Anx2
1        1     0    0      1      2      3      4      5 3.707621 3.57984960
2        2     1    0      1      2      3      4      5 3.493704 3.09390261
3        3     1    0      1      2      3      4      5 1.186357 2.21117371
4        4     1    0      1      2      3      4      5 2.942570 2.34895407
5        5     0    0      1      2      3      4      5 2.717222 4.74703344
6        6     0    0      1      2      3      4      5 2.487774 0.01596648
       Anx3      Anx4      Anx5     Sess1    Sess2     Sess3     Sess4    Sess5
1  3.874312 3.7994900  1.556770 0.1845726 7.670686  8.539096 13.273797 16.01546
2  2.022628 2.7468959  4.003687 5.5126599 6.002033  7.781499  8.367488 10.90543
3 -1.246828 0.3381523 -1.475709 1.1219976 2.529710 12.279964 12.624917 18.73150
4  5.112765 2.6254631  2.171254 2.5158497 6.317120  9.876657 13.103456 13.93061
5  3.471733 5.0021070  4.828522 5.0965424 6.040165 14.066772 15.946901 17.50296
6  1.484700 0.8271757  2.537089 6.1330161 8.275050  8.628714 13.328045 14.03743
      Intake              Weeks1              Weeks2              Weeks3
1 2021-05-28 2021-05-29 07:00:29 2021-07-20 16:40:31 2021-07-26 18:34:05
2 2021-05-28 2021-07-05 14:07:36 2021-07-09 00:20:29 2021-07-21 11:17:30
3 2021-05-28 2021-06-04 20:29:44 2021-06-14 16:59:28 2021-08-21 23:02:01
4 2021-05-28 2021-06-14 14:39:45 2021-07-11 05:16:34 2021-08-05 03:16:42
5 2021-05-28 2021-07-02 16:13:08 2021-07-09 06:44:52 2021-09-03 11:13:03
6 2021-05-28 2021-07-09 22:20:48 2021-07-24 22:12:30 2021-07-27 09:37:25
               Weeks4              Weeks5
1 2021-08-28 21:59:52 2021-09-17 02:35:52
2 2021-07-25 13:44:16 2021-08-12 08:06:43
3 2021-08-24 08:59:09 2021-10-06 02:53:31
4 2021-08-27 17:22:50 2021-09-02 12:20:35
5 2021-09-16 15:04:45 2021-09-27 12:29:50
6 2021-08-29 07:06:41 2021-09-03 06:17:18
                                     TimeInterval1
1 2021-05-29 07:00:29 UTC--2021-05-29 07:00:29 UTC
2 2021-07-05 14:07:36 UTC--2021-07-05 14:07:36 UTC
3 2021-06-04 20:29:44 UTC--2021-06-04 20:29:44 UTC
4 2021-06-14 14:39:45 UTC--2021-06-14 14:39:45 UTC
5 2021-07-02 16:13:08 UTC--2021-07-02 16:13:08 UTC
6 2021-07-09 22:20:48 UTC--2021-07-09 22:20:48 UTC
                                     TimeInterval2
1 2021-05-29 07:00:29 UTC--2021-07-20 16:40:31 UTC
2 2021-07-05 14:07:36 UTC--2021-07-09 00:20:29 UTC
3 2021-06-04 20:29:44 UTC--2021-06-14 16:59:28 UTC
4 2021-06-14 14:39:45 UTC--2021-07-11 05:16:34 UTC
5 2021-07-02 16:13:08 UTC--2021-07-09 06:44:52 UTC
6 2021-07-09 22:20:48 UTC--2021-07-24 22:12:30 UTC
                                     TimeInterval3
1 2021-05-29 07:00:29 UTC--2021-07-26 18:34:05 UTC
2 2021-07-05 14:07:36 UTC--2021-07-21 11:17:30 UTC
3 2021-06-04 20:29:44 UTC--2021-08-21 23:02:01 UTC
4 2021-06-14 14:39:45 UTC--2021-08-05 03:16:42 UTC
5 2021-07-02 16:13:08 UTC--2021-09-03 11:13:03 UTC
6 2021-07-09 22:20:48 UTC--2021-07-27 09:37:25 UTC
                                     TimeInterval4
1 2021-05-29 07:00:29 UTC--2021-08-28 21:59:52 UTC
2 2021-07-05 14:07:36 UTC--2021-07-25 13:44:16 UTC
3 2021-06-04 20:29:44 UTC--2021-08-24 08:59:09 UTC
4 2021-06-14 14:39:45 UTC--2021-08-27 17:22:50 UTC
5 2021-07-02 16:13:08 UTC--2021-09-16 15:04:45 UTC
6 2021-07-09 22:20:48 UTC--2021-08-29 07:06:41 UTC
                                     TimeInterval5 Wks1      Wks2      Wks3
1 2021-05-29 07:00:29 UTC--2021-09-17 02:35:52 UTC    0 7.4861138  8.354523
2 2021-07-05 14:07:36 UTC--2021-08-12 08:06:43 UTC    0 0.4893729  2.268839
3 2021-06-04 20:29:44 UTC--2021-10-06 02:53:31 UTC    0 1.4077127 11.157966
4 2021-06-14 14:39:45 UTC--2021-09-02 12:20:35 UTC    0 3.8012706  7.360807
5 2021-07-02 16:13:08 UTC--2021-09-27 12:29:50 UTC    0 0.9436231  8.970230
6 2021-07-09 22:20:48 UTC--2021-09-03 06:17:18 UTC    0 2.1420336  2.495697
       Wks4      Wks5
1 13.089225 15.830892
2  2.854828  5.392770
3 11.502919 17.609502
4 10.587607 11.414764
5 10.850359 12.406417
6  7.195028  7.904416
\end{verbatim}

Let's check the structure of one of the Wks\# variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TIMEwide}\SpecialCharTok{$}\NormalTok{Wks1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 num [1:12825] 0 0 0 0 0 0 0 0 0 0 ...
\end{verbatim}

Let's also take a look at the descriptives

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(TIMEwide[}\FunctionTok{c}\NormalTok{(}\StringTok{"Wks1"}\NormalTok{, }\StringTok{"Wks2"}\NormalTok{, }\StringTok{"Wks3"}\NormalTok{, }\StringTok{"Wks4"}\NormalTok{, }\StringTok{"Wks5"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     vars     n  mean   sd median trimmed  mad  min   max range  skew kurtosis
Wks1    1 12825  0.00 0.00   0.00    0.00 0.00 0.00  0.00  0.00   NaN      NaN
Wks2    2 12825  3.20 2.66   2.52    2.84 2.41 0.00 16.19 16.19  1.17     1.17
Wks3    3 12825  6.36 3.40   5.98    6.16 3.62 0.07 17.95 17.89  0.48    -0.35
Wks4    4 12825  9.52 3.61   9.51    9.52 3.96 0.30 18.79 18.48  0.00    -0.66
Wks5    5 12825 12.70 3.39  13.05   12.90 3.63 0.92 18.90 17.99 -0.48    -0.38
       se
Wks1 0.00
Wks2 0.02
Wks3 0.03
Wks4 0.03
Wks5 0.03
\end{verbatim}

The Wks1 through Wks5 variables are what we will use to clock time. We can see that they progress from 0 (hypothethically, the first appointment) to 18.90. This is consistent with the the original simulation, where I restricted the Session variable values were to range between 0 and 19.

This simulation isn't perfect in that the CCAPS administrations were not always at the first session. Additionally, there may be a few cases cases where the weeks values are extroardinarily close together or even in reverse order. A \emph{time reversal} violates Singer and Willett's \citeyearpar{singer_applied_2003} rules. None-the-less, in this simulation with over 12,000 cases, this should be sufficient for demonstration. Further, it models some of the decisions made when we are working with raw data (which, at least in my lab, always has some complexities).

\hypertarget{wide-to-long}{%
\subsection{Wide to Long}\label{wide-to-long}}

Now that our date math is finished, we can restructure the data from wide to long. We have created a number of variables related to time. Regarding the time variables will only need the Index (created automatically) and the Wks\# variables. Of course we will want our L1 (Anx\#) and L2 (ClientID, DRel0, Het0) variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table)}
\NormalTok{TIMElong }\OtherTok{\textless{}{-}}\NormalTok{ (data.table}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(}\FunctionTok{setDT}\NormalTok{(TIMEwide), }\AttributeTok{id.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"ClientID"}\NormalTok{, }\StringTok{"DRel0"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{), }\AttributeTok{measure.vars =}\FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Anx1"}\NormalTok{, }\StringTok{"Anx2"}\NormalTok{, }\StringTok{"Anx3"}\NormalTok{, }\StringTok{"Anx4"}\NormalTok{, }\StringTok{"Anx5"}\NormalTok{),  }\FunctionTok{c}\NormalTok{(}\StringTok{"Wks1"}\NormalTok{, }\StringTok{"Wks2"}\NormalTok{, }\StringTok{"Wks3"}\NormalTok{, }\StringTok{"Wks4"}\NormalTok{, }\StringTok{"Wks5"}\NormalTok{) )))}
\CommentTok{\#This process  does not preserve the variable names, so we need to rename them}
\NormalTok{TIMElong}\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(TIMElong}\OtherTok{\textless{}{-}}  \FunctionTok{rename}\NormalTok{(TIMElong, }\AttributeTok{Index =}\NormalTok{ variable, }\AttributeTok{Anxiety =} \StringTok{"value1"}\NormalTok{, }\AttributeTok{Weeks =} \StringTok{"value2"}\NormalTok{))}

\CommentTok{\#rearanging variables so that IDs are together}
\NormalTok{TIMElong }\OtherTok{\textless{}{-}}\NormalTok{ TIMElong}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ClientID, Index, Weeks, Anxiety, DRel0, Het0)}
\CommentTok{\#resorting data so that each person is together}
\NormalTok{TIMElong }\OtherTok{\textless{}{-}} \FunctionTok{arrange}\NormalTok{(TIMElong, ClientID, Index)}

\FunctionTok{head}\NormalTok{(TIMElong)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   ClientID Index     Weeks  Anxiety DRel0 Het0
1:        1     1  0.000000 3.707621     0    0
2:        1     2  7.486114 3.579850     0    0
3:        1     3  8.354523 3.874312     0    0
4:        1     4 13.089225 3.799490     0    0
5:        1     5 15.830892 1.556770     0    0
6:        2     1  0.000000 3.493704     1    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TIMElong)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classes 'data.table' and 'data.frame':  64125 obs. of  6 variables:
 $ ClientID: int  1 1 1 1 1 2 2 2 2 2 ...
 $ Index   : Factor w/ 5 levels "1","2","3","4",..: 1 2 3 4 5 1 2 3 4 5 ...
 $ Weeks   : num  0 7.49 8.35 13.09 15.83 ...
 $ Anxiety : num  3.71 3.58 3.87 3.8 1.56 ...
 $ DRel0   : int  0 0 0 0 0 1 1 1 1 1 ...
 $ Het0    : int  0 0 0 0 0 0 0 0 0 0 ...
 - attr(*, ".internal.selfref")=<externalptr> 
\end{verbatim}

Examining the structure of the data assures us that the value of the Weeks and Index variables progress in a positive direction, together. We also see that the Weeks variable is \emph{numerical}. This is appropriate since we are simply counting the number of weeks.

\hypertarget{mlm-is-for-unbalanced-designs}{%
\subsection{MLM is for unbalanced designs}\label{mlm-is-for-unbalanced-designs}}

In the prior lessons I noted that a strength of MLM is that data can be unbalanced. The original simulations produced perfectly balanced sets of data with 5 observations each. To demonstrate that MLM can accommodate sets of data where the number of observations per person, vary.

Here, I update the simulation by randomly deleting 5000 rows. With this \emph{big data} circumstance, this still leaves us a number of cases! \textbf{Note: this is for demonstration only. You would never, intentionally, delete cases without a compelling reason!}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210529}\NormalTok{)}
\NormalTok{TIMElong }\OtherTok{\textless{}{-}}\NormalTok{ TIMElong[}\SpecialCharTok{{-}}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(TIMElong), }\DecValTok{5000}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

\hypertarget{abbreviated-ols-style-exploration}{%
\subsection{Abbreviated OLS Style Exploration}\label{abbreviated-ols-style-exploration}}

A more thorough preliminary investigation of the data occurs in a \protect\hyperlink{MLMexplore}{prior lesson}. In this quick demonstration, we take a look at individual growth plots from when clocked Weeks variable as nonparametric and parametric (i.e., where individual linear regressions are superimposed on the raw data).

We start by creating a random sample. Here is script allowing us to sample entire cases (i.e., the individual client with all their appointment data).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rndm30time }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(TIMElong, ClientID }\SpecialCharTok{\%in\%} \FunctionTok{sample}\NormalTok{(}\FunctionTok{unique}\NormalTok{(TIMElong}\SpecialCharTok{$}\NormalTok{ClientID), }\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In these first two plots we look at anxiety as a function of the time-unstructured, weeks, data. Compared to the graphs in the chapter where the data was balanced (each person had 5 observations), it is apparent that not everyone has the same number of observations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rndm30time, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Weeks, }\AttributeTok{y =}\NormalTok{ Anxiety)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ClientID)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-154-1.pdf}

In the lesson on \href{MLMexplore}{longitudinal exploration} we created individual regression equations and superimposed them on the raw data. This second plot does not print the individual regression equations for us, but calculates them behind the scenes and creates the individual plots. Similar to the plots with nonparametric smoothing, we notice the difference in numbers of observations per cases. The straight line, though, does seem to be an appropriate functional form.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rndm30time, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Weeks, }\AttributeTok{y =}\NormalTok{ Anxiety)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ClientID)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-155-1.pdf}
If this were our real data, we would complete the process outlined in the \href{MLMexplore}{longitudinal exploration} chapter. Because the purpose of this lesson is focused on unstructured time with unbalanced data, we will move forward with a quick recap of building the MLM. This next section is abbreviated from the more thorough process demonstrated in the \href{LongMod}{basic longitudinal model} lesson.

\hypertarget{rebuilding-the-model-unstructured-time-unbalanced-design}{%
\subsection{Rebuilding the Model (Unstructured Time, Unbalanced Design)}\label{rebuilding-the-model-unstructured-time-unbalanced-design}}

We follow the same model building approach as in the \href{LongMod}{basic longitudinal model} lesson. This involves progressing through the following models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unconditional means model
\item
  Unconditional growth model
\item
  Uncontrolled effects of sexual identity
\item
  Controlled effects of religious affiliation
\item
  Trimming nonsignificant effects
\end{enumerate}

\hypertarget{model-1-unconditional-means-model}{%
\subsubsection{Model 1: Unconditional means model}\label{model-1-unconditional-means-model}}

We start with the unconditional means model. In this model, we are seeking the grand mean. By identifying the nesting/grouping variable (ClientID) we also understand the proportions of between- and within- subjects variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{M1a }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{(}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(M1a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ 1 + (1 | ClientID)
   Data: TIMElong

      AIC       BIC    logLik  deviance  df.resid 
 226006.8  226033.8 -113000.4  226000.8     59122 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.6726 -0.6053  0.0002  0.6052  3.9062 

Random effects:
 Groups   Name        Variance Std.Dev.
 ClientID (Intercept) 5.101    2.258   
 Residual             1.446    1.202   
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.07580    0.02056     101
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{tab\_model}\NormalTok{(M1a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Predictors

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

Random Effects

2

1.45

00 ClientID

5.10

ICC

0.78

N ClientID

12825

Observations

59125

Marginal R2 / Conditional R2

0.000 / 0.779

Deviance

226000.825

AIC

226012.756

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

We learn that the mean anxiety score is 2.08 across all individuals and all times. A primary purpose of the unconditional means model is to obtain and interpret the ICC. Between-persons variance is 78\%; within-persons variance is its inverse (22\%).

Although I would normally run the diagnostic plots after each model, in this lesson I am only displaying them after the first model (as a quick check to make sure there is nothing catastrophically wrong) and after the last model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M1a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-157-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-157-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-157-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-157-4.pdf}

\hypertarget{model-2-unconditional-growth-model}{%
\subsubsection{Model 2: Unconditional growth model}\label{model-2-unconditional-growth-model}}

The second model is introduces time (this time our Weeks variable) into the L1 submodel. Because a linear model made sense in our exploratory analyses, we specify a trajectory of linear change. We do not include any other substantive predictors.Because there are no other predictors, it is an \emph{unconditional} growth model.

\textbf{For comparison}, this was our prior model; we can see how the model building occurs.
M1a \textless- lme4::lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), TIMElong, REML = FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{M2a }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Weeks }\SpecialCharTok{+}\NormalTok{(Weeks }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(M2a )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ Weeks + (Weeks | ClientID)
   Data: TIMElong

      AIC       BIC    logLik  deviance  df.resid 
 219677.4  219731.3 -109832.7  219665.4     59119 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8701 -0.6005 -0.0026  0.6072  3.5860 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.175071263 2.274878      
          Weeks       0.000001625 0.001275 -1.00
 Residual             1.260961493 1.122925      
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
              Estimate Std. Error t value
(Intercept)  2.5697482  0.0214769  119.65
Weeks       -0.0777985  0.0009444  -82.38

Correlation of Fixed Effects:
      (Intr)
Weeks -0.290
optimizer (nloptwrap) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(M1a, M2a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.57

0.02

\textless0.001

Weeks

-0.08

0.00

\textless0.001

Random Effects

2

1.45

1.26

00

5.10 ClientID

5.18 ClientID

11

~

0.00 ClientID.Weeks

01

~

-1.00 ClientID

ICC

0.78

~

N

12825 ClientID

12825 ClientID

Observations

59125

59125

Marginal R2 / Conditional R2

0.000 / 0.779

0.121 / NA

Deviance

226000.825

219665.401

AIC

226012.756

219695.425

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

A plot of predicted values illustrates the decrease in anxiety as sessions continue.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sjPlot)}
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M2a, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{, }\AttributeTok{vars=}\StringTok{"Weeks"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$Weeks
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-159-1.pdf}

\textbf{Interpreting Fixed Effects}
Looking at the \emph{tab\_model()} in the viewer, we can see that

At Week1 (the zero value), the average client was estimated to have a non-zero anxiety level of 2.82 (\emph{p} \textless{} 0.001; \(\gamma _{00}\)). On average, anxiety decreases by 0.08 units per session.

\textbf{Interpreting variance components}
With Weeks in the model, the L1 residual variance, \(\sigma^{2}\) now summarizes the scatter of each person's data \emph{around their own linear change trajectory}.

When we have both unconditional means and growth models, we can examine the decrease in within-person residual variance. Here's the formula:

\[Pseudo R_{\varepsilon }^{2} = \frac{\sigma^{2}(unconditional. means. model) - \sigma^{2}(unconditional. growth. model)}{\sigma ^{2}(unconditional. means. model)}\]

We calculate it manually:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{1.45} \SpecialCharTok{{-}} \FloatTok{1.26}\NormalTok{)}\SpecialCharTok{/}\FloatTok{1.45}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1310345
\end{verbatim}

We conclude that 13\% of the within-person variation in anxiety is explained by the number of weeks in therapy. The only way to further reduce this variance component is to add time-covarying predictors to the L1 submodel.

The value of \(\tau _{00}\) is 5.18; this is variance remaining around the intercept (anxiety at Week1). Because the introduction of the time variable, Weeks, changes the meaning of the L2 variance components, we do not compare the \(\tau _{00}\)) values between Models 1 and 2. As we move forward (keeping Weeks in the model) we will use the Model 2 estimates as benchmarks for comparison.

The value of \(\tau _{11}\) is 0.00; this is variance remaining around the slope (rate of growth).

\hypertarget{model-3-the-uncontrolled-effects-of-sexual-identity-1}{%
\subsubsection{Model 3: The uncontrolled effects of sexual identity}\label{model-3-the-uncontrolled-effects-of-sexual-identity-1}}

We will add sexual identity into the model. Because the only other predictor is Weeks, its addition (as a L2 variable, with a cross-level interaction with time), it is considered to be an \emph{uncontrolled} addition.

\textbf{For comparison}:
M1a \textless- lme4::lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), TIMElong, REML = FALSE)
M2a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks +(Weeks \textbar{} ClientID), TIMElong, REML = FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{M3a }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Weeks}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{(Weeks }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(M3a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ Weeks * Het0 + (Weeks | ClientID)
   Data: TIMElong

      AIC       BIC    logLik  deviance  df.resid 
 219642.5  219714.4 -109813.3  219626.5     59117 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8677 -0.6008 -0.0023  0.6075  3.5884 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.158483214 2.271229      
          Weeks       0.000001621 0.001273 -1.00
 Residual             1.260967448 1.122928      
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
              Estimate Std. Error t value
(Intercept)  2.4908090  0.0252470  98.658
Weeks       -0.0778911  0.0011125 -70.016
Het0         0.2834528  0.0478504   5.924
Weeks:Het0   0.0003096  0.0021045   0.147

Correlation of Fixed Effects:
           (Intr) Weeks  Het0  
Weeks      -0.290              
Het0       -0.528  0.153       
Weeks:Het0  0.153 -0.529 -0.292
optimizer (nloptwrap) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(M1a, M2a,M3a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.57

0.02

\textless0.001

2.49

0.03

\textless0.001

Weeks

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

Weeks * Het0

0.00

0.00

0.883

Random Effects

2

1.45

1.26

1.26

00

5.10 ClientID

5.18 ClientID

5.16 ClientID

11

~

0.00 ClientID.Weeks

0.00 ClientID.Weeks

01

~

-1.00 ClientID

-1.00 ClientID

ICC

0.78

~

~

N

12825 ClientID

12825 ClientID

12825 ClientID

Observations

59125

59125

59125

Marginal R2 / Conditional R2

0.000 / 0.779

0.121 / NA

0.131 / NA

Deviance

226000.825

219665.401

219626.529

AIC

226012.756

219695.425

219675.376

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M3a, }\AttributeTok{type=}\StringTok{"int"}\NormalTok{, }\AttributeTok{terms =} \FunctionTok{c}\NormalTok{(}\StringTok{"Weeks"}\NormalTok{, }\StringTok{"Het0 [0,1]"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-162-1.pdf}

\textbf{Interpreting Fixed Effects}
The estimated anxiety at Week1 for heterosexual individuals is 2.49 (\emph{p} \textless{} .001). If the client is LGBQQ, the average Week1 anxiety level is higher: 2.77 (\emph{p} \textless{} .001; 2.49 + .28).

The estimated rate of change in anxiety for the average client who is heterosexual is -.08 (\emph{p} \textless{} .001) units per week. The estimated differential in the rate of change in anxiety between clients who are heterosexual and LGBQQ is nondistinguishable (\(\beta\) = 0.00, \emph{p} = .883). This is evident in the interaction plot produced by the \emph{plot\_model()} function.

\textbf{Interpreting Variance Components}

Not surprisingly, the \(\sigma ^{2}\) value (1.26) stayed the same from M2a to M3a. This is because we did not add a within-subjects (time covarying) predictor. If it had changed, we would have conducted the proportionate reduction in variance evaluation.

\(\tau _{00}\) decreased from 5.18 (M2a) to 5.164 (M3a). We can apply the proportionate reduction in variance formula to determine the proportion of L2 intercept variance accounted for by the Het0 addition.

\[Pseudo R_{\zeta }^{2} = \frac{\tau _{00} (unconditional. growth. model) - \tau _{00}(subsequent. model)}{\tau _{00}(unconditional. growth. model)}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{5.18{-}5.16}\NormalTok{)}\SpecialCharTok{/}\FloatTok{5.18}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003861004
\end{verbatim}

The \(\tau _{00}\) variance component decreases by less than 1\% from the unconditional growth model (M2a)

\(\tau _{11}\) is unchanged. Both M2a and M3a are 0.00; adding sexual identity did not change between-subjects' slopes. Similarly, if it had changed, we would have conducted the proportionate reduction in variance evaluation.

These variance components are now considered \emph{partial} or \emph{conditional} variances because they quantify the interindividual differences in change that remain unexplained by the model's predictors.

\hypertarget{model-4-the-controlled-effects-of-religious-affiliation}{%
\subsubsection{Model 4: The controlled effects of religious affiliation}\label{model-4-the-controlled-effects-of-religious-affiliation}}

Because sexual identity is already in the model, when we add religious affiliation we are controlling for the effects of sexual identity. Religious affiliation is an L2 variable. When we add it, we will also specify a cross-level interaction with Weeks (L1).

M1a \textless- lme4::lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), TIMElong, REML = FALSE)
M2a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks +(Weeks \textbar{} ClientID), TIMElong, REML = FALSE)
M3a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks*Het0 +(Weeks \textbar{} ClientID), TIMElong, REML = FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{M4a }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Weeks}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{ Weeks}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ Het0}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ (Weeks }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(M4a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ Weeks * Het0 + Weeks * DRel0 + Het0 * DRel0 + (Weeks |  
    ClientID)
   Data: TIMElong
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid 
 219646.0  219744.9 -109812.0  219624.0     59114 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8738 -0.6005 -0.0021  0.6074  3.6019 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.158444690 2.271221      
          Weeks       0.000001633 0.001278 -1.00
 Residual             1.260911313 1.122903      
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
              Estimate Std. Error t value
(Intercept)  2.4710295  0.0397362  62.186
Weeks       -0.0761545  0.0016017 -47.547
Het0         0.3088905  0.0734210   4.207
DRel0        0.0329141  0.0509669   0.646
Weeks:Het0   0.0002992  0.0021045   0.142
Weeks:DRel0 -0.0029014  0.0019256  -1.507
Het0:DRel0  -0.0425439  0.0933013  -0.456

Correlation of Fixed Effects:
            (Intr) Weeks  Het0   DRel0  Wks:H0 Wk:DR0
Weeks       -0.267                                   
Het0        -0.521  0.069                            
DRel0       -0.772  0.180  0.392                     
Weeks:Het0   0.098 -0.369 -0.191 -0.001              
Weeks:DRel0  0.192 -0.719  0.001 -0.249  0.003       
Het0:DRel0   0.395  0.001 -0.758 -0.512  0.002 -0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(M1a, M2a, M3a, M4a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{, }\StringTok{"Mod4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Mod4

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.57

0.02

\textless0.001

2.49

0.03

\textless0.001

2.47

0.04

\textless0.001

Weeks

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

0.31

0.07

\textless0.001

Weeks * Het0

0.00

0.00

0.883

0.00

0.00

0.887

DRel0

0.03

0.05

0.518

Weeks * DRel0

-0.00

0.00

0.132

Het0 * DRel0

-0.04

0.09

0.648

Random Effects

2

1.45

1.26

1.26

1.26

00

5.10 ClientID

5.18 ClientID

5.16 ClientID

5.16 ClientID

11

~

0.00 ClientID.Weeks

0.00 ClientID.Weeks

0.00 ClientID.Weeks

01

~

-1.00 ClientID

-1.00 ClientID

-1.00 ClientID

ICC

0.78

~

~

~

N

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

Observations

59125

59125

59125

59125

Marginal R2 / Conditional R2

0.000 / 0.779

0.121 / NA

0.131 / NA

0.131 / NA

Deviance

226000.825

219665.401

219626.529

219624.047

AIC

226012.756

219695.425

219675.376

219696.975

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

We observe that none of the additional terms were statistically significant. Further, none of the variance components changed. Additionally, the output suggested convergence problems. Let's trim.

M1a \textless- lme4::lmer(Anxiety \textasciitilde1 +(1 \textbar{} ClientID), TIMElong, REML = FALSE)
M2a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks +(Weeks \textbar{} ClientID), TIMElong, REML = FALSE)
M3a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks\emph{Het0 +(Weeks \textbar{} ClientID), TIMElong, REML = FALSE)
M4a \textless- lme4::lmer(Anxiety \textasciitilde{} Weeks}Het0 + Weeks\emph{DRel0 + Het0}DRel0 + (Weeks \textbar{} ClientID), TIMElong, REML = FALSE, control = lmerControl(optimizer= ``bobyqa''))

\hypertarget{model-5-trimming-non-signifcant-effects}{%
\subsubsection{Model 5: Trimming non-signifcant effects}\label{model-5-trimming-non-signifcant-effects}}

If we strictly trim all no-significant effects, we retain Weeks and sexual identity. We do not specify any interactions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{M5a }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Weeks }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{+}\NormalTok{ (Weeks }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(M5a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ Weeks + Het0 + (Weeks | ClientID)
   Data: TIMElong
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid 
 219640.6  219703.5 -109813.3  219626.6     59118 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8672 -0.6005 -0.0024  0.6075  3.5890 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.158472090 2.271227      
          Weeks       0.000001628 0.001276 -1.00
 Residual             1.260973321 1.122931      
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
              Estimate Std. Error t value
(Intercept)  2.4902389  0.0249481  99.817
Weeks       -0.0778045  0.0009443 -82.390
Het0         0.2855055  0.0457711   6.238

Correlation of Fixed Effects:
      (Intr) Weeks 
Weeks -0.249       
Het0  -0.511 -0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(M1a, M2a, M3a, M4a, M5a, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1"}\NormalTok{, }\StringTok{"Mod2"}\NormalTok{, }\StringTok{"Mod3"}\NormalTok{, }\StringTok{"Mod4"}\NormalTok{, }\StringTok{"Mod5"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1

Mod2

Mod3

Mod4

Mod5

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.57

0.02

\textless0.001

2.49

0.03

\textless0.001

2.47

0.04

\textless0.001

2.49

0.02

\textless0.001

Weeks

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

-0.08

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

0.31

0.07

\textless0.001

0.29

0.05

\textless0.001

Weeks * Het0

0.00

0.00

0.883

0.00

0.00

0.887

DRel0

0.03

0.05

0.518

Weeks * DRel0

-0.00

0.00

0.132

Het0 * DRel0

-0.04

0.09

0.648

Random Effects

2

1.45

1.26

1.26

1.26

1.26

00

5.10 ClientID

5.18 ClientID

5.16 ClientID

5.16 ClientID

5.16 ClientID

11

~

0.00 ClientID.Weeks

0.00 ClientID.Weeks

0.00 ClientID.Weeks

0.00 ClientID.Weeks

01

~

-1.00 ClientID

-1.00 ClientID

-1.00 ClientID

-1.00 ClientID

ICC

0.78

~

~

~

~

N

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

Observations

59125

59125

59125

59125

59125

Marginal R2 / Conditional R2

0.000 / 0.779

0.121 / NA

0.131 / NA

0.131 / NA

0.131 / NA

Deviance

226000.825

219665.401

219626.529

219624.047

219626.551

AIC

226012.756

219695.425

219675.376

219696.975

219662.908

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M5a, }\AttributeTok{type=}\StringTok{"diag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-167-1.pdf}

\begin{verbatim}
[[2]]
[[2]]$ClientID
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-167-2.pdf}

\begin{verbatim}

[[3]]
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-167-3.pdf}

\begin{verbatim}
[[4]]
\end{verbatim}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-167-4.pdf}
Because of the significant trimming (of non-significant interaction effects), our plot resemble those we have observed throughout the model building process. For this final model we cannot use the \emph{type = ``int''} because it requires an interaction term and we did not have one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M5a, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"Weeks"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-168-1.pdf}

Our results are, not surprisingly, consistent with what we expected when we trimmed. That is, Anxiety for heterosexual clients at Week1 is 2.48; for LGBQQ clients it is 2.78. For all clients, anxiety decreases, on average, .08 per week.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{2.49} \SpecialCharTok{+}\NormalTok{ .}\DecValTok{29}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.78
\end{verbatim}

\hypertarget{structured-time-reworking-the-vignette-with-index}{%
\section{Structured Time: Reworking the Vignette with Index}\label{structured-time-reworking-the-vignette-with-index}}

So what would happen if we used the Index variable -- representing the five, sequenced, observations but ignoring the dosage (i.e., when marked time with Session\# ranging from 0 to 19) or calendrical time (i.e., when we marked time in Weeks {[}fractional, ranging from 0 to 19{]}).

First, let's format Index as numeric.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr) }
\NormalTok{TIMElong }\OtherTok{\textless{}{-}}\NormalTok{ TIMElong }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{Index =} \FunctionTok{as.numeric}\NormalTok{(Index)}
\NormalTok{    )}

\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(TIMElong}\SpecialCharTok{$}\NormalTok{Index))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    [1] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 4 5 2 3 1 2 3 4 5 1 2 4
   [37] 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 3 4 5
   [73] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
  [109] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1
  [145] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 3 4 1 2 3 4 5 1
  [181] 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
  [217] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 3 5 1 2 4 5 1 2 3 4 5 1 3
  [253] 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 2 3 4 5 1 2 3 4 5
  [289] 1 2 3 4 5 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
  [325] 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
  [361] 5 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
  [397] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
  [433] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1
  [469] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
  [505] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
  [541] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
  [577] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3
  [613] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
  [649] 2 3 5 1 2 3 4 1 2 3 4 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
  [685] 4 5 1 2 3 5 1 2 3 5 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
  [721] 1 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2
  [757] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
  [793] 5 1 2 3 4 5 2 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 2 1 2 3 4 5 1 2 3 4 5 1 2 3
  [829] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3
  [865] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5
  [901] 1 2 3 5 1 2 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
  [937] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
  [973] 4 5 1 2 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
 [1009] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [1045] 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1
 [1081] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [1117] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 3
 [1153] 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
 [1189] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [1225] 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [1261] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [1297] 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
 [1333] 3 4 5 1 2 3 4 2 3 4 5 1 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [1369] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 2 3 4 5 1 2 3 4 5 1 3 4
 [1405] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2
 [1441] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
 [1477] 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [1513] 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 4 5 1 2 3 5 1 2 3 4 5
 [1549] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [1585] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 2 3
 [1621] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [1657] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [1693] 2 3 4 5 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
 [1729] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [1765] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
 [1801] 2 3 4 5 3 4 5 1 2 3 5 1 2 3 4 5 1 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
 [1837] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 5 1 2 3 4 5 2 3 4 1 2 3 4 5
 [1873] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5
 [1909] 1 2 3 4 5 1 2 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
 [1945] 1 2 3 4 5 2 4 5 1 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
 [1981] 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
 [2017] 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [2053] 2 3 4 5 1 2 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
 [2089] 2 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [2125] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 4
 [2161] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [2197] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
 [2233] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
 [2269] 1 2 3 4 5 1 3 4 1 2 4 5 2 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4
 [2305] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
 [2341] 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
 [2377] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
 [2413] 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
 [2449] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3
 [2485] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1
 [2521] 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [2557] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4
 [2593] 5 1 2 3 4 5 1 2 3 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
 [2629] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2
 [2665] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
 [2701] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 3 4 5 2 3 1 2 3 4 1 2 3 4
 [2737] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3
 [2773] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [2809] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [2845] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [2881] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
 [2917] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
 [2953] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3
 [2989] 4 5 1 2 3 4 5 1 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 4 5 1
 [3025] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [3061] 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
 [3097] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [3133] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [3169] 1 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
 [3205] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [3241] 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [3277] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [3313] 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 1 2 4
 [3349] 1 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
 [3385] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 5 1 3 4 5 1 2
 [3421] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1
 [3457] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [3493] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 3 4 1
 [3529] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [3565] 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2
 [3601] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2
 [3637] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1
 [3673] 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2
 [3709] 3 4 5 1 2 3 4 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
 [3745] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [3781] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [3817] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5
 [3853] 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [3889] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [3925] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
 [3961] 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
 [3997] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [4033] 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [4069] 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 3 4 1 2 3 4 5 1 2 3
 [4105] 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
 [4141] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 3 4 5 2 3 4 5 1 2 3
 [4177] 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4
 [4213] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 4 5
 [4249] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4
 [4285] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [4321] 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 4 5 1 2 3
 [4357] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
 [4393] 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1
 [4429] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [4465] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
 [4501] 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
 [4537] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4
 [4573] 5 1 2 3 4 5 1 2 3 4 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 1 2 3 4 5 1
 [4609] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [4645] 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [4681] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [4717] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [4753] 3 4 1 2 3 4 1 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 5 1
 [4789] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [4825] 4 5 2 4 5 2 3 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3
 [4861] 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [4897] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [4933] 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 4 5 1
 [4969] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [5005] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
 [5041] 2 3 4 5 1 2 3 4 5 1 2 3 2 3 4 5 1 2 3 4 5 1 4 5 2 3 4 5 1 4 5 1 2 3 4 5
 [5077] 2 3 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
 [5113] 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 4 5 1 2 3 4 5 2 3 4 1 2 3 4
 [5149] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [5185] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [5221] 5 1 2 3 4 5 3 4 5 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
 [5257] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [5293] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [5329] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [5365] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
 [5401] 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [5437] 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
 [5473] 1 2 3 4 5 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
 [5509] 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [5545] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4
 [5581] 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [5617] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
 [5653] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 2 3 4 5 1
 [5689] 2 3 4 1 2 3 5 1 2 3 4 5 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
 [5725] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1
 [5761] 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
 [5797] 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
 [5833] 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [5869] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4
 [5905] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 2 3 4
 [5941] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2
 [5977] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [6013] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
 [6049] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [6085] 1 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [6121] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [6157] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
 [6193] 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [6229] 3 5 1 2 3 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4
 [6265] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3
 [6301] 4 5 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1
 [6337] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 2 3 4 5
 [6373] 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
 [6409] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [6445] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
 [6481] 4 1 2 4 5 1 3 4 5 1 3 4 5 1 3 4 2 3 4 5 1 2 3 4 5 1 2 5 2 4 5 1 2 3 4 5
 [6517] 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
 [6553] 4 5 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
 [6589] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2
 [6625] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
 [6661] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
 [6697] 1 2 3 4 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [6733] 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1
 [6769] 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
 [6805] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
 [6841] 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [6877] 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 3
 [6913] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1
 [6949] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
 [6985] 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1
 [7021] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5
 [7057] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [7093] 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [7129] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5
 [7165] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [7201] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2
 [7237] 3 5 1 2 3 4 1 2 3 4 5 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5
 [7273] 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
 [7309] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3
 [7345] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
 [7381] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
 [7417] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2
 [7453] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [7489] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
 [7525] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
 [7561] 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [7597] 5 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [7633] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
 [7669] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [7705] 5 1 2 3 4 5 1 2 3 4 5 1 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5
 [7741] 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
 [7777] 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [7813] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
 [7849] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [7885] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [7921] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2
 [7957] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 5 1 2
 [7993] 3 4 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [8029] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [8065] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [8101] 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
 [8137] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5
 [8173] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [8209] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [8245] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2
 [8281] 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
 [8317] 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
 [8353] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 2 3 5 1 2 3 4 5 1 2 4 5 1 3 4 5
 [8389] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [8425] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [8461] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
 [8497] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
 [8533] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [8569] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
 [8605] 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [8641] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3
 [8677] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [8713] 3 4 5 2 3 5 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
 [8749] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4
 [8785] 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [8821] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [8857] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
 [8893] 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 4
 [8929] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
 [8965] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2
 [9001] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [9037] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
 [9073] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [9109] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2
 [9145] 3 4 5 1 2 3 4 1 2 3 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
 [9181] 4 5 1 2 3 4 2 4 5 1 2 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [9217] 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
 [9253] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5
 [9289] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 3 4 5
 [9325] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [9361] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
 [9397] 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2
 [9433] 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 5 1
 [9469] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
 [9505] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 2 3 4
 [9541] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
 [9577] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
 [9613] 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 3 5 1 2 4 1 2 3 4 5 1 2 3 4 1 2
 [9649] 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
 [9685] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [9721] 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 2 3 4 5
 [9757] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [9793] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
 [9829] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2
 [9865] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1
 [9901] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5
 [9937] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
 [9973] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[10009] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4
[10045] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[10081] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 5 1 2 3 4 5 1 2 3 4 1 2 4
[10117] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 2 3
[10153] 4 5 1 2 3 4 5 1 3 4 5 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[10189] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[10225] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[10261] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 1 2 3 4 5 1 2 3
[10297] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[10333] 5 1 2 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[10369] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[10405] 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[10441] 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[10477] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[10513] 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2
[10549] 3 4 5 1 2 3 5 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[10585] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[10621] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1
[10657] 2 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1
[10693] 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[10729] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[10765] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 5 1 2 3
[10801] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 3
[10837] 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3
[10873] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
[10909] 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5
[10945] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[10981] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 4 5 1 2 3 4 5 1 2 3
[11017] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[11053] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[11089] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 3 4 1 2 4 1 2 3 4 5 1 2 3 5
[11125] 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 4 5 1
[11161] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4
[11197] 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[11233] 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[11269] 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[11305] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[11341] 1 2 3 4 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[11377] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[11413] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[11449] 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[11485] 3 4 5 2 3 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[11521] 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4
[11557] 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 1 2 4 5 1
[11593] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[11629] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 5 1 2 3
[11665] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[11701] 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[11737] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[11773] 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2
[11809] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[11845] 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 5 1 2 3 4 5
[11881] 2 3 4 5 1 2 3 4 5 1 3 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[11917] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3
[11953] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3
[11989] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 1 2
[12025] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[12061] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 3 4 5 3 5 1 2 3
[12097] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2
[12133] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 1 2
[12169] 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3
[12205] 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[12241] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[12277] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[12313] 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 5
[12349] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[12385] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[12421] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[12457] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[12493] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[12529] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[12565] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4
[12601] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[12637] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[12673] 5 1 2 5 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 2 3
[12709] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
[12745] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[12781] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[12817] 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3
[12853] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[12889] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
[12925] 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[12961] 4 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 4 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[12997] 2 3 4 5 1 2 3 4 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[13033] 2 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[13069] 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2
[13105] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[13141] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[13177] 5 1 2 3 4 1 2 3 4 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[13213] 5 2 3 4 1 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[13249] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 2 3 4 5
[13285] 1 2 3 4 5 1 2 4 5 1 2 3 4 1 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[13321] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 4 1 2 3 4 5 1
[13357] 2 3 4 5 1 2 3 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
[13393] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5
[13429] 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[13465] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 4 1 2 3 4
[13501] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[13537] 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[13573] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[13609] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[13645] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 1 2 3 4
[13681] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[13717] 2 3 4 5 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[13753] 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[13789] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[13825] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1
[13861] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 2 3 4 5 1 2 3 4 5 1
[13897] 2 3 4 5 1 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[13933] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[13969] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[14005] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[14041] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 3
[14077] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1
[14113] 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4
[14149] 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 5 3 4
[14185] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[14221] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[14257] 1 2 4 5 1 2 3 4 5 1 2 3 4 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 1 2 3
[14293] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[14329] 1 2 3 4 5 1 2 3 5 1 2 3 4 1 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[14365] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[14401] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 4 5 1 2 3
[14437] 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5
[14473] 1 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[14509] 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[14545] 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5
[14581] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[14617] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[14653] 2 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[14689] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[14725] 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[14761] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[14797] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 4 5 1 2 4 5 1 2 3 5 1 2 3 4 1 2
[14833] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[14869] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[14905] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 4 5 1 2 3 4 5 1 2 3
[14941] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[14977] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[15013] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[15049] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[15085] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[15121] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[15157] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 3 4 1 3 4 5 1 3 4 5 4
[15193] 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[15229] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
[15265] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[15301] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1
[15337] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 4 5 1 2 3 5 1
[15373] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[15409] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[15445] 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[15481] 1 3 4 5 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[15517] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[15553] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[15589] 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[15625] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[15661] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 3 4 1 2 3 4 1
[15697] 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[15733] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3
[15769] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1
[15805] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[15841] 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
[15877] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[15913] 1 2 3 4 5 1 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[15949] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[15985] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2
[16021] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[16057] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[16093] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1
[16129] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
[16165] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[16201] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[16237] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[16273] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[16309] 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[16345] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1
[16381] 2 3 4 5 2 3 4 5 1 2 3 4 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[16417] 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[16453] 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[16489] 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3
[16525] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[16561] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[16597] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[16633] 1 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2
[16669] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[16705] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3
[16741] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[16777] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3
[16813] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[16849] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[16885] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[16921] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[16957] 5 1 2 3 5 1 2 3 4 5 1 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 4 5 1 4
[16993] 5 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[17029] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3
[17065] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[17101] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5
[17137] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4
[17173] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[17209] 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[17245] 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
[17281] 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4
[17317] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5
[17353] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[17389] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 4 5 1 2 3 4 5 1 3 5
[17425] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[17461] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[17497] 1 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 4 5 1 2 3 5 2 3 4 5 1 2 3 4
[17533] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[17569] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[17605] 4 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[17641] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3
[17677] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2
[17713] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[17749] 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[17785] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[17821] 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[17857] 5 1 2 3 4 5 1 2 3 4 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[17893] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[17929] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1
[17965] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
[18001] 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[18037] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[18073] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[18109] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[18145] 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4
[18181] 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[18217] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[18253] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3
[18289] 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3
[18325] 4 5 1 2 3 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[18361] 4 5 1 2 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[18397] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 4 5
[18433] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4
[18469] 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
[18505] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[18541] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[18577] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[18613] 2 3 4 5 1 2 4 5 1 2 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[18649] 2 3 5 1 2 3 4 5 1 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[18685] 4 1 2 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[18721] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[18757] 3 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[18793] 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[18829] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 2 3 4 5 1 2
[18865] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 1 2 3 4 5 1 2
[18901] 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[18937] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 1 2 3 4 5 1 2 3 4 5
[18973] 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[19009] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[19045] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[19081] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 2 3 4 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4
[19117] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[19153] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 3
[19189] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 2 3 4 5 1 2
[19225] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[19261] 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[19297] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[19333] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[19369] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[19405] 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5
[19441] 1 2 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[19477] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[19513] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[19549] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4
[19585] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3
[19621] 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[19657] 5 1 2 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[19693] 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[19729] 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
[19765] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5
[19801] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3
[19837] 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3
[19873] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[19909] 1 2 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 3
[19945] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[19981] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[20017] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[20053] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[20089] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2
[20125] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[20161] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1
[20197] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[20233] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[20269] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[20305] 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[20341] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[20377] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[20413] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[20449] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1
[20485] 2 3 4 5 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[20521] 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[20557] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[20593] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 1 2 3 5 1 2 3
[20629] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 1 2 3 4 1 2 3
[20665] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
[20701] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[20737] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[20773] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[20809] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
[20845] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4
[20881] 5 1 2 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[20917] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 2 3 5 1
[20953] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1
[20989] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[21025] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[21061] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[21097] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[21133] 1 2 4 1 2 3 5 1 2 3 4 5 2 3 4 5 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 4 5 1
[21169] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4
[21205] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[21241] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 5
[21277] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4
[21313] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[21349] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 4 5 1 2
[21385] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[21421] 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[21457] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[21493] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1
[21529] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[21565] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[21601] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[21637] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[21673] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2
[21709] 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[21745] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3
[21781] 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[21817] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[21853] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[21889] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[21925] 4 1 2 4 5 1 2 3 4 5 1 2 3 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[21961] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[21997] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[22033] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[22069] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[22105] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 1 2 3 4 1 3 4 5 1 2 3
[22141] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2
[22177] 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3
[22213] 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[22249] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 4 5 1 2 3 5 1 2 3
[22285] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[22321] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[22357] 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3
[22393] 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[22429] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[22465] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[22501] 1 2 3 4 5 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[22537] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 3 4 5 1 2 3 4
[22573] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[22609] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[22645] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1
[22681] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[22717] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[22753] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[22789] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[22825] 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3
[22861] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2
[22897] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[22933] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 3 4 1 2 3 4
[22969] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 3 4 1 3 5 2 3 4 1 2 3 4
[23005] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[23041] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[23077] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[23113] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[23149] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 4 5 1
[23185] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 5 1 2 3 4 5 1
[23221] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[23257] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[23293] 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[23329] 1 2 3 4 5 1 3 4 5 1 2 3 4 1 3 4 5 3 4 5 1 2 3 5 1 2 3 4 5 1 4 5 1 2 3 4
[23365] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[23401] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[23437] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
[23473] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1
[23509] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1
[23545] 2 3 4 5 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[23581] 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[23617] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[23653] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[23689] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[23725] 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[23761] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4
[23797] 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[23833] 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[23869] 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[23905] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 4 1 2 3 4 5 1 2
[23941] 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 3 4 5 1 2 3 4
[23977] 5 1 2 4 5 1 2 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[24013] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[24049] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 1 2
[24085] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1
[24121] 2 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[24157] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4
[24193] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5
[24229] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[24265] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 2 3 4
[24301] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[24337] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
[24373] 4 5 1 2 3 4 5 1 3 4 5 2 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[24409] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 2 3 4
[24445] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[24481] 1 2 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1
[24517] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[24553] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 1 2 3 4 5 1 3 4 5 2 3 4 5
[24589] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[24625] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 4 5 1 2 3 5 1 3 4
[24661] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[24697] 5 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[24733] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[24769] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[24805] 3 4 5 1 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[24841] 3 4 1 2 3 4 5 1 2 3 4 5 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[24877] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1
[24913] 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 1 2 3 4 5 1 2
[24949] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[24985] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4
[25021] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 2 3 5 1 2 3 4 5
[25057] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[25093] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 4 5 1 2
[25129] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[25165] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[25201] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[25237] 5 1 2 3 4 5 1 3 4 5 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[25273] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 4 1
[25309] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[25345] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[25381] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5
[25417] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[25453] 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 3 5 1 2 3 4 1 3 4 1 2 3 4 1 2 3
[25489] 4 5 1 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[25525] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3
[25561] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[25597] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[25633] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[25669] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4
[25705] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[25741] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[25777] 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 5 1 2
[25813] 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[25849] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[25885] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[25921] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
[25957] 3 4 1 2 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[25993] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[26029] 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[26065] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[26101] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1
[26137] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[26173] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[26209] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5
[26245] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[26281] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 1 2 3 4 2 3 4 5 2 3 4 5 1 2
[26317] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[26353] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3
[26389] 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[26425] 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[26461] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[26497] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5
[26533] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 2 4 1 2 3 4 5 1 2
[26569] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[26605] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3
[26641] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 2 3 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1
[26677] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4
[26713] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[26749] 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[26785] 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[26821] 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[26857] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[26893] 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2
[26929] 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[26965] 2 3 4 5 1 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[27001] 2 3 4 5 1 2 3 4 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[27037] 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[27073] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[27109] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[27145] 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[27181] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[27217] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[27253] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[27289] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[27325] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[27361] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 4 5 1 2 3 4 5
[27397] 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[27433] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[27469] 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[27505] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 5 1 2 3 5 1 2 4 5 1 2 3 5
[27541] 1 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[27577] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[27613] 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[27649] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[27685] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[27721] 3 4 5 1 2 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 1 2 4 5
[27757] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[27793] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[27829] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[27865] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[27901] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[27937] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5
[27973] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4
[28009] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[28045] 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3
[28081] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[28117] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[28153] 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[28189] 5 1 2 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[28225] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[28261] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2
[28297] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[28333] 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[28369] 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[28405] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[28441] 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[28477] 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 2 3 4 1 2
[28513] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[28549] 3 5 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[28585] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[28621] 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[28657] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[28693] 5 1 2 3 1 2 3 5 1 3 4 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4
[28729] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3
[28765] 4 5 2 3 4 5 1 2 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[28801] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 5 1 3 4 5 1 2 3 4 5
[28837] 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 3 4 5 1 3 4 5 1 2
[28873] 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[28909] 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[28945] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[28981] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1
[29017] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1
[29053] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[29089] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[29125] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[29161] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[29197] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1
[29233] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[29269] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[29305] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[29341] 5 1 2 3 4 5 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[29377] 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 5
[29413] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
[29449] 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[29485] 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[29521] 1 2 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[29557] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[29593] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[29629] 1 2 3 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[29665] 1 2 3 4 5 1 2 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[29701] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3
[29737] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3
[29773] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[29809] 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1
[29845] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
[29881] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[29917] 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[29953] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2
[29989] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[30025] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5
[30061] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[30097] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1
[30133] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
[30169] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[30205] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[30241] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5
[30277] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[30313] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[30349] 5 1 2 3 4 5 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[30385] 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
[30421] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[30457] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[30493] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[30529] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1
[30565] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 1
[30601] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[30637] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[30673] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[30709] 2 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 4 5 1 2 4 5 1 4 5 1
[30745] 3 4 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[30781] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 2 3 4 5 2 3 4 5 1 2 3 1 2 3 4 5
[30817] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[30853] 2 3 4 5 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[30889] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[30925] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[30961] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 3 5
[30997] 1 2 3 4 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[31033] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[31069] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[31105] 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[31141] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5
[31177] 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5
[31213] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 1 3 4 5 2 3 4 5 1 2
[31249] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[31285] 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[31321] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[31357] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[31393] 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2
[31429] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[31465] 2 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2
[31501] 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 2 3 5 1 2 3 4
[31537] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[31573] 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[31609] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[31645] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[31681] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[31717] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 3 4 1 2 3 4 5 1
[31753] 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[31789] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2
[31825] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1
[31861] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 4 5 1 2 3 4 5 1 2 3 4 5
[31897] 1 2 3 4 5 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[31933] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[31969] 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[32005] 4 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[32041] 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 3 4 1
[32077] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 4 5
[32113] 1 2 3 4 5 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[32149] 1 2 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[32185] 2 3 4 5 1 3 4 5 1 2 3 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[32221] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[32257] 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4
[32293] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3
[32329] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 4 5 1 2 3
[32365] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2
[32401] 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
[32437] 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[32473] 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 1 2 3 4 5 1
[32509] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[32545] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[32581] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2
[32617] 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[32653] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[32689] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[32725] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[32761] 5 1 3 4 5 1 2 3 4 5 1 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[32797] 1 2 3 4 5 1 2 3 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 3 4 1 2 3 4 5 1 2 4
[32833] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 3 4
[32869] 5 1 2 3 4 5 2 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[32905] 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[32941] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3
[32977] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 3 4 5 2 4 5 1 2 3 4 5 1 2
[33013] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[33049] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4
[33085] 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1
[33121] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[33157] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[33193] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
[33229] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[33265] 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[33301] 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[33337] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[33373] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[33409] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 2 3 5 1 4 5 1 2 3 4 5 1 3 4
[33445] 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[33481] 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[33517] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 4 5 1 2
[33553] 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[33589] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4
[33625] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[33661] 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[33697] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1
[33733] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[33769] 4 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3
[33805] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[33841] 3 4 5 1 2 3 4 5 2 3 4 1 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[33877] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[33913] 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[33949] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[33985] 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[34021] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[34057] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3
[34093] 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[34129] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
[34165] 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[34201] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 2 3 4 5 1
[34237] 2 3 5 1 2 3 4 5 1 2 4 5 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 4 5 1 2 3 5 1
[34273] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[34309] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 4 5 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[34345] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[34381] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[34417] 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5
[34453] 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[34489] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1
[34525] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[34561] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[34597] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[34633] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[34669] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[34705] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[34741] 2 3 4 5 1 2 3 4 5 1 2 3 4 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[34777] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1
[34813] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 5 2 3 4 5 1 2
[34849] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1
[34885] 2 4 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[34921] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1
[34957] 2 3 4 5 1 3 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[34993] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[35029] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5
[35065] 1 2 3 4 5 1 2 3 4 5 1 2 3 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[35101] 1 2 4 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[35137] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[35173] 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 2 4 5 1 2 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[35209] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[35245] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 3 4 5 1 3 4 5 1 2 3 4
[35281] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3
[35317] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[35353] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[35389] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[35425] 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[35461] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3
[35497] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[35533] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[35569] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[35605] 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[35641] 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 1 2 3 4 5
[35677] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[35713] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 3 4 5 1 2
[35749] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[35785] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[35821] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[35857] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 3 4 5
[35893] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5
[35929] 1 2 3 4 5 1 2 3 4 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[35965] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[36001] 4 1 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[36037] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[36073] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[36109] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[36145] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[36181] 1 2 3 4 5 1 2 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[36217] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[36253] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2
[36289] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[36325] 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 1 2 3 1 2 3 5 1 2 3 4
[36361] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
[36397] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5
[36433] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 5 1 2 3 1 2
[36469] 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1
[36505] 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
[36541] 3 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[36577] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[36613] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[36649] 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[36685] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[36721] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4
[36757] 5 1 2 3 4 5 1 2 3 4 5 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[36793] 4 5 2 4 5 1 2 3 4 5 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[36829] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3
[36865] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[36901] 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[36937] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[36973] 1 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[37009] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4
[37045] 5 1 3 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[37081] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[37117] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[37153] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[37189] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[37225] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[37261] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[37297] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 1 4 5
[37333] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 5
[37369] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[37405] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[37441] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[37477] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
[37513] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[37549] 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 5 1 2 3
[37585] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1
[37621] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[37657] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 2 3 4 5 1 3 4 1 2 3 4 5 1 3 4 5
[37693] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 1
[37729] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[37765] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[37801] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[37837] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3
[37873] 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[37909] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[37945] 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
[37981] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[38017] 4 5 1 2 3 5 2 3 4 5 1 2 4 5 1 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
[38053] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3
[38089] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[38125] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[38161] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[38197] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 3 5 2 4 1 2
[38233] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[38269] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[38305] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[38341] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[38377] 1 2 3 4 1 2 4 1 2 3 4 1 2 3 4 1 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5
[38413] 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[38449] 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[38485] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
[38521] 2 3 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3
[38557] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[38593] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 2 3 4 5 1 2 3 5 1 2 3 4
[38629] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4
[38665] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[38701] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[38737] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[38773] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[38809] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1
[38845] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[38881] 4 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
[38917] 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3
[38953] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[38989] 5 1 2 3 4 5 1 2 3 4 5 2 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[39025] 5 1 2 3 4 5 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[39061] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[39097] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[39133] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[39169] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[39205] 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[39241] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[39277] 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[39313] 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[39349] 5 1 2 3 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5
[39385] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3
[39421] 4 5 1 2 3 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[39457] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[39493] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
[39529] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4
[39565] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[39601] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[39637] 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[39673] 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[39709] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[39745] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[39781] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[39817] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 2 3 4 5 1 2
[39853] 3 4 1 2 3 4 5 1 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[39889] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[39925] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[39961] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[39997] 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3
[40033] 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3
[40069] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1
[40105] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[40141] 5 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[40177] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[40213] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[40249] 1 2 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[40285] 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[40321] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3
[40357] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[40393] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
[40429] 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[40465] 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[40501] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4
[40537] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5
[40573] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5
[40609] 1 2 3 4 5 1 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[40645] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[40681] 4 5 1 2 3 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[40717] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[40753] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 2
[40789] 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3
[40825] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 1 2
[40861] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[40897] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[40933] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 4 5 1 2 4 5 1 2 3 4 5 1
[40969] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[41005] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[41041] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2
[41077] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[41113] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[41149] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[41185] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 5 1 2 4 5
[41221] 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[41257] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[41293] 2 3 5 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[41329] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[41365] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[41401] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[41437] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 4 5 1 2 3 4 5 1 2 3 4
[41473] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[41509] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[41545] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 4 5
[41581] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[41617] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[41653] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 2 3 4 5 1 3
[41689] 4 5 1 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[41725] 3 4 5 1 2 3 4 5 1 2 3 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[41761] 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[41797] 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[41833] 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[41869] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 3 4 1
[41905] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4
[41941] 1 2 3 4 5 1 2 3 4 5 2 3 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
[41977] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[42013] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[42049] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[42085] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[42121] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 2
[42157] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 2 4 5 1 2 3 5 1 2 3 4
[42193] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3
[42229] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[42265] 4 5 1 2 3 4 5 1 2 4 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[42301] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[42337] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[42373] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[42409] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5
[42445] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[42481] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 5 1
[42517] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 4 5 3 4 1 2 3
[42553] 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3
[42589] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1
[42625] 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 2 4 5 1 2 3 4 5 1 2 4
[42661] 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[42697] 4 5 1 2 4 5 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[42733] 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[42769] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[42805] 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[42841] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1
[42877] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[42913] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 4 5 1
[42949] 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[42985] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[43021] 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 3 4 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5 1
[43057] 2 3 4 5 1 2 3 4 5 1 2 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2
[43093] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[43129] 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[43165] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1
[43201] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[43237] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4
[43273] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 1 2 3 4
[43309] 5 1 2 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[43345] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[43381] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
[43417] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4
[43453] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 4
[43489] 5 1 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[43525] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[43561] 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2
[43597] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5
[43633] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[43669] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[43705] 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[43741] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[43777] 3 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[43813] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[43849] 3 4 5 1 2 3 4 5 1 4 5 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5
[43885] 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[43921] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[43957] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[43993] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[44029] 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[44065] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[44101] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5
[44137] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 1 2 3 5 1 2 3 5 1
[44173] 2 3 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[44209] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[44245] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[44281] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[44317] 3 4 2 3 4 5 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[44353] 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[44389] 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2
[44425] 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3
[44461] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3
[44497] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5
[44533] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[44569] 2 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 5 1 2
[44605] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2
[44641] 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[44677] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 5 1
[44713] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[44749] 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4
[44785] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[44821] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 4 5 1 3 4 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2
[44857] 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3
[44893] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[44929] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[44965] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[45001] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[45037] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[45073] 4 5 1 2 3 4 5 1 2 4 1 2 3 4 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[45109] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[45145] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[45181] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[45217] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 3 4 5 1 2 3 4 5 1 2
[45253] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 5 1 3 4 1 2 3 4 5 1 2 3 4
[45289] 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 1 4 5 1 3
[45325] 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[45361] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[45397] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5
[45433] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[45469] 5 1 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[45505] 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 5 1 2 3 4
[45541] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[45577] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[45613] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[45649] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[45685] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[45721] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[45757] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[45793] 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[45829] 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[45865] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[45901] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[45937] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[45973] 1 2 3 4 5 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[46009] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[46045] 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[46081] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[46117] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 2 4 5 1 2 3 4 5 1 2
[46153] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[46189] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3
[46225] 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[46261] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[46297] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[46333] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 2 3 4 5 1 3 4 5 1 2 4 5 1 2 4
[46369] 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[46405] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1
[46441] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[46477] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[46513] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[46549] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[46585] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 5
[46621] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3
[46657] 4 5 1 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[46693] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2
[46729] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[46765] 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1
[46801] 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5
[46837] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[46873] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[46909] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[46945] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[46981] 4 5 1 3 4 1 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[47017] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[47053] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[47089] 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[47125] 1 2 3 4 5 2 3 4 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 2 3
[47161] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2
[47197] 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[47233] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[47269] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[47305] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[47341] 3 4 1 3 4 5 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[47377] 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[47413] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[47449] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3
[47485] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[47521] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[47557] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[47593] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 5 1 2 3 4 5 1 2 3 4 5
[47629] 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[47665] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[47701] 3 4 5 2 3 4 5 1 2 3 5 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4
[47737] 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[47773] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[47809] 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[47845] 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[47881] 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4
[47917] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3
[47953] 5 1 2 3 4 5 1 2 3 4 5 1 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 4 5 1 2 3 4 5
[47989] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[48025] 3 4 5 1 2 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[48061] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[48097] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[48133] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 5 1 2
[48169] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[48205] 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4
[48241] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 4 5 1 2
[48277] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[48313] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3
[48349] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2
[48385] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[48421] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[48457] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[48493] 5 1 2 3 4 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[48529] 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[48565] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[48601] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[48637] 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[48673] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[48709] 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[48745] 2 3 4 5 1 2 3 4 5 2 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[48781] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4
[48817] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[48853] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5
[48889] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[48925] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[48961] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[48997] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 3 4 1 2 3 4 1 2 3 4
[49033] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[49069] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[49105] 5 1 2 4 5 1 2 3 4 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 1 3 4 5 1 2
[49141] 3 4 5 1 3 4 5 1 2 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[49177] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[49213] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[49249] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[49285] 4 5 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[49321] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[49357] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[49393] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[49429] 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[49465] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 1 2 3 4
[49501] 5 1 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[49537] 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3
[49573] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5
[49609] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[49645] 4 5 1 2 4 5 1 3 4 5 1 4 5 1 2 4 5 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2
[49681] 3 4 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2
[49717] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[49753] 5 1 3 4 5 1 2 3 4 1 2 3 4 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[49789] 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1
[49825] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[49861] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1
[49897] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4
[49933] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2
[49969] 3 4 5 1 3 4 5 1 4 1 2 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[50005] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3
[50041] 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 1 2 3
[50077] 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[50113] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3
[50149] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[50185] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[50221] 1 2 3 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[50257] 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[50293] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[50329] 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 4 5 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 2
[50365] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[50401] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[50437] 3 4 5 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 4 5 1 2 3 4
[50473] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[50509] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[50545] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[50581] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3
[50617] 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[50653] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[50689] 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[50725] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2
[50761] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[50797] 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3
[50833] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[50869] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4
[50905] 5 1 3 4 5 1 2 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[50941] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[50977] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 2 3
[51013] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2
[51049] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[51085] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[51121] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[51157] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 5 1
[51193] 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[51229] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[51265] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[51301] 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[51337] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[51373] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 4 5 2 3 4 5 2 3 4 5 1 2 3 5 1
[51409] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1
[51445] 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[51481] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[51517] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5
[51553] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5
[51589] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[51625] 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 2 3 1 3 4 5
[51661] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[51697] 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[51733] 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[51769] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[51805] 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 5 1 2 3
[51841] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3
[51877] 4 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[51913] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[51949] 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[51985] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5
[52021] 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4
[52057] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[52093] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4
[52129] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 3 4 5 1 2 3 4
[52165] 5 1 2 3 4 2 3 4 5 1 2 3 4 1 2 3 4 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[52201] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[52237] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[52273] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[52309] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4
[52345] 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[52381] 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2
[52417] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2
[52453] 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[52489] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[52525] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 3 4 5 1 2 3 5 1 3 4 5 2 3 4 5 1
[52561] 2 3 4 5 1 2 4 5 1 2 3 4 5 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 3 4
[52597] 5 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[52633] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 1 2 3
[52669] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[52705] 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[52741] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2
[52777] 3 4 5 1 2 3 5 1 2 3 4 5 2 4 5 2 3 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5
[52813] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4
[52849] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[52885] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 4 1 2 3 4 1 2 3 4 5 1
[52921] 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4
[52957] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[52993] 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[53029] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[53065] 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 5 1 2
[53101] 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[53137] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[53173] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[53209] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 4
[53245] 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3 4
[53281] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2
[53317] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 5 1 2 3 4 5 1
[53353] 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5
[53389] 1 2 3 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[53425] 5 1 2 3 5 1 2 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1
[53461] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1
[53497] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5
[53533] 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[53569] 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 4 5 1 2 3 4 1 2 4 5 1 2 3 4 5
[53605] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 5
[53641] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3
[53677] 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2
[53713] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[53749] 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[53785] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[53821] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 5 1 2 3
[53857] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[53893] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[53929] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 2 4 5 1
[53965] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 5
[54001] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 3 4 5 1 2 3
[54037] 4 5 2 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[54073] 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 3 5 1 2 3 5 1 2 3
[54109] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 5 1 2
[54145] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5
[54181] 1 2 3 4 5 1 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 5 1
[54217] 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 5 1 2
[54253] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5
[54289] 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 4 5 1 2 3 4 5 1 2 4 1 2
[54325] 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[54361] 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[54397] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[54433] 5 1 2 3 4 5 1 2 3 4 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[54469] 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[54505] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 3 4
[54541] 5 2 3 4 5 1 2 4 1 2 4 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 1 3 4 5 1 2 3 4 5
[54577] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 2 3 4
[54613] 5 1 2 3 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 3 4 1
[54649] 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[54685] 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2
[54721] 3 4 5 1 2 3 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[54757] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 4 1 2
[54793] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[54829] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[54865] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 2 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[54901] 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[54937] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[54973] 1 2 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[55009] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[55045] 1 2 3 4 1 2 3 5 1 2 3 4 5 1 3 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1
[55081] 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1
[55117] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5 1 2 3 4 1 2 3 4 5
[55153] 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5
[55189] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3
[55225] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2
[55261] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 2 3 5 1 2 3 4 5 1 2 3
[55297] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5
[55333] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 5 1 2 3 4
[55369] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1
[55405] 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[55441] 3 4 1 2 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2
[55477] 3 4 5 2 3 5 1 2 3 4 5 1 2 4 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[55513] 5 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[55549] 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 1 2 3 4 5
[55585] 1 2 3 4 5 1 2 3 4 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[55621] 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 3 4
[55657] 5 1 2 3 4 5 1 2 3 4 5 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 2 3 4 5 1 2
[55693] 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[55729] 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 5 1
[55765] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3
[55801] 4 5 1 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3
[55837] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2
[55873] 3 4 5 1 2 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 2 3 4 1 2 3 4 1 2 3 4 5
[55909] 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 4 5
[55945] 1 2 3 4 5 1 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 2 3 4 5 1 2
[55981] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 3 4 5
[56017] 1 2 3 4 5 1 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 5 1
[56053] 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[56089] 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[56125] 3 4 5 1 2 3 4 1 2 3 5 2 3 4 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[56161] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5
[56197] 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 2 3 4 5
[56233] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2
[56269] 4 5 1 4 5 1 2 3 1 2 3 4 5 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 3 4 5 2 3 4 5
[56305] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[56341] 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2
[56377] 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1
[56413] 2 3 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 4 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[56449] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 5 1 3 5 1 2 3
[56485] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[56521] 3 4 5 1 2 3 4 1 2 3 4 5 1 4 5 1 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4
[56557] 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 4
[56593] 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 3 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5
[56629] 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5
[56665] 1 2 4 5 2 3 4 5 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2
[56701] 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[56737] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[56773] 5 2 3 4 5 2 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 2 3
[56809] 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[56845] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 5 2 3 4 5 1 2 3 4 1 2 3
[56881] 5 2 3 4 5 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4
[56917] 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[56953] 3 4 5 1 2 3 5 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2
[56989] 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 1 2 3 5
[57025] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 1 2 3 4 5 1 2 3 4
[57061] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 2 3 4 1 2 3
[57097] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5
[57133] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3
[57169] 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1
[57205] 2 4 5 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[57241] 1 2 4 5 1 3 4 5 1 2 3 4 5 1 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[57277] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 5 2 3 4
[57313] 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 3
[57349] 4 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[57385] 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[57421] 4 5 1 2 3 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[57457] 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 1 2 3 4 5 1 2 3 5 1 2 5
[57493] 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 4 5 1 3 4 5 1 2
[57529] 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 3 4 1 3 4 5 1 2 3 4 5 1 2 3
[57565] 4 5 1 2 4 5 1 3 4 5 1 2 3 4 5 1 2 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[57601] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1
[57637] 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[57673] 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[57709] 1 2 3 4 5 2 3 4 5 1 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[57745] 5 1 3 4 5 1 2 3 4 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5
[57781] 1 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4
[57817] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[57853] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[57889] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3
[57925] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 1 2 3 4 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3
[57961] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 4
[57997] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2
[58033] 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[58069] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[58105] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[58141] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4 1 2 3 4 5 1 2 3
[58177] 4 5 1 2 3 4 5 1 2 3 4 5 1 3 5 1 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4
[58213] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[58249] 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4
[58285] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[58321] 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 1 2 3 4 5
[58357] 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 1 2 3 4
[58393] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[58429] 3 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[58465] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2
[58501] 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[58537] 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1
[58573] 3 4 5 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1
[58609] 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 5 1 3 4 5 1 2 3 4 1 3 4 5 2 3
[58645] 4 5 1 2 3 4 5 1 2 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2
[58681] 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[58717] 5 1 2 4 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[58753] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4
[58789] 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2
[58825] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 1
[58861] 2 3 4 5 1 2 3 4 5 1 3 4 5 1 2 3 4 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
[58897] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 2 3 4 5 1 2 3
[58933] 4 1 2 3 4 5 1 2 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 5 1 2 3 4
[58969] 5 1 2 3 4 5 1 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 1 2 4 5 1 2 3
[59005] 5 1 2 3 4 5 1 2 4 5 1 2 3 4 5 2 3 4 5 1 2 3 5 1 2 3 4 5 1 2 3 4 5 1 2 3
[59041] 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 5 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 4 5 1 2
[59077] 3 4 5 1 2 3 4 5 3 4 5 1 2 5 1 2 3 4 5 1 2 3 5 1 2 3 5 1 2 3 4 5 1 2 3 5
[59113] 1 2 4 5 2 3 4 5 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TIMElong}\SpecialCharTok{$}\NormalTok{Index)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 num [1:59125] 1 2 3 4 5 1 2 4 5 1 ...
\end{verbatim}

Next, the abbreviated preliminary exploration. As you will see, the unbalanced design has prevented them from running properly. While MLM can accomodate unbalanced designs, it can cause difficulties here and there. I will be looking for an alternate way to produce these graphs that can accommodate missing data more effectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rndm30time, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Index, }\AttributeTok{y =}\NormalTok{ Anxiety)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ClientID)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
geom_path: Each group consists of only one observation. Do you need to adjust
the group aesthetic?
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-172-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rndm30time, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Index, }\AttributeTok{y =}\NormalTok{ Anxiety)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ClientID)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-172-2.pdf}

I will run all of the models in one chunk and produce the \emph{tab\_model} table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#with lme4 package}
\NormalTok{M1i }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{(}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{M2i }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index }\SpecialCharTok{+}\NormalTok{ (Index }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control=}\FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{check.nobs.vs.nRE=}\StringTok{"ignore"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M3i }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{(Index }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M4i }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index}\SpecialCharTok{*}\NormalTok{Het0 }\SpecialCharTok{+}\NormalTok{ Index}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ Het0}\SpecialCharTok{*}\NormalTok{DRel0 }\SpecialCharTok{+}\NormalTok{ (Index }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M5i }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(Anxiety }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Index }\SpecialCharTok{+}\NormalTok{ Het0 }\SpecialCharTok{+}\NormalTok{ (Index }\SpecialCharTok{|}\NormalTok{ ClientID), TIMElong, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{lmerControl}\NormalTok{(}\AttributeTok{optimizer=} \StringTok{"bobyqa"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(M5a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Anxiety ~ Weeks + Het0 + (Weeks | ClientID)
   Data: TIMElong
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid 
 219640.6  219703.5 -109813.3  219626.6     59118 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.8672 -0.6005 -0.0024  0.6075  3.5890 

Random effects:
 Groups   Name        Variance    Std.Dev. Corr 
 ClientID (Intercept) 5.158472090 2.271227      
          Weeks       0.000001628 0.001276 -1.00
 Residual             1.260973321 1.122931      
Number of obs: 59125, groups:  ClientID, 12825

Fixed effects:
              Estimate Std. Error t value
(Intercept)  2.4902389  0.0249481  99.817
Weeks       -0.0778045  0.0009443 -82.390
Het0         0.2855055  0.0457711   6.238

Correlation of Fixed Effects:
      (Intr) Weeks 
Weeks -0.249       
Het0  -0.511 -0.001
optimizer (bobyqa) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tab\_model}\NormalTok{(M1i, M2i, M3i, M4i, M5i, }\AttributeTok{p.style =} \StringTok{"numeric"}\NormalTok{, }\AttributeTok{show.ci =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.df =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.re.var =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.aic =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{show.dev =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{use.viewer =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dv.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mod1i"}\NormalTok{, }\StringTok{"Mod2i"}\NormalTok{, }\StringTok{"Mod3i"}\NormalTok{, }\StringTok{"Mod4i"}\NormalTok{, }\StringTok{"Mod5i"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

Mod1i

Mod2i

Mod3i

Mod4i

Mod5i

Predictors

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

Estimates

std. Error

p

(Intercept)

2.08

0.02

\textless0.001

2.83

0.02

\textless0.001

2.75

0.03

\textless0.001

2.72

0.04

\textless0.001

2.75

0.03

\textless0.001

Index

-0.25

0.00

\textless0.001

-0.25

0.00

\textless0.001

-0.25

0.01

\textless0.001

-0.25

0.00

\textless0.001

Het0

0.28

0.05

\textless0.001

0.31

0.08

\textless0.001

0.28

0.05

\textless0.001

Index * Het0

-0.00

0.01

0.969

-0.00

0.01

0.965

DRel0

0.04

0.05

0.447

Index * DRel0

-0.01

0.01

0.248

Het0 * DRel0

-0.04

0.09

0.671

Random Effects

2

1.45

1.29

1.29

1.29

1.29

00

5.10 ClientID

5.21 ClientID

5.19 ClientID

5.19 ClientID

5.19 ClientID

11

~

0.00 ClientID.Index

0.00 ClientID.Index

0.00 ClientID.Index

0.00 ClientID.Index

01

~

-1.00 ClientID

-1.00 ClientID

-1.00 ClientID

-1.00 ClientID

ICC

0.78

~

0.80

~

~

N

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

12825 ClientID

Observations

59125

59125

59125

59125

59125

Marginal R2 / Conditional R2

0.000 / 0.779

0.089 / NA

0.022 / 0.803

0.099 / NA

0.099 / NA

Deviance

226000.825

220678.909

220640.825

220639.293

220640.826

AIC

226012.756

220706.410

220684.625

220704.650

220674.661

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap this statement with the "file = "TabMod\_Table"" to get Viewer output or the outfile that you can open in Word}
\CommentTok{\#file = "TabMod\_Table.doc"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sjPlot}\SpecialCharTok{::}\FunctionTok{plot\_model}\NormalTok{ (M5i, }\AttributeTok{type=}\StringTok{"pred"}\NormalTok{,}\AttributeTok{terms=}\FunctionTok{c}\NormalTok{(}\StringTok{"Index"}\NormalTok{, }\StringTok{"Het0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Multilevel_files/figure-latex/unnamed-chunk-174-1.pdf}
How do the models differ when Index (time-structured) is used to mark time?

In the fixed effects:

\begin{itemize}
\tightlist
\item
  All the models we evaluated came to similar conclusions. That is, the trimmed model included the time variable (SessNum, Weeks, or Index) and sexual identity as predictors.
\item
  The effect of the sexual identity was comparable. That is, LGBQQ clients had anxiety levels that were about 0.28 points higher than heterosexual clients.
\item
  The value of decline for the Index variable (0.25) was three times higher than the value for SessNum or Weeks (0.08). Given that the span of SessNum and weeks was about three times greater (i.e., 0 to 19 versus 0 to 5), this finding is comparable even though the numbers are different. SessNum and Weeks are simply on different \emph{clocks} than Index.
\end{itemize}

The patterns observed in the random effects are quite comparable across all three time variants.

\hypertarget{apa-style-writeup-3}{%
\section{APA Style Writeup}\label{apa-style-writeup-3}}

In each lesson I like to describe or provide an example of an APA style write-up. Because this lesson was a demonstration of techniques and then a reworking of the research vignette in \href{MLMexplore}{longitudinal exploration}, I will refer you back to that lesson for an example of an APA Style write-up.

The only difference in writing it up would be to provide a description about how the Weeks variable clocked time and some detail about how it was calculated or coded.

\hypertarget{residual-and-related-questions-2}{%
\section{Residual and Related Questions\ldots{}}\label{residual-and-related-questions-2}}

..that you might have; or at least I had, but if had answered them earlier it would have disrupt the flow.

\hypertarget{what-did-we-gainlose-by-using-weeks-or-sessnum-unstructured-versus-index-structured-to-mark-time}{%
\subsection{What did we gain/lose by using Weeks or SessNum (unstructured) versus Index (structured) to mark time?}\label{what-did-we-gainlose-by-using-weeks-or-sessnum-unstructured-versus-index-structured-to-mark-time}}

My strong preference is to use unstructured time. In this particular case, it ``stretches out'' the time scale so that we get a more precise understanding of change over time. While the effect of the time variable may not appear to be as strong (-.25 for Index; -0.08 for SessNum and Weeks), this is misleading. Why? The ``clock'' for Index has only 5 values. In contrast, the clock for SessNum had 20 values (0 to 19, required to be integers/whole numbers) and the clock for Weeks was rather infinite (i.e., the range for Weeks was limited from 0 to 19 but it allowed for fractions).

\hypertarget{how-did-the-unbalanced-time-impact-the-analysis-and-results}{%
\subsection{How did the unbalanced time impact the analysis and results?}\label{how-did-the-unbalanced-time-impact-the-analysis-and-results}}

The quality of data is always improved when there are at least three observations per case in longitudinal models. Models \emph{can} run when there are 1 or 2 observations per individual, but only in case where the rest of the data is really robust (this dataset is really robust!). Evidence, though, of problems caused by unequal cell sizes was observed when ggplot failed to render the individual growth plots.

While our unbalanced design ran, I have had data where models would not converge. In those cases, convergence was made possible by (a) replacing control statements or optimizers and (b) deleting cases with only one observation per cases (and then if that didn't run, deleting cases with only two observations per case).

Although we haven't yet addressed it in this OER, there are also options related to fixing and freeing the slopes and intercepts.

\hypertarget{practice-problems-3}{%
\section{Practice Problems}\label{practice-problems-3}}

The prior lessons suggests working a longitudinal MLM from exploration through model building and interpretation. Consequently, for practice, I encourage further adaptation to that analysis by (a) changing the time metric and comparing analyses and (b) if using a simulation with no missingness, randomly delete some rows from the long file so that the numbers of cases are different. There are a number of ways to do this and all data are different. Consequently, the ideas below are merely suggestions.

\hypertarget{problem-1-rework-this-lessons-example-by-changing-the-time-metric-to-days-or-months}{%
\subsection{Problem \#1: Rework this Lesson's Example by Changing the Time Metric to Days or Months}\label{problem-1-rework-this-lessons-example-by-changing-the-time-metric-to-days-or-months}}

If you would like practice with time conversions, rework this lesson by changing calendrical time from weeks (dweeks) to another unit such as days or months. Work the example and compare the answers to those in the lesson. For more of a challenge use the simulation from the Lefevor et al. \citeyearpar{lefevor_religious_2017} where depression is the outcome. This will require you to engage in more of the simulation. The simulation is available in the Bonus Track of the \href{MLMexplore}{longitudinal exploration}lesson.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7387}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1261}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Apply date format to variables & 5 & \_\_\_\_\_ \\
2. Establish a ``time interval'' variable & 5 & \_\_\_\_\_ \\
3. Extract the duration from the variable & 5 & \_\_\_\_\_ \\
4. Convert to a long file & 5 & \_\_\_\_\_ \\
5. Produce set of individual growth plots with an imposed regression line & 5 & \_\_\_\_\_ \\
6. Run a set of models (unconditional means through trimming) & 5 & \_\_\_\_\_ \\
7. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
8. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
9. Interpret the output, comparing how it differs from a different metric of time & 5 & \_\_\_\_\_ \\
10. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 50 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-2-compare-balanced-and-unbalanced-designs}{%
\subsection{Problem \#2: Compare balanced and unbalanced designs}\label{problem-2-compare-balanced-and-unbalanced-designs}}

Using (a) this lesson's data or (b) the simulation where depression is the outcome (located in the Bonus Track of the \href{MLMexplore}{longitudinal exploration}lesson), delete a different number of cases from the long file and rework the problem.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7387}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1261}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. From the long file, delete a meaningful number of cases & 5 & \_\_\_\_\_ \\
2. Produce set of individual growth plots with an imposed regression line & 5 & \_\_\_\_\_ \\
3. Run a set of models (unconditional means through trimming) & 5 & \_\_\_\_\_ \\
4. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
5. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
6. Interpret the output, comparing how it differs from a different metric of time & 5 & \_\_\_\_\_ \\
7. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 35 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{problem-3-experiment-with-time-and-balanceunbalance-in-data-that-is-available-to-you}{%
\subsection{Problem \#3: Experiment with time and balance/unbalance in data that is available to you}\label{problem-3-experiment-with-time-and-balanceunbalance-in-data-that-is-available-to-you}}

Using data for which you have permission and access play around with time and balance/unbalance.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7387}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1261}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule
\endhead
1. Play with time?\_\_\_\_\_\_\_\_\_\_ & 5 & \_\_\_\_\_ \\
2. Plan with missingness?\_\_\_\_\_\_\_\_\_ & 5 & \_\_\_\_\_ \\
3. Produce set of individual growth plots with an imposed regression line & 5 & \_\_\_\_\_ \\
4. Run a set of models (unconditional means through trimming) & 5 & \_\_\_\_\_ \\
5. Create a tab\_model table with the final set of models & 5 & \_\_\_\_\_ \\
6. Create a figure to represent the result & 5 & \_\_\_\_\_ \\
7. Interpret the output, comparing how it differs from a prior analysis & 5 & \_\_\_\_\_ \\
8. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\bottomrule
\end{longtable}

\hypertarget{bonus-track-2}{%
\section{Bonus Track:}\label{bonus-track-2}}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.45833in,height=2.19792in]{images/film-strip-1.jpg}
\caption{Image of a filmstrip}\label{id}
}
\end{figure}

\hypertarget{faqs}{%
\subsection{FAQs}\label{faqs}}

\hypertarget{what-if-this-was-a-qualtrics-datestamp}{%
\subsubsection{What if this was a Qualtrics datestamp?}\label{what-if-this-was-a-qualtrics-datestamp}}

Qualtrics date stamps look like this: 2018-01-03 10:05:46

Correspondingly, it is important to let lubridate know that the format is year/month/day\_hour/minute/second. The script below, denotes that with the function \emph{ymd\_hms()}. If your downloaded Qualtrics data does not have all those elements (or they are in different order), then consult the \emph{lubridate} package options for the correct function.

Below, hashtagged out, is code I have used to format the Qualtrics datestamp. Unless you have changed it for your own surveys, Qualtrics data is collected in Mountain time. Therefore, I have indicated the US/Mountain timezone.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#df$DateVariable \textless{}{-} ymd\_hms(df$DateVariable, tz = "US/Mountain")}
\end{Highlighting}
\end{Shaded}

\hypertarget{what-if-i-care-about-time-zones}{%
\subsubsection{What if I care about time zones?}\label{what-if-i-care-about-time-zones}}

As noted in the prior question, ``tz'' is the function used to indicate the time zone. A complete list of timezones can be found with the \emph{OlsonNames()} function. I have hashtagged it out because it is quite a length list!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#OlsonNames()}
\end{Highlighting}
\end{Shaded}

\hypertarget{refs}{%
\chapter*{References}\label{refs}}
\addcontentsline{toc}{chapter}{References}

  \bibliography{STATSnMETH.bib}

\end{document}
